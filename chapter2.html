<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter II: From Deep Learning to Deep Voting | ABC in AI</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Source+Serif+4:ital,opsz,wght@0,8..60,400;0,8..60,600;1,8..60,400&display=swap"
        rel="stylesheet">
    <!-- MathJax for LaTeX rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-8HDSX11G9D"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'G-8HDSX11G9D');
    </script>
    <!-- Privacy-friendly analytics by Plausible -->
    <script async src="https://plausible.io/js/pa-OtFEayY3RhHZVOzX3iV76.js"></script>
    <script>
        window.plausible = window.plausible || function () { (plausible.q = plausible.q || []).push(arguments) }, plausible.init = plausible.init || function (i) { plausible.o = i || {} };
        plausible.init()
    </script>
</head>

<body>
    <div class="wrapper">
        <nav>
            <a href="index.html">I. Introduction</a>
            <a href="chapter2.html" class="active">II. Deep Voting</a>
            <a href="chapter3.html">III. Network-Source AI</a>
            <a href="chapter4.html">IV. Broad Listening</a>
            <a href="chapter5.html">V. Conclusion</a>
            <a href="https://andrewtrask.com">About</a>
            <a href="appendix1.html">Appendix I</a>
            <a href="appendix2.html">Appendix II</a>
        </nav>

        <div class="page-container">
            <aside class="toc-sidebar">
                <h4>On This Page</h4>
                <ul>
                    <li class="toc-h3"><a href="#chapter-summary">Chapter Summary</a></li>
                    <li class="toc-h3"><a href="#symptom">The Symptom: Underutilization</a></li>
                    <li class="toc-h4"><a href="#underutilized-compute">Underutilized Compute</a></li>
                    <li class="toc-h4"><a href="#siloed-data">Siloed Data</a></li>
                    <li class="toc-h4"><a href="#root-causes">Root Causes (Three Whys)</a></li>
                    <li class="toc-h3"><a href="#first-why">First Why: ABC</a></li>
                    <li class="toc-h3"><a href="#second-why">Second Why: Feature Mixing</a></li>
                    <li class="toc-h3"><a href="#third-why">Third Why: Addition</a></li>
                    <li class="toc-h3"><a href="#third-hypothesis">Third Hypothesis: Concatenation</a></li>
                    <li class="toc-h3"><a href="#second-hypothesis">Second Hypothesis: Deep Voting</a></li>
                    <li class="toc-h3"><a href="#first-hypothesis">First Hypothesis: 6+ OOM</a></li>
                    <li class="toc-h3"><a href="#empirical-evidence">Empirical Evidence</a></li>
                </ul>
                <div class="ascii-decoration">
                    ████▓▓▒▒░░

                    gradients
                    adding
                    erasing
                    sources
                </div>
            </aside>

            <main>
                <div class="chapter-header">
                    <p class="chapter-number">Chapter II</p>
                    <h1>From Deep Learning to Deep Voting</h1>
                </div>

                <figure class="full-width">
                    <img src="1411.3146/abc_ch2_2_v8.png"
                        alt="The lack of ABC creates problems for data-owning institutions">
                    <figcaption>The lack of ABC creates problems for data-owning institutions, problems which avert
                        their sharing of data and compute for AI training, problems which are underpinned&mdash;at their
                        core&mdash;by the overuse of addition within deep learning systems.</figcaption>
                </figure>

                <section id="chapter-summary" class="abstract">
                    <h2>Chapter Summary</h2>

                    <p>
                        Estimates below suggest that models are trained using less than 1/1,000,000th of the world’s
                        data
                        and AI compute productivity. Consequently, following AI’s scaling laws (<a href="#ref-23"
                            class="cite">Kaplan et al., 2020</a>; <a href="#ref-24" class="cite">Hoffmann et al.,
                            2022</a>), AI models possess
                        capabilities which are insignificant compared to what
                        existing data, compute, and algorithms could create.
                    </p>

                    <p>
                        Yet, if AI models (and their capabilities) are the lifeblood of the AI industry, why are data
                        and compute so underutilized? Why is AI capability so constrained? This chapter unpacks the
                        cause of such a drastic resource under-utilization. It begins by linking resource utilization to
                        attribution-based control (ABC). It then breaks attribution-based control into problems with
                        attribution and control, which are themselves underpinned by deep learning’s core philosophy
                        of mixing dense features. This mixing is only problematic because of a specific technical
                        choice: the use of addition to update model weights, which erases provenance information
                        during gradient descent.
                    </p>
                </section>

                <figure class="full-width">
                    <img src="1411.3146/deep_learning_to_deep_voting_v3.png" alt="Deep Learning vs Deep Voting">
                    <figcaption>
                        Traditional open/closed-source deep learning systems (left) pool all data into a
                        deep learning model (i.e. by adding weight updates) which is later used for predictions, while
                        deep voting systems (right) learn weight parameters which remain partitioned by source (i.e.
                        concatenated), but which are learned in a way that they can be rapidly synthesized on the fly.
                        Partitions (pictured above as pie slices) are rapidly synthesized to form a model used for a
                        specific prediction. Darker lines/slices indicate information being used for a particular
                        prediction.
                        Lighter lines/slices indicate information not being used for a particular prediction. The circle
                        on top of the slices represents a dense model capable of rapidly synthesizing slices in
                        practice.
                        The combination of concatenated weights and concatenated metadata with inference predictions
                        (which tracks the synthesis and subsequent use of slices for an AI prediction) enables ABC.
                    </figcaption>
                </figure>

                <section>
                    <p>
                        The chapter then explores alternatives to addition during the training process, revealing
                        a fundamental trade off between three factors: AI capability (driven by unrestricted feature
                        learning), attribution (tracking where features came from), and computational complexity
                        (tracking the path of feature mixing). It proposes a key innovation, a re-purposing of
                        differential
                        privacy for attribution: <em>differential attribution</em>, using the natural boundaries of
                        training
                        documents
                        to identify which concepts must mix freely and vice versa, thereby pushing this Pareto frontier
                        by providing a data-driven approach to balance addition and concatenation.
                    </p>

                    <p>
                        Building on this insight, the chapter develops a specific form of concatenation to replace
                        addition in key sections of the deep learning training process. This transformation&mdash;from
                        deep
                        learning to <strong>deep voting</strong>&mdash;cascades upward through the aformentioned
                        hierarchy of problems,
                        reducing the need for dense feature mixing across data sources, enabling attribution-based
                        control, and unlocking a viable path towards another 6+ orders of magnitude of training data and
                        compute productivity. Taken together, the chapter reveals how a seemingly technical choice (the
                        use of addition) creates far-reaching consequences for AI systems, and how careful, data-driven
                        use of concatenation may dramatically expand AI’s access to computational and data resources.
                    </p>
                </section>

                <section id="symptom">
                    <h2>The Symptom: Data/Compute Underutilization</h2>

                    <figure class="full-width">
                        <img src="1411.3146/abc_ch2_2_v9.png" alt="Problem tree">
                        <figcaption>
                            The lack of ABC creates problems for data-owning institutions, problems which avert their
                            sharing of data and compute for AI training, problems which are underpinned by the overuse
                            of addition within deep learning systems.
                        </figcaption>
                    </figure>

                    <p>
                        As of NeurIPS 2024, leading AI researchers have reported that available compute and data
                        reserves are approaching saturation, creating constraints on both computational resources and
                        pre-training scale (<a href="#ref-3" class="cite">Robison 2024</a>; <a href="#ref-25"
                            class="cite">Strati et al., 2024</a>). However, this assessment overlooks
                        approximately six orders of magnitude of underutilized compute productivity and siloed data.
                        Rather than absolute scarcity, the industry faces structural problems of data and compute access
                        and productivity.
                    </p>

                    <h3 id="underutilized-compute">6+ OOM: Underutilized Training Compute Productivity</h3>

                    <p>
                        The AI industry’s computational requirements have driven significant economic and geopolitical
                        consequences, including NVIDIA’s rise to become the world’s most valuable company, U.S.
                        export restrictions on AI chips to China, and intense competition for latest-generation hardware
                        among startups, enterprises, and major technology firms (<a href="#ref-26" class="cite">Kaye
                            2025</a>; <a href="#ref-27" class="cite">Kachwala and Bajwa 2025</a>; <a href="#ref-28"
                            class="cite">Howley 2023</a>). However, recent evidence suggests that current AI training
                        and inference
                        processes utilize less than 0.0002% of available compute productivity, indicating that perceived
                        compute scarcity may reflect inefficiency rather than absolute resource limits. To evaluate this
                        claim, this section estimates computational waste in two key activities: inference (forward
                        propagation) and learning (backpropagation and gradient descent).
                    </p>

                    <figure class="full-width">
                        <img src="1411.3146/abc_ch2_2_1_v8.png" alt="Compute inefficiency tree">
                        <figcaption>A tree of sub-problems regarding inefficient training compute productivity.
                        </figcaption>
                    </figure>

                    <h4>2-3 OOM: Inefficient AI inference</h4>

                    <p>
                        Due to its role in commercial deployments, analysts estimate that AI firms spend billions of
                        dollars annually on inference (<a href="#ref-29" class="cite">You 2025</a>). However, while
                        these costs might appear to reflect
                        fundamental requirements for achieving high performance, a growing body of empirical work
                        indicates substantial inefficiency in current inference practices.
                    </p>

                    <div class="callout">
                        <p class="callout-title">A Library Analogy</p>

                        <p>
                            Consider a library. When someone asks a librarian about the rules of chess, the librarian
                            doesn’t subsequently read <em>every book in the library</em> to find the answer. Instead,
                            they use
                            the catalog system to find a relevant bookshelf, the titles of books on that shelf to find
                            the relevant book, and the table of contents of that book to find the relevant section. This
                            practice stands in stark contrast to how AI systems process information. To make an AI
                            prediction with a model like GPT-3, AI users forward propagate <em>through the entire model
                                and all of its knowledge</em> (i.e., read every book in the library). And in the case of
                            large
                            language models, they don’t just do this once per answer, they do this <em>for every token
                                they
                                predict</em>. This is analogous to a librarian reading every book in the library every
                            time they
                            utter a word. Given how implausible it is that any prediction requires <em>the entirety of
                                an AI
                                models knowledge</em>, AI’s full, dense inference is a staggering inefficiency.
                        </p>
                    </div>

                    <p>
                        AI models store information within their weights. General-purpose models (e.g., Gemini,
                        ChatGPT, Claude, Llama) encode substantial portions of their training corpora (often
                        representing significant fractions of publicly available internet data) in these parameters.
                        However, when
                        models like GPT-3 generate predictions, they forward propagate through every non-embedding
                        parameter in the network, regardless of query relevance. This constitutes a form of exhaustive
                        computation wherein all stored knowledge is activated for each inference, analogous to searching
                        an entire corpus rather than querying relevant subsets.
                    </p>

                    <p>
                        From an information-theoretic perspective, this practice is inefficient; the relevant question
                        concerns the magnitude of this inefficiency. A comprehensive answer would require empirically
                        measuring the maximum percentage of model weights that can be excluded from inference
                        without degrading accuracy. While such systematic measurement remains incomplete, existing
                        work provides lower bounds on potential efficiency gains.
                    </p>

                    <p>
                        DeepMind’s RETRO achieves comparable performance to GPT-3 while using 1/25th of
                        the parameters through retrieval from a large-scale vector database (Borgeaud et al., 2022).
                        Similarly, Meta’s ATLAS demonstrates that models can be reduced to 1/50th their original size
                        while maintaining or exceeding baseline performance through database-augmented inference
                        (<a href="#ref-30" class="cite">Izacard et al., 2023</a>).
                    </p>

                    <p>
                        We adopt RETRO/ATLAS-style parameter efficiency as a conservative lower bound on
                        current compute waste, noting that these approaches have not been widely adopted in either the
                        sparsity literature (<a href="#ref-31" class="cite">Lederer 2024</a>) or frontier AI
                        deployments, nor have comparable efficiency
                        gains been demonstrated through alternative methods (cf. the persistent redundancy problem
                        in Mixture of Experts models (<a href="#ref-32" class="cite">Dai et al., 2024</a>)). These
                        results
                        suggest that at least 96-98% of
                        parameters activated during dense inference are unnecessary for individual queries.
                    </p>

                    <p>
                        This estimate is likely conservative, as it implies that 2-4% of a model’s knowledge base
                        is relevant to any individual query. <a href="#sidenote-1" class="sidenote-ref"><sup>1</sup></a>
                        However, parameter overuse during forward propagation
                        represents only one source of computational waste. A second form of inefficiency arises in how
                        models store and access information.
                    </p>

                    <div class="callout">
                        <p class="callout-title">A Library Analogy</p>

                        <p>
                            Consider a library once again. When someone asks a librarian about the rules of chess, and
                            the librarian goes to fetch a particular book, the librarian doesn’t bring back <em>every
                                copy
                                of
                                the book in the library</em>. And, as unintuitive as this might seem, the librarian also
                            doesn’t
                            bring back <em>empty books from random assortments of shelves</em>. Instead, they use the
                            catalog
                            system to find a relevant bookshelf, the titles of books on that shelf to find the relevant
                            book, and then they select a single book for the library’s customer.
                        </p>

                        <p>
                            This practice stands in stark contrast to how AI systems process information. To
                            make an AI prediction within a model like GPT-3, AI users don’t merely forward propagate
                            <em>through the entire model and all of its knowledge</em> (i.e., read every book in the
                            library),
                            AI users must forward propagate through some mixture of <em>multiple copies of the same
                                information</em> (i.e. multiple copies of the same book) as well as <em>empty vector
                                space</em> (i.e.
                            empty books) in order to create an output.
                        </p>
                    </div>

                    <div class="sidenote" id="sidenote-1">
                        <p>
                            <sup>1</sup>
                            While this fraction seems implausibly large for most queries, systematic measurement of
                            query-specific
                            parameter relevance remains limited in the literature. We therefore retain this conservative
                            2-4% estimate.
                        </p>
                    </div>

                    <p>
                        Recent work demonstrates that current architectures contain redundant and underutilized
                        parameters. Guo et al. achieve 5-10x reduction in parameter count through lossless compression
                        while maintaining accuracy: ”Notably, we distill CIFAR-10 and CIFAR-100 to 1/5 and Tiny
                        ImageNet to 1/10 of their original sizes without any performance loss on ConvNet, offering
                        the first lossless method of dataset distillation” (<a href="#ref-33" class="cite">Guo et al.,
                            2023</a>). This compression has been
                        demonstrated across multiple standard architectures, as shown in <a href="#table-2-1">Table
                            2.1.</a>
                    </p>

                    <div class="table-container" id="table-2-1">
                        <table class="data-table">
                            <caption>Table 2.1: Table and caption from (<a href="#ref-33" class="cite">Guo et al.,
                                    2023</a>)
                                (see paper for additional
                                detailed descriptions). ”Comparison with previous dataset distillation methods on
                                CIFAR-10, CIFAR-100
                                and Tiny ImageNet... Highlighted results indicate we achieve lossless distillation.” -
                                (<a href="#ref-33" class="cite">Guo et al., 2023</a>)</caption>
                            <thead>
                                <tr>
                                    <th>Dataset</th>
                                    <th colspan="5">CIFAR-10</th>
                                    <th colspan="4">CIFAR-100</th>
                                    <th colspan="3">Tiny ImageNet</th>
                                </tr>
                                <tr>
                                    <th>IPC</th>
                                    <th>1 <br> 0.02</th>
                                    <th>10 <br> 0.2</th>
                                    <th>50 <br>1</th>
                                    <th>500 <br> 10</th>
                                    <th>1000 <br>20</th>
                                    <th>1 <br>0.2</th>
                                    <th>10 <br> 2</th>
                                    <th>50 <br> 10</th>
                                    <th>100 <br>20</th>
                                    <th>1 <br>0.2</th>
                                    <th>10 <br>2</th>
                                    <th>50 <br>10</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Random</td>
                                    <td>15.4±0.3</td>
                                    <td>31.0±0.5</td>
                                    <td>50.6±0.3</td>
                                    <td>73.2±0.3</td>
                                    <td>78.4±0.2</td>
                                    <td>4.2±0.3</td>
                                    <td>14.6±0.5</td>
                                    <td>33.4±0.4</td>
                                    <td>42.8±0.3</td>
                                    <td>1.4±0.1</td>
                                    <td>5.0±0.2</td>
                                    <td>15.0±0.4</td>
                                </tr>
                                <tr>
                                    <td>DC</td>
                                    <td>28.3±0.5</td>
                                    <td>44.9±0.5</td>
                                    <td>53.9±0.5</td>
                                    <td>72.1±0.4</td>
                                    <td>76.6±0.3</td>
                                    <td>12.8±0.3</td>
                                    <td> 25.2±0.3</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                </tr>
                                <tr>
                                    <td>DM</td>
                                    <td>26.0±0.8</td>
                                    <td>48.9±0.6</td>
                                    <td>63.0±0.4</td>
                                    <td>75.1±0.3</td>
                                    <td>78.8±0.1</td>
                                    <td>11.4±0.3</td>
                                    <td>29.7±0.3</td>
                                    <td>43.6±0.4</td>
                                    <td>-</td>
                                    <td>3.9±0.2</td>
                                    <td>12.9±0.4</td>
                                    <td>24.1±0.3</td>
                                </tr>
                                <tr>
                                    <td>DSA</td>
                                    <td>28.8±0.7</td>
                                    <td>52.1±0.5</td>
                                    <td>60.6±0.5</td>
                                    <td>73.6±0.3</td>
                                    <td>78.7±0.3</td>
                                    <td>13.9±0.3</td>
                                    <td>32.3±0.3</td>
                                    <td>42.8±0.4</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                </tr>
                                <tr>
                                    <td>CAFE</td>
                                    <td>30.3±1.1</td>
                                    <td>46.3±0.6</td>
                                    <td>55.5±0.6</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>12.9±0.3</td>
                                    <td>27.8±0.3</td>
                                    <td> 37.9±0.3</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                </tr>
                                <tr>
                                    <td>KIP<sup>1</sup></td>
                                    <td>49.9±0.2</td>
                                    <td>62.7±0.3</td>
                                    <td>68.6±0.2</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>15.7±0.2</td>
                                    <td>28.3±0.1</td>
                                    <td> 37.9±0.3</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                </tr>
                                <tr>
                                    <td>FRePo<sup>1</sup></td>
                                    <td>46.8±0.7</td>
                                    <td>65.5±0.4</td>
                                    <td>71.7±0.2</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>28.7±0.1</td>
                                    <td>42.5±0.2</td>
                                    <td> 44.3±0.2</td>
                                    <td>-</td>
                                    <td>15.4±0.3</td>
                                    <td>25.4±0.2</td>
                                    <td>-</td>
                                </tr>
                                <tr>
                                    <td>RCIG<sup>1</sup></td>
                                    <td>53.9±1.0</td>
                                    <td>69.1±0.4</td>
                                    <td>73.5±0.3</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>39.3±0.4</td>
                                    <td>44.1±0.4</td>
                                    <td> 46.7±0.3</td>
                                    <td>-</td>
                                    <td>25.6±0.3</td>
                                    <td>29.4±0.2</td>
                                    <td>-</td>
                                </tr>
                                <tr>
                                    <td>MTT<sup>2</sup></td>
                                    <td>46.2±0.8</td>
                                    <td>65.4±0.7</td>
                                    <td>71.6±0.2</td>
                                    <td>
                                        <img src="1411.3146/figures/chart_down.png" width="20" height="20" alt="">
                                    </td>
                                    <td>
                                        <img src="1411.3146/figures/chart_down.png" width="20" height="20" alt="">
                                    </td>
                                    <td>24.3±0.3</td>
                                    <td>39.7±0.4</td>
                                    <td>47.7±0.2</td>
                                    <td>49.2±0.4</td>
                                    <td>8.8±0.3</td>
                                    <td>23.2±0.2</td>
                                    <td>28.0±0.3</td>
                                </tr>
                                <tr>
                                    <td>TESLA<sup>2</sup></td>
                                    <td><strong>48.5±0.8</strong></td>
                                    <td>66.4±0.8</td>
                                    <td>72.6±0.7</td>
                                    <td>
                                        <img src="1411.3146/figures/chart_down.png" width="20" height="20" alt="">
                                    </td>
                                    <td>
                                        <img src="1411.3146/figures/chart_down.png" width="20" height="20" alt="">
                                    </td>
                                    <td>24.8±0.4</td>
                                    <td>41.7±0.3</td>
                                    <td>47.9±0.3</td>
                                    <td>49.2±0.4</td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>-</td>
                                </tr>
                                <tr>
                                    <td>FTD<sup>2,3</sup></td>
                                    <td>46.0±0.4</td>
                                    <td>65.3±0.4</td>
                                    <td>73.2±0.2</td>
                                    <td>
                                        <img src="1411.3146/figures/chart_down.png" width="20" height="20" alt="">
                                    </td>
                                    <td>
                                        <img src="1411.3146/figures/chart_down.png" width="20" height="20" alt="">
                                    </td>
                                    <td>24.4±0.4</td>
                                    <td>42.5±0.2</td>
                                    <td>48.5±0.3</td>
                                    <td>49.7±0.4 </td>
                                    <td>10.5±0.2</td>
                                    <td>23.4±0.3</td>
                                    <td>28.2±0.4</td>
                                </tr>
                                <tr class="highlight">
                                    <td><strong>DATM (Ours)</strong></td>
                                    <td>46.9±0.5</td>
                                    <td><strong>66.8±0.2 </strong></td>
                                    <td><strong>76.1±0.3</strong></td>
                                    <td><strong>83.5±0.2 </strong></td>
                                    <td class="lossless"><strong>85.5±0.4</strong></td>
                                    <td><strong>27.9±0.2</strong></td>
                                    <td><strong>47.2±0.4</strong></td>
                                    <td><strong>55.0±0.2</strong></td>
                                    <td class="lossless"><strong>57.5±0.2</strong></td>
                                    <td><strong>17.1±0.3</strong></td>
                                    <td><strong>31.1±0.3</strong></td>
                                    <td class="lossless"><strong>39.7±0.3</strong></td>
                                </tr>
                                <tr class="baseline">
                                    <td>Full Dataset</td>
                                    <td colspan="5">84.8±0.1</td>
                                    <td colspan="4">56.2±0.3 </td>
                                    <td colspan="3">37.6±0.4</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <p>
                        Because they account for information waste in different ways, these inefficiencies compound
                        multiplicatively: irrelevant parameters (25-50x+) and redundant parameters (5-10x+) suggest a
                        125-500x+ lower-bound to inference inefficiency. And while these are estimates, both bounds
                        come from working implementations that maintain model performance, using techniques which
                        are not widely popular, suggesting this waste is common in frontier AI systems, and that this
                        waste stems from architectural choices rather than fundamental limitations
                    </p>

                    <h4>6 OOM: Underutilized and Inefficient Compute in AI Learning</h4>

                    <p>
                        AI firms famously spend immense amounts of money training their AI models, a point which
                        features heavily in their marketing (<a href="#ref-34" class="cite">Meta AI 2024</a>; <a
                            href="#ref-34" class="cite">Brown et al., 2020</a>; <a href="#ref-34" class="cite">Wiggers
                            2024</a>). However,
                        despite widespread hype around AI training spend, a theoretical inefficiency is backed by an
                        increasingly large body of empirical observations, suggesting that the compute requirements for
                        training AI models are largely misunderstood. To introduce the theory, consider an analogy.
                    </p>

                    <div class="callout">
                        <p class="callout-title">A Library Analogy</p>

                        <p>
                            As before, consider a library. When a library adds or removes a significant number of books
                            to/from their collection, they don’t <em>rebuild the entire building and re-print all of
                                their
                                books
                                from scratch</em>, they simply add/remove books, shelves, or rooms. These practices
                            stand
                            in stark contrast to how AI systems process information. To add or remove a significant
                            portion of knowledge from a deep learning system, AI researchers <em>retrain them from
                                scratch</em>
                            (i.e., tear down the entire library, burn all the books, rebuild the library, and re-print
                            all
                            the books from scratch). Despite being a widespread, even ubiquitous practice within AI
                            research, this practice is staggeringly inefficient. It re-characterizes the claims of
                            compute
                            scarcity in an entirely new light. It is like a librarian who repeatedly tears down their
                            library,
                            and re-prints all their books — lamenting insufficient bricks, paper, or ink.
                        </p>
                    </div>

                    <p>
                        AI models store information within their weights. To acquire this information, models are
                        trained on large corpora at substantial computational cost (e.g., significant portions of the
                        public
                        internet) (<a href="#ref-37" class="cite">Epoch AI 2025</a>). However, when models require
                        substantial updates (either
                        incorporating new information or removing outdated content) current practice involves retraining
                        from
                        scratch (<a href="#ref-38" class="cite">Goodfellow et al., 2016</a>). This approach discards all
                        previously
                        computed parameters
                        and repeats the entire training process, even when the majority of learned representations
                        remain
                        valid.
                    </p>

                    <p>
                        From an information-theoretic perspective, this practice is inefficient; the question concerns
                        its magnitude. Comprehensive quantification would require detailed documentation of compute
                        allocation within leading AI firms, information that is not publicly available. However, public
                        disclosures and industry analysis provide sufficient data to establish lower bounds on this
                        inefficiency.
                    </p>

                    <p>
                        Analysis of the largest AI firms reveals that pre-training their most capable models consumes
                        less than 1% of quarterly compute budgets (see <a href="#table-2-2">Table 2.2</a>; methodology
                        detailed in Appendices
                        I and II). Yet these same firms continue expanding computational infrastructure to support
                        larger models (<a href="#ref-39" class="cite">Mehta 2024</a>; <a href="#ref-40"
                            class="cite">OpenAI 2024</a>;
                        <a href="#ref-41" class="cite">Sevilla and Roldan 2024</a>), suggesting that remaining
                        compute capacity is allocated to other training activities rather than final model production.
                    </p>

                    <div class="table-container" id="table-2-2">
                        <table class="data-table">
                            <caption>Table 2.2: The far right columns present the estimated compute usage by large AI
                                models
                                relative to three baselines: the estimated compute usage of all public models from that
                                company
                                (Model / Public Models (<em>%</em>)), the estimated annual FLOP capacity for a year
                                running at
                                PEAK
                                (Model / Peak Annual(%)), and the former multiplied by 100x. For full details on this
                                table and
                                sources of data which feed it, please see Appendix II.</caption>
                            <thead>
                                <tr>
                                    <th>Model</th>
                                    <th>Organization</th>
                                    <th>Lab/Cloud</th>
                                    <th>Train <br> FLOPs</th>
                                    <th>Parent Org Peak <br> Annual FLOPs</th>
                                    <th>Model/Public <br> Models (<em>Lab/Cloud</em>)</th>
                                    <th>Model/Peak <br>Annual (<em>%</em>)</th>
                                    <th>Model/Peak <br>w/100x (<em>%</em>)</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Gemini 1.0 Ultra</td>
                                    <td>Google DeepMind</td>
                                    <td>Google DeepMind</td>
                                    <td>$5.00 \times 10^{25}$</td>
                                    <td>$3.87 \times 10^{28}$</td>
                                    <td>45.65</td>
                                    <td>0.129</td>
                                    <td>12.93</td>
                                </tr>
                                <tr>
                                    <td>Claude 3.5 Sonnet</td>
                                    <td>Anthropic</td>
                                    <td>Anthropic/Amazon</td>
                                    <td>$4.98 \times 10^{25}$</td>
                                    <td>$2.27 \times 10^{28}$</td>
                                    <td>69.74</td>
                                    <td>0.220</td>
                                    <td>21.96</td>
                                </tr>
                                <tr>
                                    <td>GPT-4o</td>
                                    <td>OpenAI</td>
                                    <td>Microsoft/OpenAI</td>
                                    <td>$3.81 \times 10^{25}$</td>
                                    <td>$4.35 \times 10^{28}$</td>
                                    <td>53.36</td>
                                    <td>0.088</td>
                                    <td>8.75</td>
                                </tr>
                                <tr>
                                    <td>Llama 3.1-405B</td>
                                    <td>Meta AI</td>
                                    <td>Meta AI</td>
                                    <td>$3.80 \times 10^{25}$</td>
                                    <td>$5.65 \times 10^{28}$</td>
                                    <td>66.32</td>
                                    <td>0.067</td>
                                    <td>6.72</td>
                                </tr>
                                <tr>
                                    <td>GPT-4</td>
                                    <td>OpenAI</td>
                                    <td>Microsoft/OpenAI</td>
                                    <td>$2.10 \times 10^{25}$</td>
                                    <td>$4.35 \times 10^{28}$</td>
                                    <td>29.41</td>
                                    <td>0.048</td>
                                    <td>4.82</td>
                                </tr>
                                <tr>
                                    <td>Gemini 1.0 Pro</td>
                                    <td>Google DeepMind</td>
                                    <td>Google DeepMind</td>
                                    <td>$1.83 \times 10^{25}$</td>
                                    <td>$3.87 \times 10^{28}$</td>
                                    <td>16.71</td>
                                    <td>0.047</td>
                                    <td>4.73</td>
                                </tr>
                                <tr>
                                    <td>Claude 3 Opus</td>
                                    <td>Anthropic</td>
                                    <td>Anthropic/Amazon</td>
                                    <td>$1.64 \times 10^{25}$</td>
                                    <td>$2.27 \times 10^{28}$</td>
                                    <td>22.97</td>
                                    <td>0.072</td>
                                    <td>7.23</td>
                                </tr>
                                <tr>
                                    <td>Gemini 1.5 Pro</td>
                                    <td>Google DeepMind</td>
                                    <td>Google DeepMind</td>
                                    <td>$1.58 \times 10^{25}$</td>
                                    <td>$3.87 \times 10^{28}$</td>
                                    <td>14.43</td>
                                    <td>0.041</td>
                                    <td>4.09</td>
                                </tr>
                                <tr>
                                    <td>Llama 3-70B</td>
                                    <td>Meta AI</td>
                                    <td>Meta AI</td>
                                    <td>$7.86 \times 10^{24}$</td>
                                    <td>$5.65 \times 10^{28}$</td>
                                    <td>13.72</td>
                                    <td>0.014</td>
                                    <td>1.39</td>
                                </tr>
                                <tr>
                                    <td>GPT-4o mini</td>
                                    <td>OpenAI</td>
                                    <td>Microsoft/OpenAI </td>
                                    <td>$7.36 \times 10^{24}$</td>
                                    <td>$4.35 \times 10^{28}$</td>
                                    <td>10.31</td>
                                    <td>0.017</td>
                                    <td>1.69</td>
                                </tr>
                                <tr>
                                    <td>PaLM 2</td>
                                    <td>Google</td>
                                    <td>Google DeepMind</td>
                                    <td>$7.34 \times 10^{24}$</td>
                                    <td>$3.87 \times 10^{28}$</td>
                                    <td>6.70</td>
                                    <td>0.019</td>
                                    <td>1.90</td>
                                </tr>
                                <tr>
                                    <td>Llama 3.3</td>
                                    <td>Meta AI</td>
                                    <td>Meta AI</td>
                                    <td>$6.86 \times 10^{24}$</td>
                                    <td>$5.65 \times 10^{28}$</td>
                                    <td>11.98</td>
                                    <td>0.012</td>
                                    <td>1.21</td>
                                </tr>
                                <tr>
                                    <td>Amazon Nova Pro</td>
                                    <td>Amazon</td>
                                    <td>Anthropic/Amazon</td>
                                    <td>$6.00 \times 10^{24}$</td>
                                    <td>$2.27 \times 10^{28}$</td>
                                    <td>8.40</td>
                                    <td>0.026</td>
                                    <td>2.65</td>
                                </tr>
                                <tr>
                                    <td>Amazon Titan</td>
                                    <td>Amazon</td>
                                    <td>Anthropic/Amazon</td>
                                    <td>$4.80 \times 10^{24}$</td>
                                    <td>$2.27 \times 10^{28}$</td>
                                    <td>6.72</td>
                                    <td>0.021</td>
                                    <td>2.12</td>
                                </tr>
                                <tr>
                                    <td>Claude 2</td>
                                    <td>Anthropic</td>
                                    <td>Anthropic/Amazon</td>
                                    <td>$3.87 \times 10^{24}$</td>
                                    <td>$2.27 \times 10^{28}$</td>
                                    <td>5.41</td>
                                    <td>0.017</td>
                                    <td>1.70</td>
                                </tr>
                                <tr>
                                    <td>Minerva (540B)</td>
                                    <td>Google</td>
                                    <td>Google DeepMind </td>
                                    <td>$2.74 \times 10^{24}$</td>
                                    <td>$3.87 \times 10^{28}$</td>
                                    <td>2.50</td>
                                    <td>0.007</td>
                                    <td>0.71</td>
                                </tr>
                                <tr>
                                    <td>GPT-3.5 (text-davinci-003)</td>
                                    <td>OpenAI</td>
                                    <td>Microsoft/OpenAI </td>
                                    <td>$2.58 \times 10^{24}$</td>
                                    <td>$4.35 \times 10^{28}$</td>
                                    <td>3.61</td>
                                    <td>0.006</td>
                                    <td>0.59</td>
                                </tr>
                                <tr>
                                    <td>U-PaLM (540B)</td>
                                    <td>Google</td>
                                    <td>Google DeepMind </td>
                                    <td>$2.53 \times 10^{24}$</td>
                                    <td>$3.87 \times 10^{28}$</td>
                                    <td>2.31</td>
                                    <td>0.007</td>
                                    <td>0.65</td>
                                </tr>
                                <tr>
                                    <td>PaLM (540B)</td>
                                    <td>Google Research</td>
                                    <td>Google DeepMind </td>
                                    <td>$2.53 \times 10^{24}$</td>
                                    <td>$3.87 \times 10^{28}$</td>
                                    <td>2.31</td>
                                    <td>0.007</td>
                                    <td>0.65</td>
                                </tr>
                                <tr>
                                    <td>Flan-PaLM 540B</td>
                                    <td>Google</td>
                                    <td>Google DeepMind </td>
                                    <td>$2.50 \times 10^{24}$</td>
                                    <td>$3.87 \times 10^{28}$</td>
                                    <td>2.28</td>
                                    <td>0.006</td>
                                    <td>0.65</td>
                                </tr>
                                <tr>
                                    <td>FLAN 137B</td>
                                    <td>Google Research</td>
                                    <td>Google DeepMind </td>
                                    <td>$2.05 \times 10^{24}$</td>
                                    <td>$3.87 \times 10^{28}$</td>
                                    <td>1.87</td>
                                    <td>0.005</td>
                                    <td>0.53</td>
                                </tr>
                                <tr>
                                    <td>Meta Movie Gen Video</td>
                                    <td>Meta AI</td>
                                    <td>Meta AI</td>
                                    <td>$2.65 \times 10^{24}$</td>
                                    <td>$5.65 \times 10^{28}$</td>
                                    <td>2.88</td>
                                    <td>0.003</td>
                                    <td>0.29</td>
                                </tr>
                                <tr>
                                    <td>Megatron-Turing NLG 530B</td>
                                    <td>Microsoft,NVIDIA</td>
                                    <td>Microsoft/OpenAI </td>
                                    <td>$1.17 \times 10^{24}$</td>
                                    <td>$4.35 \times 10^{28}$</td>
                                    <td>1.64</td>
                                    <td>0.003</td>
                                    <td>0.27</td>
                                </tr>
                                <tr>
                                    <td>Llama 2-70B</td>
                                    <td>Meta AI</td>
                                    <td>Meta AI </td>
                                    <td>$8.10 \times 10^{23}$</td>
                                    <td>$5.65 \times 10^{28}$</td>
                                    <td>1.41</td>
                                    <td>0.001</td>
                                    <td>0.14</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <p>
                        The contrast between frontier models consuming a small fraction of quarterly compute
                        budgets and ongoing infrastructure expansion suggests that leading AI firms train numerous
                        experimental models beyond their final deployments. This interpretation aligns with widely
                        documented practices in frontier AI laboratories. Major labs employ hundreds to thousands of
                        researchers who routinely train models during development. Standard optimization procedures,
                        such as hyperparameter sweeps, involve training single model architectures tens to hundreds of
                        times to identify optimal configurations (<a href="#ref-42" class="cite">Weights & Biases
                            2025</a>).
                    </p>

                    <p>
                        Taken together, producing a final frontier model requires training hundreds to thousands of
                        intermediate models during the R&D process. While hyperparameter optimization is essential
                        to model development, current approaches necessarily involve complete retraining for each
                        configuration. This contrasts with modular systems where components can be incrementally
                        optimized without discarding the entire structure. Recent work on targeted model modification
                        (e.g., LLM surgery (<a href="#ref-43" class="cite">Veldanda et al., 2024</a>)) suggests
                        alternatives to full retraining, but such
                        techniques have not been widely adopted in frontier model development, necessitating substantial
                        computational expenditure during optimization.
                    </p>

                    <p>
                        Consider the magnitude of this inefficiency. If frontier labs aim to produce general-purpose
                        models, then computational resources allocated to intermediate experimental models represent
                        overhead that does not directly contribute to final model capability. Based on <a
                            href="#table-2-2">Table 2.2</a>,
                        approximately 98.53% of annual training budgets are allocated to models other than the final
                        deployment (calculated as 100% − (0.22%/15%), where 15% represents the estimated fraction
                        of total compute dedicated to training (<a href="#ref-44" class="cite">Bratt 2025</a>)). Under
                        the objective of producing a
                        generalpurpose model, this implies that the vast majority of training compute is allocated to
                        parameters
                        that are ultimately discarded during the optimization process.
                    </p>

                    <p>
                        This annual estimate may understate total inefficiency, as it assumes frontier labs must train
                        at least one model from scratch annually. If model parameters could be efficiently reused across
                        generations (as RETRO/ATLAS demonstrate through their retrieval databases, where up to
                        98% of knowledge can be transferred between model versions (<a href="#ref-45"
                            class="cite">Borgeaud et al., 2022</a>; <a href="#ref-46" class="cite">Izacard
                            et al., 2023</a>)) the efficiency gap would be larger. However, retraining overhead
                        represents only
                        one source of computational waste. A second source arises from how models process information
                        during training. To observe the theory behind this phenomena, consider the following analogy.
                    </p>

                    <div class="callout">
                        <p class="callout-title">A Library Analogy</p>

                        <p>
                            Once again, consider a library. When someone submits a new book to a library, the
                            librarian doesn’t subsequently read <em> every book in the library</em> to figure out where
                            to store
                            it on the shelves. Instead, they use the catalog system to find a relevant bookshelf, the
                            decimal system to locate the right placement on the shelf, and (perhaps) the title of the
                            book to find its alphabetical placement on that shelf.
                        </p>

                        <p>
                            This practice stands in stark contrast to how AI systems are trained. To add a <em>single
                                training example</em> into a model like GPT-3, AI users forward propagate <em>through
                                the
                                entire model and all of its knowledge</em> (i.e., read every book in the library). And
                            to train
                            an
                            entire model like GPT-3 on modern training corpuses (i.e. trillions of tokens), it repeats
                            this
                            process <em>trillions of times</em>. It is like a librarian who repeatedly reads every book
                            to figure
                            out
                            where a book should be placed, and then complains about not having enough librarian
                            assistants (i.e. GPU compute threads) to accomplish the task.
                        </p>
                    </div>

                    <p>
                        The question concerns the magnitude of this inefficiency. The RETRO and ATLAS results
                        previously discussed demonstrate that models can achieve comparable performance while being
                        25-50x smaller in parameter count. This parameter reduction translates directly to reduced
                        training costs: fewer parameters require proportionally fewer FLOPs during both forward and
                        backward propagation. The compression results further indicate that models can be trained with
                        5-10x fewer parameters without performance degradation, compounding the potential efficiency
                        gains.
                    </p>

                    <p>
                        Yet, these three sources of training inefficiency (retraining overhead, dense forward
                        propagation, and parameter redundancy) do not exhaust the potential efficiency gains. A fourth
                        source
                        arises from how models organize information during the training process itself.
                    </p>

                    <div class="callout">
                        <p class="callout-title">A Library Analogy</p>

                        <p>
                            Consider a brand new library. When a librarian goes to stock their library, they do not
                            necessarily store <em>every book in the universe</em> in their library. Instead, libraries
                            participate as a
                            part of a library <em>network</em>. In this way, a nation-wide (or even global) community of
                            libraries
                            each store a cache of books, and when one user asks their local library for a book they do
                            not
                            have, that library will call in that book from another library. Through this process, even
                            tiny,
                            rural libraries are (in a way) making a massive, global collection of knowledge available to
                            their local community. Some might even say that a small, local library makes all of
                            humanity’s knowledge available to their local community, even if their local collection is
                            small.
                        </p>

                        <p>
                            This practice stands in stark contrast to how AI systems are trained. Firms around the world
                            are scraping the internet (or downloading web scrapes) and training their own models
                            from scratch <em>largely on the same information scraped from the internet</em>. In these
                            practices,
                            they are encoding the same information redundantly across many organizations instead of
                            building upon the existing knowledge already encoded into neural weights by other parties.
                            That is to say, AI companies repeat each others’ work to a great degree.
                        </p>
                    </div>

                    <p>
                        AI models store information within their weights. General-purpose models encode substantial
                        portions of publicly available internet data through training on large, overlapping corpora.
                        Multiple organizations train models on similar or identical datasets, often drawn from common
                        sources such as web scrapes and public repositories. This results in redundant encoding of the
                        same information across independently trained models.
                    </p>

                    <div class="table-container" id="table-2-3">
                        <table class="data-table">
                            <caption>Table 2.3: Estimated Total Worldwide AI Computing Capacity (Q4 2024).</caption>
                            <thead>
                                <tr>
                                    <th>Category</th>
                                    <th>Computing Power <br> (FLOP/s)</th>
                                    <th>Share (%)</th>
                                    <th>Source/Calculation</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td colspan="3"><em>Cloud/AI Providers</em></td>
                                </tr>
                                <tr>
                                    <td>Meta</td>
                                    <td>$1.79 \times 10^{21}$</td>
                                    <td>5.57</td>
                                    <td>From <a href="appendix2.html#table-6.3">Table 6.3</a> Total Q4 2024</td>
                                </tr>
                                <tr>
                                    <td>Microsoft/OpenAI</td>
                                    <td>$1.38 \times 10^{21}$</td>
                                    <td>4.29</td>
                                    <td>From <a href="appendix2.html#table-6.3">Table 6.3</a> Total Q4 2024</td>
                                </tr>
                                <tr>
                                    <td>Google/DeepMind</td>
                                    <td>$1.23 \times 10^{21}$</td>
                                    <td>3.81</td>
                                    <td>From <a href="appendix2.html#table-6.3">Table 6.3</a> Total Q4 2024</td>
                                </tr>
                                <tr>
                                    <td>Amazon/Anthropic</td>
                                    <td>$7.19 \times 10^{20}$</td>
                                    <td>2.23</td>
                                    <td>From <a href="appendix2.html#table-6.3">Table 6.3</a> Total Q4 2024</td>
                                </tr>
                                <tr>
                                    <td colspan="3"><em>Consumer Computing</em></td>
                                </tr>
                                <tr>
                                    <td>Smartphones</td>
                                    <td>$7.48 \times 10^{21}$</td>
                                    <td>23.23</td>
                                    <td>Sum of Active iPhones/Androids from <a href="appendix2.html#table-6.5"> Table
                                            6.5</a></td>
                                </tr>
                                <tr>
                                    <td>PC CPUs/GPUs</td>
                                    <td>$2.23 \times 10^{21}$</td>
                                    <td>6.92</td>
                                    <td>Sum of PC CPUs and GPUs from <a href="appendix2.html#table-6.5"> Table 6.5</a>
                                    </td>
                                </tr>
                                <tr>
                                    <td>Game Consoles</td>
                                    <td>$8.64 \times 10^{20}$</td>
                                    <td>2.68</td>
                                    <td>From <a href="appendix2.html#table-6.5">Table 6.5</a></td>
                                </tr>
                                <tr>
                                    <td>Other Cloud/Pre-2023</td>
                                    <td>$1.65 \times 10^{22}$</td>
                                    <td>51.28</td>
                                </tr>
                                <tr class="baseline">
                                    <td><strong>Total</strong></td>
                                    <td>$3.22 \times 10^{22}$</td>
                                    <td>100.00</td>
                                    <td>Sum of all rows above</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <p>
                        From an information-theoretic perspective, this redundancy is inefficient; the question is:
                        how much? Comprehensive measurement would require documenting the overlap in training
                        data and model capabilities across organizations, information that is not systematically
                        available.
                    </p>

                    <p>
                        However, industry analysis provides order-of-magnitude estimates of aggregate underutilization.
                        The largest AI firm controls less than 5.57% of global computing capacity (<a
                            href="#table-2-3">Table 2.3</a>).
                        If training resources could be pooled across organizations (analogous to libraries participating
                        in a global network rather than maintaining independent collections) available compute
                        would increase by a factor of approximately 100/5.57 ≈ 17.95x relative to any single firm’s
                        capacity. This represents the theoretical gain from distributed training architectures that
                        enable
                        collaborative model development across organizational boundaries.
                    </p>

                    <p>
                        Yet, even these four training inefficiencies may not fully describe the inefficiency present in
                        modern AI. To observe the theory behind this next problem, consider the following analogy.
                    </p>

                    <div class="callout">
                        <p class="callout-title">A Library Analogy</p>

                        <p>
                            Consider a brand new library which doesn’t even have any books in it yet. Let’s say the
                            librarian is in a hurry, and so they take the first book, pick one of the empty shelves, set
                            the book on that shelf, and then run to get the second book. Then, looking at the second
                            book, they ask, ”is this similar to the first book... or different”. And if it’s similar to
                            the first
                            book, they put it closer to the first book on the shelves, and if it’s different from the
                            first
                            book, they put it farther away. This librarian repeats this process over and over until they
                            encounter a problem. After 10,000 books (out of the millions they have to load), one of the
                            bookshelves is full. So, because the shelf they need is full, they run to the next shelf and
                            <em> empty it... throwing books onto the floor</em> to make space for their new book, which
                            needs to
                            be in this location. But now, they need to re-stock the books they just threw on the floor!
                            They then pick up the books from the floor and attempt to find them all new places in the
                            library, accidentally filling up shelves in the process. They then repeat this process many,
                            many times... stocking and re-stocking all the books until a sensible organization emerges.
                        </p>

                        <p>
                            Or consider another librarian, who is opening a new library. But before they begin stocking
                            books, they set up a Dewey Decimal System. They label each book in
                            the system, count the number of books in each category, and plan their shelf capacity
                            appropriately. Then, they take each book and load it into its appropriate shelf in a single
                            pass. This second technique stands in stark contrast to how AI systems are trained. To add
                            <em> the first training example</em> into an untrained model like GPT-3, AI users forward
                            propagate
                            <em>through the entire model and all of its knowledge</em> and store that information in
                            random
                            locations throughout the model. Then, as more training examples pile into the model, the
                            model experiences <em>catastrophic forgetting</em> (<a href="#ref-47"
                                class="cite">Kirkpatrick et al., 2017</a>) as collisions
                            occur.
                            And to train an entire model like GPT-3 on modern training corpuses (i.e. trillions of
                            tokens), it repeats this process <em>trillions of times</em>.
                        </p>
                    </div>

                    <p>
                        Quantifying the computational cost of catastrophic forgetting remains challenging due to
                        limited empirical work on information segmentation during training. RETRO and ATLAS
                        provide partial evidence, as does work on curriculum learning and knowledge distillation.
                        Kemker et al. observe that avoiding catastrophic forgetting requires approximately 40x larger
                        model capacity (<a href="#ref-48" class="cite">Kemker et al., 2018</a>), though this estimate is
                        not especially recent. Multiple
                        sources of training inefficiency compound: full model retraining during updates, dense forward
                        propagation through all parameters, parameter redundancy from insufficient compression, and
                        capacity overhead to prevent catastrophic forgetting during sequential training.
                    </p>

                    <p>
                        These inefficiencies compound multiplicatively in terms of FLOPs. Parameter reuse across
                        model generations could increase compute productivity by a factor of 100/(100 − 98.53%) ≈
                        68x. RETRO/ATLAS-demonstrated parameter efficiency provides 25-50x gains. Lossless
                        compression techniques offer an additional 5-10x reduction. Catastrophic forgetting avoidance
                        inflates model sizes by approximately 40x. Combined multiplicatively, these factors suggest
                        potential efficiency gains ranging from (68 × 25 × 5 × 17.95) ≈ 153,000x to (68 × 50 × 10 ×
                        17.95 × 40) ≈ 24,400,000x, representing approximately 5-7 orders of magnitude of potential
                        compute productivity improvement.
                    </p>

                    <div class="table-container" id="table-2-4">
                        <table class="data-table">
                            <caption>Table 2.4: Summary of Major AI System Inefficiencies</caption>
                            <thead>
                                <tr>
                                    <th>Inefficiency Type</th>
                                    <th>Range</th>
                                    <th>Evidence</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td colspan="3"><em>Inference Inefficiencies:</em></td>
                                </tr>
                                <tr>
                                    <td>Full Forward Propagation</td>
                                    <td>25-50x+</td>
                                    <td>RETRO/ATLAS</td>
                                </tr>
                                <tr>
                                    <td>Parameter Redundancy</td>
                                    <td>5-10x+</td>
                                    <td>Compression</td>
                                </tr>
                                <tr>
                                    <td>Catastrophic Forgetting</td>
                                    <td>40x</td>
                                    <td>Size Heuristic</td>
                                </tr>
                                <tr>
                                    <td colspan="3"><em>Training Inefficiencies:</em></td>
                                </tr>
                                <tr>
                                    <td>Re-training from Scratch</td>
                                    <td>68x+</td>
                                    <td>Industry Analysis</td>
                                </tr>
                                <tr>
                                    <td>Full Forward Propagation</td>
                                    <td>25-50x+</td>
                                    <td>RETRO/ATLAS</td>
                                </tr>
                                <tr>
                                    <td>Parameter Redundancy</td>
                                    <td>5-10x+</td>
                                    <td>Compression</td>
                                </tr>
                                <tr>
                                    <td>Siloed Compute</td>
                                    <td>~17.95x</td>
                                    <td>Global Compute</td>
                                </tr>
                                <tr>
                                    <td>Catastrophic Forgetting</td>
                                    <td>40x</td>
                                    <td>Size Heuristic</td>
                                </tr>
                                <tr>
                                    <td colspan="3"><em>Combined Effects:</em></td>
                                </tr>
                                <tr class="highlight">
                                    <td>Inference Total</td>
                                    <td>5,000-20,000x+</td>
                                    <td>Multiplicative</td>
                                </tr>
                                <tr class="highlight">
                                    <td>Training Total</td>
                                    <td>6,103,000-24,412,000x+</td>
                                    <td>Multiplicative</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <p>
                        Taken together, these estimates suggest that current inference practices exhibit inefficiency
                        factors of approximately 5,000-20,000x, while training practices exhibit inefficiency factors
                        of approximately 150,000-24,000,000x. These bounds support the conclusion that at least six
                        orders of magnitude of compute productivity remains unexploited in current AI systems.
                    </p>

                    <p>
                        Yet even these estimates contain within them one especially conservative estimate, the 25-50x
                        inefficiency from dense forward propagation. While RETRO/ATLAS provides the only concrete
                        lower bound on this inefficiency, the true sparsity opportunity is almost certainly
                        significantly
                        more. To return to the library analogy, what percentage of the world’s collective library is
                        needed
                        to answer a particular question? If one believes that 2-4 percent of all human knowledge is
                        needed for every question, then perhaps the RETRO/ATLAS estimate is accurate.
                    </p>

                    <p>
                        While systematic measurement is lacking, the assumption that any query requires more
                        than one-millionth of a model’s knowledge base (> 10−6
                        ) appears conservative, suggesting
                        these efficiency estimates may understate potential gains by an additional 4 orders of magnitude
                        (although this is merely conjecture... future empirical work is needed).
                    </p>

                    <div class="callout">
                        <p class="callout-title">A Full Picture of Compute Waste: The Library Analogy</p>

                        <p>
                            Consider first how an AI system would operate as a librarian. When someone asks about
                            the rules of chess, this librarian doesn’t merely consult the games section. Instead, they
                            read <em>every single book in the library</em>. Not just once, they do this for <em>every
                                single query</em>.
                            When this AI librarian needs to add a new book to their collection, they don’t simply locate
                            an appropriate shelf using a catalog system. Instead (and this characterizes a fundamental
                            inefficiency in current AI systems) they first read <em>every book in the library</em>, then
                            displace
                            existing books onto the floor to make space, then must <em>re-read everything</em> to
                            determine
                            where to relocate those displaced books. This process repeats, sometimes <em>trillions of
                                times</em>, until the library reaches a new equilibrium. Worse, if a book needs to be
                            decisively
                            removed, the entire library must be burned to the ground, all of the books burned, and a
                            new library constructed from scratch, repeating the entire aformentioned process over again.
                        </p>

                        <p>
                            Furthermore, this AI librarian doesn’t participate in an efficient network of libraries.
                            Instead, they insist on maintaining their own complete copy of <em>every book in
                                existence</em>,
                            greatly amplifying the challenge of the aformentioned processes. When other AI librarians
                            open new libraries, they too download and store the same vast collection, redundantly
                            encoding identical information in countless separate locations (but also re-paying the cost
                            of learning how to organize their libraries). And when these AI librarians need to modify
                            their collections (to add or remove significant knowledge) they don’t merely reorganize
                            their existing structure. Instead, they <em>retrain from scratch</em>... also equivalent to
                            burning
                            their entire library to the ground, demolishing every book, and rebuilding the complete
                            collection from the ground up (repaying all of the aformentioned costs).
                        </p>

                        <p>
                            Now consider how human librarians process information. When someone inquires about
                            chess, they navigate directly to the games section, select a relevant text, and locate the
                            rules. When adding a new book, they utilize the Dewey Decimal system to identify
                            the appropriate shelf and place it there. The process is direct, efficient, and purposeful.
                            Moreover, human librarians don’t attempt to store every book in existence in their local
                            library. Instead, they participate in an interconnected system of libraries, each
                            maintaining
                            their own cache of books. When a patron requests a book not locally available, the librarian
                            simply requests it from another library in the network. Through this elegant system, even
                            the smallest rural library can provide access to humanity’s collective knowledge.
                        </p>
                    </div>

                    <p>
                        The contrast between these approaches illuminates a critical insight about current AI systems.
                        They operate with a level of inefficiency that we’ve somehow normalized within the field. In
                        essence, what we’ve built is a global network of millions of librarians who must read their
                        entire
                        library just to fetch a single book, read it again to add a new book, and then read it countless
                        more times to relocate all the books they displaced in the process. And when they’re not doing
                        this trillions of times over, they’re burning their buildings to the ground, destroying their
                        entire
                        collections, and starting over from scratch. And as the world’s AI compute costs approach the
                        level of a small nation, this practice, despite being ubiquitous within AI research, represents
                        perhaps the greatest inefficiency in the history of information processing. And via this
                        chapter,
                        this thesis will describe existing innovations which could alleviate much of this inefficiency.
                    </p>

                    <figure class="full-width">
                        <img src="1411.3146/abc_ch2_2_2_v5.png" alt="Data siloing tree">
                        <figcaption>A tree of sub-problems regarding data siloing.</figcaption>
                    </figure>

                    <h3 id="siloed-data">6+ OOM: Siloed Data</h3>

                    <p>
                        Following growing rumors across the AI research community that data is becoming a major
                        bottleneck, OpenAI’s former chief scientist, Ilya Sutskever, announced during his test of time
                        award speech at NeurIPS 2024 that data for training AI has peaked, ”We’ve achieved peak data
                        and there’ll be no more” (<a href="#ref-49" class="cite">Robison 2024</a>). However, while this
                        may be true for the AI industry,
                        and Ilya (being recent Chief Scientist at OpenAI) is perhaps one of the best people in the world
                        to know, Ilya’s statement does not reflect the reality of what data exists in the world.
                    </p>

                    <div class="callout">
                        <p class="callout-title">A Library Analogy</p>

                        <p>
                            Consider a world where libraries could only acquire books through anonymous donations
                            left on their doorstep. No matter how many valuable books exist in private collections,
                            university archives, or government repositories, libraries would be limited to what people
                            voluntarily abandon. In such a world, librarians might reasonably conclude they’re
                            ”running out of books”, even while surrounded by vast, inaccessible collections within
                            surrounding businesses and homes.
                        </p>

                        <p>
                            This mirrors the current state of AI training. When frontier models like GPT-4
                            (trained on 6.5 trillion tokens), and Qwen2.5-72B (18 trillion tokens), LLama 4 (30 trillion
                            tokens), (<a href="#ref-50" class="cite">Epo</a>) report hitting data limits, they’re really
                            hitting access limits. They’re
                            not
                            running out of data, they’re running out of data they can freely collect.
                        </p>
                    </div>

                    <h3>2-4 Orders of Magnitude: Text Humans Create By Hand</h3>

                    <p>
                        Dataset sizes for frontier AI models range from publicly disclosed values to industry estimates.
                        GPT-4 was trained on approximately 6.5 trillion tokens, while Alibaba’s Qwen2.5-72B used 18
                        trillion tokens. The largest reported text dataset, used for Meta’s Llama 4, contains 30
                        trillion
                        tokens (<a href="#ref-37" class="cite">Epoch AI 2025</a>). Using RedPajama as a reference
                        (<a href="#ref-51" class="cite">Together 2023</a>), each trillion tokens
                        requires less than 6TB of storage, implying that the largest known training dataset (Llama 4)
                        occupies less than 180TB.
                    </p>

                    <div class="table-container" id="table-2-5">
                        <table class="data-table">
                            <caption>Table 2.5: Estimated Volume of Public Digital Text Content (<a href="#ref-52"
                                    class="cite">Cummins 2024</a>)</caption>
                            <thead>
                                <tr>
                                    <th>Category &amp; Source</th>
                                    <th>Words (T)</th>
                                    <th>Tokens (T)</th>
                                    <th>Rel. Size*</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td colspan="4"><em>Web Data</em></td>
                                </tr>
                                <tr>
                                    <td>FineWeb</td>
                                    <td>11</td>
                                    <td>15</td>
                                    <td>1.0</td>
                                </tr>
                                <tr>
                                    <td>Non-English Common Crawl (high quality)</td>
                                    <td>13.5</td>
                                    <td>18</td>
                                    <td>1.0</td>
                                </tr>
                                <tr>
                                    <td>All high quality web text</td>
                                    <td>45-120</td>
                                    <td>60-160</td>
                                    <td>4.0-11.0</td>
                                </tr>
                                <tr>
                                    <td colspan="4"><em>Code</em></td>
                                </tr>
                                <tr>
                                    <td>Public code</td>
                                    <td>-</td>
                                    <td>0.78</td>
                                    <td>0.05</td>
                                </tr>
                                <tr>
                                    <td>Private Code</td>
                                    <td>-</td>
                                    <td>20</td>
                                    <td>1.3</td>
                                </tr>
                                <tr>
                                    <td colspan="4"><em>Academic Publications and Patents</em></td>
                                </tr>
                                <tr>
                                    <td>Academic articles</td>
                                    <td>0.8</td>
                                    <td>1</td>
                                    <td>0.07</td>
                                </tr>
                                <tr>
                                    <td>Patents</td>
                                    <td>0.15</td>
                                    <td>0.2</td>
                                    <td>0.01</td>
                                </tr>
                                <tr>
                                    <td colspan="4"><em>Books</em></td>
                                </tr>
                                <tr>
                                    <td>Google Books</td>
                                    <td>3.6</td>
                                    <td>4.8</td>
                                    <td>0.3</td>
                                </tr>
                                <tr>
                                    <td>Anna's Archive</td>
                                    <td>2.8</td>
                                    <td>3.9</td>
                                    <td>0.25</td>
                                </tr>
                                <tr>
                                    <td>Every unique book</td>
                                    <td>16</td>
                                    <td>21</td>
                                    <td>1.4</td>
                                </tr>
                                <tr>
                                    <td colspan="4"><em>Court Documents</em></td>
                                </tr>
                                <tr>
                                    <td>US federal court documents</td>
                                    <td>2</td>
                                    <td>2.7</td>
                                    <td>0.2</td>
                                </tr>
                            </tbody>
                            <tfoot>
                                <tr>
                                    <td colspan="4"><small>*Relative size using Llama 3 = 1 as reference</small></td>
                                </tr>
                            </tfoot>
                        </table>
                    </div>

                    <p>
                        The scale of untapped data is staggering. As shown in Tables <a href="#table-2-5">2.5</a> and <a
                            href="#table-2.6">2.6</a>, stored email and
                        instant messages alone contain over 1,850 trillion tokens, approximately 60 times the largest
                        known training dataset (<a href="#ref-53" class="cite">Cummins 2024</a>). Daily human
                        communication generates approximately
                        150 trillion tokens, accumulating to roughly 55 quadrillion tokens annually (approximately
                        1,750 times the scale of frontier training sets).
                    </p>

                    <h3>6+ Orders of Magnitude: Multi-media data broadly</h3>

                    <p>
                        Yet even this vast sea of text represents merely a drop in the ocean of total digital data.
                        While
                        frontier AI models train on curated web scrapes such as Common Crawl (454 TB as of December
                        2023) (<a href="#ref-54" class="cite">Wikipedia contributors 2024</a>), the Internet Archive’s
                        Wayback Machine alone stores
                        approximately 100 petabytes (<a href="#ref-55" class="cite">Kahle 2024</a>). Meanwhile, global
                        digital data is projected to reach
                        180 zettabytes by 2025 (<a href="#ref-56" class="cite">Mider 2024</a>; <a href="#ref-57"
                            class="cite">Taylor 2024</a>), six orders
                        of magnitude larger than The
                        Internet Archive and nine orders of magnitude larger than the largest known training datasets.
                    </p>

                    <div class="table-container" id="table-2.6">
                        <table class="data-table">
                            <caption>Table 2.6: Estimated Volume of Text Data (<a href="#ref-53" class="cite">Cummins
                                    2024</a>)</caption>
                            <thead>
                                <tr>
                                    <th>Category &amp; Source</th>
                                    <th>Words (T)</th>
                                    <th>Tokens (T)</th>
                                    <th>Rel. Size*</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td colspan="4"><em>Social Media</em></td>
                                </tr>
                                <tr>
                                    <td>Twitter / X</td>
                                    <td>8</td>
                                    <td>11</td>
                                    <td>0.7</td>
                                </tr>
                                <tr>
                                    <td>Weibo</td>
                                    <td>29</td>
                                    <td>38</td>
                                    <td>2.5</td>
                                </tr>
                                <tr>
                                    <td>Facebook</td>
                                    <td>105</td>
                                    <td>140</td>
                                    <td>10.0</td>
                                </tr>
                                <tr>
                                    <td colspan="4"><em>Publicly Available Audio (Transcribed)</em></td>
                                </tr>
                                <tr>
                                    <td>YouTube</td>
                                    <td>5.2</td>
                                    <td>7</td>
                                    <td>0.5</td>
                                </tr>
                                <tr>
                                    <td>TikTok</td>
                                    <td>3.7</td>
                                    <td>4.9</td>
                                    <td>0.3</td>
                                </tr>
                                <tr>
                                    <td>All podcasts</td>
                                    <td>0.56</td>
                                    <td>0.75</td>
                                    <td>0.05</td>
                                </tr>
                                <tr>
                                    <td>Television archives</td>
                                    <td>0.05</td>
                                    <td>0.07</td>
                                    <td>0.001</td>
                                </tr>
                                <tr>
                                    <td>Radio archives</td>
                                    <td>0.5</td>
                                    <td>0.6</td>
                                    <td>0.04</td>
                                </tr>
                                <tr>
                                    <td colspan="4"><em>Private Data</em></td>
                                </tr>
                                <tr class="highlight">
                                    <td>All stored instant messages</td>
                                    <td>500</td>
                                    <td>650</td>
                                    <td>45.0</td>
                                </tr>
                                <tr class="highlight">
                                    <td>All stored email</td>
                                    <td>900</td>
                                    <td>1200</td>
                                    <td>80.0</td>
                                </tr>
                                <tr>
                                    <td colspan="4"><em>Total Human Communication</em></td>
                                </tr>
                                <tr>
                                    <td>Daily</td>
                                    <td>115</td>
                                    <td>150</td>
                                    <td>10</td>
                                </tr>
                                <tr>
                                    <td>Since 1800</td>
                                    <td>3,000,000</td>
                                    <td>4,000,000</td>
                                    <td>$10^5$</td>
                                </tr>
                                <tr>
                                    <td>All time</td>
                                    <td>6,000,000</td>
                                    <td>8,000,000</td>
                                    <td>$10^5$</td>
                                </tr>
                            </tbody>
                            <tfoot>
                                <tr>
                                    <td colspan="4"><small>*Relative size using Llama 3 = 1 as reference</small></td>
                                </tr>
                            </tfoot>
                        </table>
                    </div>

                    <div class="callout">
                        <p class="callout-title">A Library Analogy</p>

                        <p>
                            Consider a national library system. While a single library might proudly maintain millions
                            of books, this represents only a tiny fraction of all written human knowledge. Beyond
                            its walls lie vast corporate archives, government repositories, university collections, and
                            personal libraries. To get a sense of scale &mdash; consider the size of a library’s
                            physical
                            building, and compare that to the size of the rest of the physical buildings in a city
                            &mdash;
                            each
                            with books and letterboxes and filing cabinets containing all manner of correspondence and
                            record. Each holds unique and valuable information, yet remains inaccessible to the library
                            system not because of physical constraints, but because of attribution and control concerns.
                        </p>

                        <p>
                            Similarly, when AI companies claim to have ”reached peak data,” they’re really
                            saying they’ve exhausted what they can freely obtain (often without permission or
                            attribution). The actual digital data of the world &mdash; in private databases, corporate
                            systems,
                            government archives, and personal devices &mdash; remains largely untapped, representing
                            over
                            six orders of magnitude more information than current AI systems can access.
                        </p>
                    </div>

                    <p>
                        The magnitude of this disparity is difficult to comprehend. While the largest known AI
                        dataset (to the awareness of this researcher) is roughly 180 TB of information, and may be
                        derived from a dataset as big as common crawl (450 TBs of information), or if we were very, very
                        conservative, might be as big as the full history of the entire publicly indexable internet (and
                        other
                        data the Internet Archive keeps) 100 PBs, even this number is <em>6+ orders of magnitude
                            smaller</em>
                        than the amount of digital data in the world. Taken together, even under highly conservative
                        estimates, it is very likely that AI has not yet trained on even one millionth of the amount of
                        data that humanity has digitized. And beyond what humanity has digitized lies the vast amounts
                        of information which is not yet encoded into a computer.
                    </p>

                    <p>
                        Consider the 150 trillion tokens humans create every day, the zettabytes-worth of
                        yet-to-bevideoed information going on across the planet and all of its inhabitants which
                        (despite helping
                        living creatures get smarter day-in-and-day-out) is completely inaccessible to systems which
                        only read digital information. This striking disparity between used and available data raises a
                        crucial question: why, in an era of unprecedented digital abundance, do AI systems train on such
                        a microscopic fraction of all knowledge? The answer lies not in easily observable symptoms
                        like data availability, but in fundamental problems underpinned by insufficient ABC.
                    </p>
                </section>

                <section id="root-causes">
                    <h3>The Search for Root Causes (Three "Whys")</h3>

                    <figure class="full-width">
                        <img src="1411.3146/abc_ch2_2_3_v6.png" alt="Cascade of causes">
                        <figcaption>The cascade of causes between the addition problem and ABC.</figcaption>
                    </figure>

                    <p>
                        The previous section revealed a paradox: despite widespread beliefs of data and compute
                        scarcity, AI systems access less than one millionth of digital resources, and an untold
                        microfraction of the world’s information broadly. This under-utilization raises a critical
                        question:
                        if more data and compute directly improves AI capabilities through scaling laws, why do AI
                        systems use such a tiny fraction of what’s available? The answer lies in a cascade of technical
                        and institutional barriers, each revealing a deeper ”why” that must be understood:
                    </p>

                    <ul>
                        <li><strong>First Why:</strong> Attribution-based Control</li>
                        <li><strong>Second Why:</strong> Deep Learning's Feature Mixing Precludes Partitioning</li>
                        <li><strong>Third Why (Root Cause):</strong> Addition of Source-Separable Concepts</li>
                    </ul>

                    <p>
                        As we follow this chain of questions, we’ll see how each answer reveals a deeper technical
                        challenge. More importantly, we’ll discover how recent breakthroughs in cryptography, deep
                        learning, and distributed systems have already created solutions to these challenges (solutions
                        which remain largely unrecognized by the AI community).
                    </p>
                </section>

                <section id="first-why">
                    <h2>First Why: Attribution-based Control</h2>

                    <figure class="full-width">
                        <img src="1411.3146/abc_ch2_3_0_v5.png" alt="First Why graph">
                        <figcaption>Subset of conceptual graph regarding Section 2.3</figcaption>
                    </figure>

                    <p>
                        The previous section revealed significant inefficiencies in the training of AI systems: 6+
                        orders of magnitude in underutilized data and compute. While there may be multiple contributing
                        factors to these constraints, this thesis and chapter examines one particular root cause: AI’s
                        inability to provide <a href="https://attribution-based-control.ai/">attribution-based
                            control</a> (ABC). An AI model possesses attribution-based
                        control when two properties are true: data sources control which AI predictions they support, AI
                        users control which data sources they rely upon for an AI prediction. Following this definition
                        (Definition <a href="index.html#definition-1-1-1">1.1.1</a>), ABC implies certain architectural
                        properties as novel requirements:
                    </p>

                    <ul>
                        <li><strong>Source-Partitionable Representations:</strong> Knowledge within an AI system is
                            partition-able
                            by source, otherwise sources lose control when their information is mixed, requiring:

                            <ul>
                                <li>
                                    <strong> Source-Partitionable Inference:</strong> Partitions are independently
                                    usable at inference,
                                    otherwise users can’t select specific sources and sources can’t participate
                                    irrespective
                                    of the decisions of other sources
                                </li>

                                <li>
                                    <strong> Source-Partitionable Training: </strong> Partitions are independently
                                    trainable, otherwise
                                    sources can’t update their contributions without requiring other sources to do so.
                                </li>
                            </ul>
                        </li>

                        <li><strong>Rapid Partition Synthesis:</strong> Partitions are rapidly synthesize-able during
                            inference,
                            otherwise collective insights which are only learnable via information from multiple
                            sources cannot be realized in production AI systems.</li>
                    </ul>

                    <p>
                        Frontier AI systems lack these properties. Yet, if these properties were achieved, attribution
                        would be achieved and aforementioned problems regarding data and compute productivity would
                        be impacted. Let’s examine each in detail, linking each problem to attribution-based control.
                    </p>

                    <h3>ABC and Compute Productivity (6+ OOM)</h3>

                    <p>
                        ABC would address compute productivity issues along two dimensions: access and learning
                        structure. Regarding structure, successful ABC would necessarily provide a means to structure
                        the learning and representation process, reducing re-training, forward propagation, redundancy,
                        and catastrophic forgetting. Regarding access, ABC would provide a means to overcome
                        incentive issues presently siloing the world’s compute resources. Let us consider these claims
                        in
                        the context of inference and learning.
                    </p>

                    <h3>
                        2-3 OOM: Inefficient AI inference
                    </h3>

                    <div class="callout">
                        <p class="callout-title">A Library Analogy</p>

                        <p>
                            Consider a library wherein a librarian reads every book in the library whenever they need
                            to answer a question, including reading multiple copies of the same book and pretending to
                            read an empty book whenever shelves contain empty sections.
                        </p>

                        <p>
                            A solution to attribution-based control would necessarily reconfigure the library to
                            fetch information based on its source (book or author). While such a solution might be
                            challenging, if it successfully delivered capable predictions, it would necessarily do so
                            while skipping an enormous amount of wasted computation. This is because ABC is not
                            actually about selecting the sources one desires (normal full forward propagation already
                            does this), its actually about <em>ignoring all the books you don’t want</em>. ABC requires
                            making
                            an AI inference while skipping an enormous amount of wasted computation, because
                            (to return to the analogy) it would involve training the librarian on how to skip reading
                            the entire library when making a prediction... and only fetch information from specific,
                            relevant sources (i.e. books).
                        </p>

                        <p>
                            Thus, while ABC’s definition might not appear to require an increase in computational
                            efficiency, its definition directly requires that a vast amount of information not be
                            included in the computational process, reducing the need to compute over that information.
                        </p>
                    </div>

                    <p>
                        Recall that current AI models must activate vast numbers of parameters for every prediction
                        because they struggle to pre-identify which parameters are storing relevant information to the
                        present query; they struggle with sparse forward propagation. While various approaches to
                        sparse AI exist, successful ABC would necessarily enable one particularly compelling form of
                        sparsity: source-based sparsity (i.e. source-based partitioning). Consequently, if an AI model
                        could forward predict based on a relevant subset (i.e., the top 10) of many (i.e. billions) of
                        sources, then it is very likely to also greatly reduce the computational complexity involved in
                        forward propagating, because it would be forward propagating through vastly less information.
                    </p>

                    <p>
                        RETRO and ATLAS demonstrate the minimum scale of such a breakthrough. By maintaining
                        source-based partitions through their database architecture, they achieve equal performance
                        while activating only 2-4% of the parameters of similarly performant dense models (<a
                            href="#ref-30" class="cite">Borgeaud
                            et al., 2022</a>; <a href="#ref-46" class="cite">Izacard et al., 2023</a>).
                    </p>

                    <p>
                        Similarly, recall how current models process some combination of redundant copies of
                        knowledge and empty regions of parameter space during inference. While there are many
                        approaches to reducing these forms of waste (e.g., distillation, compression, etc.), successful
                        ABC would necessarily enable one particularly compelling way to avoid copies or empty space:
                        source-based partitioning. If an AI model could forward predict based on a relevant subset
                        (i.e.,
                        the top 10) of many (i.e., billions) of sources, then tuning the number of sources being relied
                        upon could also tune the redundancy being used in forward propagation. Meanwhile, ensuring
                        that the partitions being leveraged for forward propagation were relevant to the query would
                        combat the risk of forward propagating empty space <a href="#sidenote-2"
                            class="sidenote-ref"><sup>2</sup></a>.
                    </p>

                    <p>
                        RETRO and ATLAS demonstrate these principles through their database architecture, showing how
                        source-based organization can naturally eliminate redundant processing and avoid
                        computation on irrelevant parameters while maintaining model capability. They also demonstrate
                        the ability for such partitioning to increase the number of samples being used against a
                        dense (i.e. non-partitioned) section of the network, reducing empty space. Yet as significant as
                        these inference inefficiencies are, they pale in comparison to the waste in how AI systems
                        learn.
                    </p>

                    <h3>6+ OOM: Underutilized and Inefficient Compute in AI Learning</h3>

                    <p>
                        Recall that current AI models must retrain entirely from scratch when updating their knowledge,
                        because of problems such as catastrophic forgetting (<a href="#ref-48" class="cite">Kemker et
                            al., 2018</a>). While various
                        approaches to incremental training exist, successful ABC would necessitate one particular
                        solution: source-partitioned retraining. Consequently, if an AI model can train source-separated
                        subsections of its weights, it can re-train them as well, and it is very likely to also greatly
                        reduce
                        the computational complexity involved in the re-training process, because it would be
                        re-training
                        vastly fewer parameters at a time. RETRO/ATLAS demonstrate one such approach, wherein
                        re-training can be done with the computational complexity involved in adding or removing
                        vector-embeddings from a database.
                    </p>

                    <p>
                        Similarly, recall how current training processes waste compute in multiple ways: through
                        redundant copies of the same knowledge, through activation of irrelevant parameters, and
                        through inefficient parameter density. While various approaches to training efficiency exist,
                        successful ABC would necessarily enable compelling means to overcome these problems (as
                        already described in the previous section describing ABC’s impacts on inference).
                    </p>

                    <p>
                        Additionally, these ABC opportunities further compound when we consider how compute is
                        distributed across organizations. Consider our library analogy once more: libraries don’t each
                        maintain copies of every book ever written; they form networks to share resources efficiently.
                        In contrast, frontier AI models are created by a host of organizations around the world, each
                        re-paying the cost of creating an AI model (most of which are trained on largely the same data).
                    </p>

                    <p>
                        Successful ABC may activate an economic incentive addressing this waste <a href="#sidenote-3"
                            class="sidenote-ref"><sup>3</sup></a>
                        . At the present
                        moment, frontier AI models are economic bundles, marketed under a story which itself is an
                        economic bundle: artificial general intelligence. Because of this, if one AI company re-trains
                        the
                        capabilities of another company (e.g. spending $200M on compute) and then adds a bit more
                        to it (e.g., another $10M in data and compute), an end user must then choose between that full
                        economic bundle and another full economic bundle. This creates a situation wherein companies
                        effectively have to pay the minimum amount to catch up to the leading position and then extend
                        some beyond it.
                    </p>

                    <div class="sidenote" id="sidenote-2">
                        <p>
                            <sup>2</sup>
                            Additionally, if successful ABC reduced the number of parameters being used as a result of
                            these other
                            techniques, then it would also increase the number of samples being applied to each set of
                            dense parameters &mdash; perhaps reducing the opportunity for empty vector space (more on
                            this later)
                        </p>
                    </div>

                    <div class="sidenote" id="sidenote-3">
                        <p>
                            <sup>3</sup>
                            This hypothesis has been somewhat validated by OpenMined in early pilots of ABC-enabled AI
                            systems with
                            publishers, but this research is as-of-yet unfinished
                        </p>
                    </div>

                    <div class="callout">
                        <p class="callout-title">A Library Analogy</p>

                        <p>
                            Consider a new library which doesn’t yet have any books. To load the library in the style of
                            AI, a librarian would first build tens-of-thousands of libraries and print copies of books
                            into
                            each one (i.e. signifying both the redundant training of models by many companies and the
                            hyperparameter sweeps occurring within each one). Then, within each library, a librarian
                            would first load all the shelves (of which there are a fixed number) books containing
                            random strings of letters (i.e., initialize a model randomly). Then, the librarian would
                            select
                            the first book to load into the library, pretend to read every word in every one of the
                            random
                            books, and after that was done, select which book to replace with the book being loaded
                            into the library, casting the book being replaced onto the floor. The process would repeat
                            until all of the books had been loaded... with a catch. Each time the book being thrown
                            on the floor wasn’t a random book (but was a <em>real</em> book) that real book then needs
                            to go
                            through the process again itself. And in the end, if the library wasn’t big enough to
                            contain
                            all the books, all the libraries would be destroyed, all the copies of books burned, and
                            everyone would re-build bigger libraries to hold the vast and growing collection of books.
                        </p>

                        <p>
                            A solution to attribution-based control would necessarily reconfigure the library to
                            load information based on its source (book or author). While such a solution might be
                            challenging, if it successfully delivered capable predictions, it would necessarily do so
                            while skipping an enormous amount of wasted computation. Instead of each library
                            creating a collection big enough to hold the world, the vast collection of the world’s books
                            could be divided among the libraries (perhaps with some mild redundancy). Each library
                            could then organize their books by the dewey decimal system, measuring how big each
                            section in their library needs to be in order to hold the sections they are meant to store.
                            After these measurements were completed, the building could be constructed, the books
                            loaded in their proper places, and the job could be completed.
                        </p>

                        <p>
                            And in this way, the librarian could avoid storing all the world’s information in
                            their own library, re-building libraries from scratch, reading all the books in the library
                            over
                            and over, loading in many copies of the same book, loading in empty or random books,
                            and re-loading books which no longer fit on the shelves they’re loading. Taken together,
                            while ABC’s definition might not appear to require an increase in computational efficiency,
                            its definition directly requires that a vast amount of information not be processed during
                            iterative steps in the training process, reducing the need to compute over that information.
                            In some cases, ABC implies the elimination of iterative processes altogether.
                        </p>
                    </div>

                    <p>
                        However, successful ABC would modularize the initial capability, such that some percentage
                        of the original $200M could be inherited from previous models and then extended with new
                        capabilities. Given the immense costs involved, such a modularization breakthrough would
                        constitute an enormous economic pressure. If firms pedantically chose to re-pay the cost to
                        train
                        their own from scratch, recreating 90% of the modules which already exist in the market, they
                        would incur very high costs which must correspond with very high prices to recoup those costs.
                        Inversely, a startup which came along and inherited the 90% produced by others, paying only for
                        their specialization, would (all else equal) be able to charge lower prices, and win in the
                        market.
                    </p>

                    <p>
                        From a compute efficiency standpoint, the prospect of cross-market weight-reuse translates
                        directly into the sharing of compute costs for training AI systems. Industry analysis reveals
                        the
                        potential impact: no single AI provider controls more than 5.57% of global compute capacity.
                        Thus, since source-based partitioning could unlock this siloed compute by enabling controlled
                        sharing of specialized knowledge, it could increase effective compute by 17.95x (100/5.57)
                        or more because organizations would waste fewer resources re-computing features which are
                        already commoditized in the market. Taken together, economic unbundling would plausibly
                        drive specialization and more efficient use of compute resources in the market.
                    </p>

                    <p>
                        While various approaches to these inefficiencies exist, solving ABC would necessarily
                        enable one comprehensive solution path. Note that this is not saying that ABC is the solution,
                        merely that ABC is difficult because it would involve solving these other difficult
                        challenges...
                        because ABC requires that sources maintain control over their contributions while enabling
                        rapid synthesis. Taken together, any solution to ABC solution must provide:
                    </p>

                    <ul>
                        <li>
                            Selective retraining instead of full rebuilding (68x+ improvement)
                        </li>

                        <li>
                            Efficient computation during training (25-50x+ improvement)
                        </li>

                        <li>
                            Reduced parameter redundancy through source-based organization (5-10x+ improvement)
                        </li>

                        <li>
                            Specialization with controlled sharing (17.95x+ improvement)
                        </li>

                        <li>
                            Organization averting catastrophic forgetting (17.95x+ improvement)
                        </li>
                    </ul>

                    <p>
                        Industry analysis and empirical results suggest the combined impact could be dramatic.
                        When these improvements compound multiplicatively, they point to potential training efficiency
                        gains of 6+ orders of magnitude. Yet these numbers, as striking as they are, point to something
                        more fundamental: our failure to maintain attribution in AI systems coincides with a broader
                        acceptance of wasteful practices as inevitable. More than a technical issue, the inefficiency of
                        AI
                        training is a symptom of how we’ve structured AI computation. While other solutions may exist,
                        attribution-based control offers one path to reimagining how AI systems learn and compute,
                        potentially unlocking orders of magnitude more efficiency in the process.
                    </p>

                    <h3>How Failing ABC Siloes Data (6 OOM)</h3>

                    <p>
                        Recall that current AI models can only train on data they can access, which is dominated by
                        data available to the public over the internet. Consequently, AI models almost certainly train
                        on
                        less than 1/1,000,000th of the digitized information in the world because they cannot access the
                        other 99.9999%, which remains hidden amongst the world’s 360 million companies, 8+ billion
                        citizens, etc. (<a href="#ref-58" class="cite">Bogwasi 2025</a>).
                    </p>

                    <p>
                        While various approaches to data access exist, successful ABC would necessarily enable
                        one compelling solution: controlled data sharing. We take as an assumption that the world’s
                        data owners have some uses in the world they would support (for which their data could be
                        useful). We take as a second assumption that a significant portion of those sources are hidden
                        because the incentives are not sufficient for them to support &mdash; or more crucially because
                        the
                        negative consequences would be too great, that their data might not just activate the uses they
                        wish to support, but that it would also activate mis-uses (concerns regarding privacy, security,
                        IP,
                        competition, legal risks, etc.).
                    </p>

                    <p>
                        Successful ABC would necessarily enable one particularly compelling form of data sharing.
                        The ability for a data source to decide which AI predictions to support is (almost
                        tautologically)
                        the ability for a data source to enable uses while averting mis-uses. Consequently, AI empowered
                        by ABC may activate vastly more data than is presently available. One could argue that truly
                        successful ABC would constitute an incentive shift attracting all of the world’s data to be
                        pulled
                        into at least some AI predictions.
                    </p>

                    <p>
                        The potential impact is staggering. While frontier AI models train on carefully curated
                        web scrapes on the order of 180 TBs (and the public web plausibly less than common crawl’s
                        largest copy, 450 TBs) the world’s total digital data is estimated to reach 180 zettabytes by
                        2025. This six-to-nine orders of magnitude difference represents all the data locked behind
                        organizational boundaries, including some data of immense value (e.g. financial data, health
                        data, environmental data, etc.).
                    </p>

                    <p>
                        RETRO and ATLAS demonstrate part of potential path forward, demonstrating a type of
                        partial ABC at scale by training AI models which query from a database. Certain extensions
                        (such as those suggested in this thesis) could take this further to enable full
                        attribution-based
                        control, and the shifting of incentives around data sharing.
                    </p>

                    <h3>Synthesis: Attribution as a Path Forward</h3>

                    <p>
                        The previous sections revealed two significant inefficiencies in current AI systems: 6+ orders
                        of
                        magnitude waste in compute and 6+ orders of magnitude in untapped data. While there may
                        be many approaches to addressing these inefficiencies, this thesis focuses on attribution-based
                        control as one solution path. As we’ve seen, solving ABC (if such a solution exists) would
                        necessarily enable both efficient compute through source-based partitioning and broad data
                        access through attribution preservation.
                    </p>

                    <p>
                        Yet this raises a deeper question: if ABC offers such compelling benefits, why don’t current
                        AI systems maintain attribution? The answer, as we’ll see in the next section, lies in how
                        neural
                        networks fundamentally process information. The unconstrained mixing of features during
                        training makes it impossible to partition knowledge by source, revealing our second ”why”:
                        deep learning’s feature mixing precludes the very partitioning that ABC requires.
                    </p>
                </section>

                <section id="second-why">
                    <h2>Second Why: Deep Learning's Feature Mixing Precludes Partitioning</h2>

                    <p>
                        The previous section revealed how solving attribution-based control would necessarily enable
                        massive data and compute gains in AI systems. Yet this raises a deeper question: why do current
                        AI systems fail to maintain attribution in the first place? The answer lies in deep learning’s
                        foundational premise: algorithms should learn everything from scratch through layers of
                        (largely)
                        unrestricted feature mixing on raw data (<a href="#ref-8" class="cite">Goodfellow et al.,
                            2016</a>).
                    </p>

                    <p>
                        This commitment to unrestricted learning manifests in how neural networks fundamentally
                        process information. Through operations that combine and mix information at every step
                        (from layer computations to weight updates to knowledge accumulation) neural networks create
                        increasingly complex representations of patterns in their training data. While this flexibility
                        enables powerful pattern recognition, it creates a fundamental problem: features become stored
                        in a disorganized, obfuscated way within the deep learning model... a black box.
                    </p>

                    <figure class="full-width">
                        <img src="1411.3146/abc_ch2_4_0_v5.png" alt="Second Why graph">
                        <figcaption>Subset of conceptual graph highlighting the Second Why and focus of this section.
                        </figcaption>
                    </figure>

                    <p>
                        Consider what happens when a neural network learns to recognize cats. Rather than storing
                        clear, interpretable features like ”pointy ears” or ”whiskers”, the network distributes this
                        knowledge across its weights in complex, entangled patterns (<a href="#ref-59" class="cite">Le
                            2013</a>). This unrestricted mixing
                        of
                        features makes post-hoc source-based partitioning impossible (at the present moment):
                    </p>

                    <ul>
                        <li>
                            Features can’t be attributed to specific sources (preventing data control)
                        </li>

                        <li>
                            Knowledge can’t be updated independently (requiring full retraining)
                        </li>

                        <li>
                            Computation can’t be selectively activated (forcing dense inference)
                        </li>

                        <li>
                            Resources can’t be efficiently shared (blocking specialization)
                        </li>
                    </ul>

                    <p>
                        The research community has made extensive efforts to address these limitations. Recent work
                        has attempted to trace predictions back to training data through influence functions, remove
                        specific datapoints’ influence through machine unlearning, and develop various attribution
                        methods to reverse engineer the source-prediction relationship (<a href="#ref-60"
                            class="cite">Nguyen et al., 2024</a>). Yet
                        despite these attempts, both influence functions and unlearning remain unsolved challenges in
                        the literature. So far, the relationship between sources and predictions has been irreversibly
                        compressed during training, and no amount of post-training intervention has successfully
                        restored
                        these lost connections.
                    </p>

                    <p>
                        The consequences are severe. New models like GPT-5 cannot inherit features from predecessors
                        like GPT-4... they must relearn basic patterns from scratch. Even during inference,
                        models must activate vast parameter spaces for every prediction, unable to pre-identify which
                        features are relevant (and only forward propagate those features). These inefficiencies aren’t
                        mere implementation details that clever algorithms might solve. They stem from something
                        more fundamental about how deep learning processes information.
                    </p>

                    <p>
                        Yet this raises an even deeper question: why does feature mixing obfuscate attribution-based
                        control? The answer, as we’ll see in the next section, traces back to deep learning’s most basic
                        mathematical operation and how it fundamentally precludes the partitioning that ABC requires.
                    </p>

                    <div class="callout">
                        <p class="callout-title">A Library Analogy</p>

                        <p>
                            Consider a library wherein all of the books have had their covers removed, their table
                            of contents erased, and their chapters torn out and shuffled amongst all the books.
                            Consequently, when someone wants to answer a specific question, they have to read
                            through the entire library searching for relevant information for their query.
                        </p>

                        <p>
                            Deep learning stores information in a similar way, with so-called <em>distributed
                                representations</em> spreading concepts across many neurons... each of which is
                            unlabeled (i.e.
                            ”hidden”). Far from an accident, this form of learning is at the center of deep learning’s
                            core philosophy, the unrestricted learning of dense, hidden features.
                        </p>
                    </div>
                </section>

                <section id="third-why">
                    <h2>Third Why (Root Cause): Addition of Source-Separable Concepts</h2>

                    <figure class="full-width">
                        <img src="1411.3146/abc_ch2_5_0_v5.png" alt="Third Why graph">
                        <figcaption>Subset of conceptual graph highlighting the Third Why and the focus of this section.
                        </figcaption>
                    </figure>

                    <p>
                        The previous section revealed how deep learning’s feature mixing precludes the partitioning
                        required for attribution-based control. Yet this raises our final ”why”: what makes this mixing
                        fundamentally irreversible? The answer lies in deep learning’s most basic mathematical
                        operation: addition.
                    </p>

                    <p>
                        Addition might seem like an implementation detail, but it fundamentally prevents recovering
                        source information. When values combine through addition, the result does not uniquely
                        determine its inputs&mdash;multiple distinct source combinations produce identical outputs:
                    </p>

                    <div class="definition-box">
                        <h3>Non-Injectivity of Addition</h3>
                        <p>Addition is not injective: for any sum y, there exist infinitely many distinct pairs
                            (x<sub>1</sub>, x<sub>2</sub>) and (x'<sub>1</sub>, x'<sub>2</sub>) such that x<sub>1</sub>
                            + x<sub>2</sub> = y = x'<sub>1</sub> + x'<sub>2</sub> where (x<sub>1</sub>, x<sub>2</sub>)
                            &ne; (x'<sub>1</sub>, x'<sub>2</sub>).</p>
                    </div>

                    <p>
                        This non-injectivity means that observing the sum provides no information about which
                        specific sources contributed. Consider the contrast with concatenation:
                    </p>

                    <div class="definition-box">
                        <h3>Concatenation vs Addition</h3>
                        <p><strong>Concatenation preserves sources:</strong><br>
                            "1" &oplus; "6" = "16"<br>
                            "2" &oplus; "5" = "25"<br>
                            (distinct inputs &rarr; distinct outputs)</p>
                        <p><strong>Addition erases them:</strong><br>
                            1 + 6 = 7<br>
                            2 + 5 = 7<br>
                            (distinct inputs &rarr; identical outputs)</p>
                    </div>

                    <p>
                        When partitions are known, concatenation of numbers is injective; different inputs produce
                        different outputs, allowing source recovery. Addition is not; the output 7 could arise from 1 +
                        6,
                        2 + 5, 3 + 4, 0 + 7, or infinitely many other combinations. This non-injectivity is the
                        mechanism
                        through which deep neural networks erase attribution information: once gradients from different
                        sources are summed into shared parameters, no function of those parameters can recover which
                        sources contributed what information.
                    </p>

                    <p>
                        And neural networks use addition extensively: combining features between layers, aggregating
                        gradients during backpropagation, and updating weights during training. Each addition
                        irreversibly combines information, destroying the provenance of where that information came
                        from and how it was interpreted.
                    </p>

                    <p>
                        A natural solution might seem possible: why not just track every operation during training ...
                        while also doing these additions? Unfortunately, this approach fails for three fundamental
                        reasons:
                    </p>

                    <p>
                        First, in models like GPT-3, through forward and backward propagation, each example’s
                        information eventually touches every weight in the network. Even tiny weight changes alter how
                        future examples flow through the network, creating cascading effects that can amplify initially
                        small influences (related: vanishing and exploding gradients (<a href="#ref-61"
                            class="cite">Hochreiter 1998</a>; <a href="#ref-62" class="cite">Hanin 2018</a>)).

                    </p>

                    <p>
                        Second, these influences compound exponentially and recursively, creating higher order
                        all-to-all relationships between inputs and weights as they do. Consider the mathematics of
                        weight updates for a weight update function <em>g</em>, weights and data at time <em>t</em> as
                        <em>w
                            <sub>t</sub></em> and <em>x <sub>t</sub></em>
                        :
                    </p>

                    <p>
                        <em>w <sub>t+1</sub> = g(w<sub>t</sub>, x<sub>t</sub>)</em> <br>
                        <em>w <sub>t+2</sub> = g(g(w<sub>t</sub>, x<sub>t</sub>), x<sub>t+1</sub>)</em> <br>
                        <em>w <sub>t+3</sub> = g(g(g(w<sub>t</sub>, x<sub>t</sub>), x<sub>t+1</sub>), x<sub>t+2</sub>)
                        </em>
                    </p>

                    <p>
                        The number of potential attribution paths grows as Ω(<em>w · n)<sup>t</sup></em> , where
                        <em>w</em> is the number of
                        weights, <em>n</em> is the number of examples, and <em>t</em> is the number of steps.
                    </p>

                    <p>
                        Third, this exponential growth makes exact attribution computationally intractable. The most
                        capable language models frequently leverage just under 1% of their parent organization’s AI
                        training budget (see Appendix I and II for details). Thus, tracking the full web of dependencies
                        (every interaction, every update, every influence path) would require many orders of magnitude
                        more compute than is available to the largest tech firms.
                    </p>

                    <p>
                        These three barriers (information loss through addition, exponential propagation of influences,
                        and computational intractability) combine to create a fundamental limitation. No amount
                        of clever engineering can fully recover what addition has destroyed. This mathematical reality
                        explains why attempts at machine unlearning and influence functions remain fundamentally
                        limited: they try to reconstruct what addition has already erased.
                    </p>

                    <p>
                        <strong>The Root Problem</strong>: The implications are significant. Without the ability to
                        track sources
                        through training, we cannot provide the attribution that ABC requires. Without attribution, we
                        cannot enable the partitioned sharing and use of data and compute that could unlock orders of
                        magnitude more AI resources. Addition itself blocks the very data and compute gains described
                        earlier in this chapter, and holds up the many problems described in Chapter 1.
                    </p>

                    <div class="callout">
                        <p class="callout-title">A Library Analogy</p>

                        <p>
                            Consider a library wherein all of the books have had their covers removed, their table of
                            contents erased, and individual sentences on each page torn out into their own strips. Now
                            imagine that each word in each strip is converted into a number ”aardvark = 1”, ”abernathe
                            = 2”, and so forth. And then imagine that each of these strips from the whole library is
                            shuffled around, and groups of strips are placed back in the coverless books. Yet, instead
                            of
                            each strip being glued in place, it is first combined with many other strips, adding their
                            respective numbers together. Consequently, when someone wants to answer a specific
                            question, they have to read through the entire library searching for relevant information
                            for
                            their query, first by converting their query into numbers and then attempting to match it to
                            numbers found in the books.
                        </p>

                        <p>
                            Deep learning stores information in a similar way, with so-called <em>distributed
                                representations</em> spreading concepts across many neurons... each of which is
                            unlabeled (i.e.
                            ”hidden”). Far from an accident, this form of learning is at the center of deep learning’s
                            core philosophy, the unrestricted learning of dense, hidden features which are formed
                            through many successive additions.
                        </p>
                    </div>
                </section>

                <section id="third-hypothesis">
                    <h2>Third Hypothesis (Root Solution): Concatenating Along Natural Boundaries in Data Sources Enables
                        Attribution</h2>

                    <figure class="full-width">
                        <img src="1411.3146/abc_ch2_6_0_v6.png" alt="Third Hypothesis graph">
                        <figcaption>Subset of concept graph highlighting the Third Hypothesis and focus of this section.
                        </figcaption>
                    </figure>

                    <p>
                        We now continue by constructing a hypothesis (the “Third Hypothesis”) which corresponds
                        to the Third Why, which will support the Second Hypothesis addressing the Second Why and
                        so forth. The previous sections revealed how addition in deep learning creates a fundamental
                        barrier to attribution. Yet examining why addition fails suggests a testable hypothesis: can we
                        significantly reduce the use of addition, perhaps swapping it with concatenation?
                    </p>

                    <p>
                        Deep learning’s central hypothesis would suggest we can’t, that features need to densely mix
                        (using addition) in order to learn the powerful correlations and representations that give deep
                        learning its predictive capability. However, presumably deep learning maps multiple distinct
                        concepts into a shared feature when those two concepts are related (<a href="#ref-63"
                            class="cite">Krizhevsky et al., 2012</a>).
                        For
                        example, a deep learning model which classifies images might have features which detect ears,
                        fur, and eyes — features which would be useful for modeling many different animals which
                        possess these related concepts (<a href="#ref-13" class="cite">Zeiler and Fergus 2014</a>).
                        However, as these features are not laid
                        out in advance, deep learning needs to densely mix its features in order to discover these
                        related
                        patterns across training datapoints. That is to say, perhaps over-zealous dense feature mixing
                        is
                        more about training than inference
                    </p>

                    <p>
                        Yet, perhaps deep learning is over-zealous in its feature mixing, opening itself to
                        representation power which could mix any feature with any other when in the real-world not all
                        concepts
                        are closely related. That is to say, not all concepts require that level of general
                        representation
                        power. Perhaps some concepts are actually unrelated to one another, such that some proportion
                        of dense feature mixing in deep learning is superfluous
                    </p>

                    <p>
                        That said, some concepts are densely mixed, while others are clearly less so. Some information
                        patterns appear ubiquitously: basic rules of grammar that structure language, logical
                        operations that appear in reasoning, morphological patterns which make up words, edges and
                        corners in images, etc. Elements like these are frequently reused across almost every data
                        point, appearing in many billions of documents, images, audio, and videos. Such dense patterns
                        suggest unrestricted mixing through addition may be appropriate for a core subset of features.
                        Their ubiquity also makes attribution less critical; they represent shared computational tools
                        rather than source-specific claims about the world. While perhaps not formally stated, noted
                        researcher Andrej Karpathy recently suggested a similar concept when referring to a future LLM
                        “cognitive core”:
                    </p>

                    <div class="callout" style="background: #f5f5f5; border-left: 3px solid #888;">
                        <p style="font-style: italic;">
                            The race for LLM “cognitive core”—a few billion param model that maximally sacrifices
                            encyclopedic knowledge for capability. It lives always-on and by default on every computer
                            as the kernel of LLM personal computing. Its features are slowly crystalizing:
                        </p>

                        <ul style="font-style: italic;">
                            <li>Natively multimodal text/vision/audio at both input and output.</li>

                            <li>Matryoshka-style architecture allowing a dial of capability up and down at test time.
                            </li>
                            <li>Reasoning, also with a dial. (system 2)</li>
                            <li>Aggressively tool-using.</li>
                            <li>On-device finetuning LoRA slots for test-time training, personalization and
                                customization.</li>
                            <li>Delegates and double checks just the right parts with the oracles in the cloud if
                                internet is available</li>
                        </ul>

                        <p style="font-style: italic;">
                            It doesn’t know that William the Conqueror’s reign ended in September 9 1087, but it
                            vaguely recognizes the name and can look up the date. It can’t recite the SHA-256 of
                            empty string as e3b0c442..., but it can calculate it quickly should you really want it.
                        </p>

                        <p style="font-style: italic;">
                            What LLM personal computing lacks in broad world knowledge and top tier problemsolving
                            capability it will make up in super low interaction latency (especially as multimodal
                            matures), direct / private access to data and state, offline continuity, sovereignty (“not
                            your weights not your brain”). i.e. many of the same reasons we like, use and buy personal
                            computers instead of having thin clients access a cloud via remote desktop or so.
                        </p>
                        <p style="text-align: right;">— Andrej Karpathy<sup><a
                                    href="https://x.com/karpathy/status/1938626382248149433">[link]</a></sup></p>
                    </div>

                    <p>
                        In contrast, perhaps most information is encyclopedic and appears sparsely: specific facts
                        about the world, domain expertise in particular fields, claims made by individual sources, etc.
                        The capital of France, the rules of chess, statistics about pizza... each appears in distinct
                        contexts
                        with limited overlap. As Chomsky noted in linguistics (<a href="#ref-14" class="cite">Chomsky
                            2014</a>), while we use common
                        patterns to express all knowledge, the knowledge itself often remains naturally partitioned by
                        topic, and when documents are topic specific... by source.
                    </p>

                    <p>
                        Let us assume for a moment that this is true. If so, then in theory some section of a neural
                        network could be made sparse, namely the part of the neural network which stores concepts
                        which are largely decoupled from the rest of a neural network’s knowledge (facts, domain
                        expertise, semantic information, etc.). Perhaps this section could use less addition (and more
                        concatenation), enabling sparsity which could drive attribution. Meanwhile, another part of
                        the neural network might need to remain dense, storing and synthesizing concepts which are
                        ubiquitous across a statistical distribution (logic, reasoning, syntax, etc.).
                    </p>

                    <p>
                        The key question: how would one go about training a neural network which successfully
                        partitioned information into sparse and dense sections?
                    </p>

                    <p>
                        A key insight of this chapter is that techniques from privacy-preserving machine learning,
                        particularly differential privacy (DP) (<a href="#ref-64" class="cite">Dwork et al., 2006</a>),
                        provide a principled way to measure
                        and control which features benefit from dense mixing versus sparse representation. Differential
                        privacy quantifies how much a model’s outputs depend on any individual training example:
                    </p>

                    <div class="definition-box">
                        <h3>(ε, δ)-Differential Privacy</h3>
                        <p>A randomized mechanism $\mathcal{M}: \mathcal{D} \to \mathcal{R}$ satisfies (ε,
                            δ)-differential privacy if for all adjacent datasets $D, D' \in \mathcal{D}$ (differing in
                            one example) and all subsets of outputs $S \subseteq \mathcal{R}$:</p>
                        <p style="text-align: center;">$\Pr[\mathcal{M}(D) \in S] \leq e^\epsilon \cdot
                            \Pr[\mathcal{M}(D') \in S] + \delta$</p>
                        <p>where small ε indicates strong privacy—outputs barely depend on any individual example.</p>
                    </div>

                    <p>The parameter ε provides a quantitative measure of individual example influence on outputs. This
                        same measure can serve three distinct control objectives:</p>

                    <div class="definition-box">
                        <h3>Three Regimes of Influence Control</h3>
                        <p>For a mechanism <em>M</em>, examples <em>e</em>, and
                            thresholds 0 < τ<sub>min</sub>
                                < τ<sub>max</sub>:</p>
                        <ul>
                            <li><strong>Privacy (constrain influence):</strong> Enforce &epsilon;<sub>e</sub>
                                &lt;
                                &tau;<sub>min</sub> for all examples, guaranteeing that
                                individual examples cannot be distinguished through their influence on outputs</li>
                            <li><strong>Measurement (track influence):</strong> Compute &epsilon;<sub>e</sub>
                                for each
                                example, enabling quantification
                                of which examples influence which outputs, without enforcing bounds</li>
                            <li><strong>Attribution (ensure influence):</strong> Enforce &epsilon;<sub>e</sub>
                                &gt;
                                &tau;<sub>max</sub> for specified examples, guaranteeing
                                that certain examples have measurable influence on outputs</li>
                        </ul>
                    </div>

                    <p>
                        These three regimes serve different stakeholder needs. Data owners might seek differential
                        privacy (ensuring their data cannot be identified in model outputs), <em>differential
                            measurement</em>
                        (understanding their data’s contribution), or <em>differential attribution</em> (guaranteeing
                        their data
                        influences predictions). Users might seek privacy (preventing their queries from revealing
                        information) or attribution (ensuring they can identify which sources influenced their results).
                        The same mathematical framework (DP’s ϵ parameter) enables each form of control.
                    </p>

                    <p>
                        Given differential privacy’s use in deep learning (e.g., (<a href="#ref-65" class="cite">Abadi
                            et al., 2016</a>)), this framework
                        suggests an architectural insight: a neural network serving diverse stakeholders requires the
                        capability to enforce different ϵ regimes for different information. Information requiring
                        privacy
                        (small <em>ϵ</em>) can pass through privacy-constrained layers with dense mixing via addition.
                        Information requiring attribution (large <em>ϵ</em>) must route through pathways preserving
                        source
                        identity via
                        sparse concatenation. Information requiring measurement sits between these extremes, with
                        <em>ϵ</em>
                        tracked but not bounded. The model’s optimization pressure, when constrained to respect these
                        different <em>ϵ</em> regimes, might naturally partition information accordingly. This framework
                        suggests
                        two testable predictions:
                    </p>

                    <p>
                        First, features with high source-specific attribution should cluster naturally by source, while
                        features with low attribution should appear consistently across sources. Recent work with
                        RETRO and ATLAS provides initial evidence for this prediction, showing how knowledge
                        naturally separates into general computational patterns (the Transformer reading from a database
                        of vectors, similar to Karpathy’s “cognitive core”) and source-specific information (the
                        database
                        of vectors).
                    </p>

                    <p>
                        Second, respecting these natural boundaries through architectural choices might enable
                        more efficient computation. If most information is sparsely distributed, then forcing it through
                        dense addition operations wastes significant compute. Models that preserve sparse patterns
                        through concatenation while sharing dense patterns through addition should achieve better
                        computational efficiency. Again RETRO and ATLAS provide early evidence of this ability,
                        wherein information within their vector database is concatenated (i.e. in different rows of the
                        database), while information in the neural network consuming from the database is densely
                        stored within Transformer weights.
                    </p>

                    <p>
                        The next section builds on this hypothesis, suggesting how these natural boundaries in
                        information usage could enable new approaches to AI development. If correct, this could
                        resolve the tension between deep learning’s powerful pattern recognition and the need for
                        attribution-based control in AI systems.
                    </p>

                    <div class="callout">
                        <p class="callout-title">A Library Analogy</p>

                        <p>
                            Consider a library wherein all of the books have had their covers removed, their table of
                            contents erased, and individual sentences on each page torn out into their own strips. Now
                            imagine that each word in each strip is converted into a number ”aardvark = 1”, ”abernathe
                            = 2”, and so forth. And then imagine that each of these strips from the whole library is
                            shuffled around, and groups of strips are placed back in the coverless books. Yet, instead
                            of
                            each strip being glued in place, it is first combined with many other strips, adding their
                            respective numbers together. Consequently, when someone wants to answer a specific
                            question, they have to read through the entire library searching for relevant information
                            for
                            their query, first by converting their query into numbers and then attempting to match it to
                            numbers found in the books.
                        </p>

                        <p>
                            Following the analogy, differential attribution ensures that each strip of numbers
                            remains separated (instead of added) into each other strip, and preserved within the same
                            book as before, partitioning data in a way which might be indexed by source or topic.
                        </p>
                    </div>
                </section>

                <section id="second-hypothesis">
                    <h2>Second Hypothesis: From Deep Learning to Deep Voting
                        with AI Recycling and Src-specific Intelligence Budgets</h2>

                    <figure class="full-width">
                        <img src="1411.3146/abc_ch2_7_0_v5.png" alt="Second Hypothesis graph">
                        <figcaption>Subset of concept graph highlighting the Second Hypothesis and focus of this
                            section.</figcaption>
                    </figure>

                    <p>
                        The previous section revealed how privacy mechanisms might naturally separate dense from
                        sparse information patterns. Yet this theoretical insight raises a practical question: how do we
                        actually <em>implement</em> the measurement and control regimes we defined? The definitions in
                        the
                        previous section assumed worst-case bounds (requiring that ϵ constraints hold for <em>all</em>
                        pairs of
                        neighboring datasets). But attribution-based control requires the opposite: not uniform bounds
                        across all sources, but <em>source-specific</em> control where each source can have different
                        influence
                        levels matching different stakeholder needs.
                    </p>

                    <h3>From Worst-Case to Individual Differential Privacy</h3>

                    <p>
                        Standard differential privacy enforces worst-case bounds across all possible pairs of
                        neighboring
                        datasets. Consider a dataset where most individuals’ data appears in common patterns, but one
                        individual has highly unique data. Worst-case differential privacy must constrain the entire
                        mechanism based on that one outlier, reducing utility for everyone&mdash;even though the
                        mechanism
                        only ever operates on the <em>actual</em> dataset, not all possible datasets. Individual
                        differential
                        privacy
                        (<a href="#ref-66" class="cite">Soria-Comas et al., 2016</a>) provides a more nuanced approach
                        by focusing privacy guarantees
                        on the <em>actual</em> dataset rather than all possible datasets:
                    </p>

                    <div class="definition-box">
                        <h3>Individual Differential Privacy</h3>
                        <p>Given a dataset $D$, a response mechanism $\mathcal{M}(\cdot)$ satisfies ε-individual
                            differential privacy (ε-iDP) if, for any dataset $D'$ that is a neighbor of $D$ (differing
                            in one example), and any $S \subset \text{Range}(\mathcal{M})$:</p>
                        <p style="text-align: center;">$\exp(-\epsilon) \cdot \Pr[\mathcal{M}(D') \in S] \leq
                            \Pr[\mathcal{M}(D) \in S] \leq \exp(\epsilon) \cdot \Pr[\mathcal{M}(D') \in S]$</p>
                    </div>

                    <p>
                        The crucial difference from standard DP: $D$ refers to the <em>actual</em> dataset being
                        protected, while $D'$ ranges over $D$'s neighbors. Standard DP requires indistinguishability for
                        <em>any</em> pair of neighbors; individual DP requires indistinguishability only between the
                        actual dataset and its neighbors. This asymmetry allows the mechanism to adjust noise based on
                        properties of the
                        actual dataset (such as its local sensitivity) rather than worst-case properties across all
                        possible
                        datasets.
                    </p>

                    <p>
                        This enables tighter privacy guarantees in practice. When the actual dataset has low local
                        sensitivity (changing any individual barely affects outputs), individual DP requires minimal
                        noise. When the actual dataset has high local sensitivity (some individuals significantly affect
                        outputs), individual DP adds proportionate noise. Standard DP must always assume worst-case
                        sensitivity regardless of the actual dataset’s properties.
                    </p>

                    <h3>From Individual Privacy to Individual Attribution</h3>

                    <p>
                        Just as we extended standard differential privacy to define attribution regimes in the previous
                        section, we can extend individual differential privacy to define individual attribution. The key
                        insight remains the same: privacy and attribution are opposite ends of the same ϵ spectrum, now
                        applied to the actual dataset rather than worst-case bounds.
                    </p>

                    <div class="definition-box">
                        <h3>Individual Differential Privacy vs. Attribution</h3>
                        <p>For a mechanism $\mathcal{M}$, actual dataset $D$, and threshold $\tau > 0$:</p>
                        <ul>
                            <li><strong>Individual Privacy:</strong> Enforce $\epsilon < \tau$ for all neighbors $D'$ of
                                    $D$, guaranteeing that any individual's data in the dataset cannot be distinguished
                                    through its influence on outputs.</li>
                            <li><strong>Individual Measurement:</strong> Compute $\epsilon$ for the actual dataset,
                                enabling quantification of influence without enforcing bounds.</li>
                            <li><strong>Individual Attribution:</strong> Enforce $\epsilon > \tau$ for the actual
                                dataset, guaranteeing that individuals in the actual dataset have measurable influence
                                on outputs.</li>
                        </ul>
                    </div>

                    <h3>From Examples to Sources: Group Differential Privacy</h3>

                    <p>
                        But attribution-based control requires more than individual-level bounds, it requires
                        <em>source-level</em>
                        control. Individual differential privacy protects single examples in the actual dataset, but
                        data
                        sources typically contribute many examples. Consider a medical AI trained on data from 100
                        hospitals: each hospital contributes thousands of patient records. ABC needs to measure and
                        control influence at the hospital level, not just the individual patient level. The differential
                        privacy literature addresses this through group differential privacy (<a href="#ref-67"
                            class="cite">Dwork et al., 2014</a>), which
                        extends privacy guarantees from individuals to groups of records:
                    </p>

                    <div class="definition-box">
                        <h3>Group Differential Privacy</h3>

                        <p>A randomized mechanism $\mathcal{M}: \mathcal{D} \to \mathcal{R}$ satisfies ε-group
                            differential privacy for groups of size $k$ if for all datasets $D, D'$ differing in at most
                            $k$ records and all subsets of outputs $S \subseteq \mathcal{R}$:</p>
                        <p style="text-align: center;">$\Pr[\mathcal{M}(D) \in S] \leq e^\epsilon \cdot
                            \Pr[\mathcal{M}(D') \in S]$</p>
                    </div>

                    <p>We can combine group differential privacy with individual differential privacy to obtain
                        source-level control calibrated to the actual dataset. When we partition a dataset by sources $D
                        = \bigcup_{s \in S} D_s$, we treat each source as a group and apply individual DP at the group
                        level:</p>

                    <div class="definition-box">
                        <h3>Source-Level Individual Differential Privacy</h3>
                        <p>Given a dataset $D$ partitioned by sources $D = \bigcup_{s \in S} D_s$, a response mechanism
                            $\mathcal{M}(\cdot)$ satisfies $\epsilon_s$-individual differential privacy for source $s$
                            if, for any dataset $D'$ differing from $D$ only in source $s$'s data (i.e., $D' = D_{-s}
                            \cup D'_s$ where $|D'_s| = |D_s|$), and any $S \subset \text{Range}(\mathcal{M})$:</p>
                        <p style="text-align: center;">$\exp(-\epsilon_s) \cdot \Pr[\mathcal{M}(D') \in S] \leq
                            \Pr[\mathcal{M}(D) \in S] \leq \exp(\epsilon_s) \cdot \Pr[\mathcal{M}(D') \in S]$</p>
                    </div>

                    <p>
                        Standard differential privacy automatically provides group privacy through composition: if
                        a mechanism satisfies $ϵ$-DP for individuals, it satisfies $kϵ$-DP for groups of size $k$.
                        However,
                        this is a worst-case bound that assumes all $k$ individuals have maximal independent influence.
                        Group differential privacy makes group protection explicit, enabling tighter analysis when group
                        members have correlated data or when mechanisms can exploit group structure.
                    </p>

                    <p>
                        We can combine group differential privacy with individual differential privacy to obtain
                        source-level control calibrated to the actual dataset. When we partition a dataset by sources
                        \( D = \bigcup_{s \in S} D_s \), we treat each source as a group and apply individual DP at the
                        group level:
                    </p>

                    <div class="definition-box">
                        <h3>Source-Level Individual Differential Privacy</h3>

                        <p>
                            Given a dataset \( D \) partitioned by sources
                            \( D = \bigcup_{s \in S} D_s \), a response mechanism \( \mathcal{M}(\cdot) \)
                            satisfies \( \epsilon_s \)-individual differential privacy for source \( s \) if,
                            for any dataset \( D' \) differing from \( D \) only in source \( s \)’s data
                            (i.e., \( D' = D_{-s} \cup D'_s \) where \( |D'_s| = |D_s| \)),
                            and any \( S \subset \mathrm{Range}(\mathcal{M}) \):
                        </p>

                        <p style="text-align: center;">
                            \[
                            \exp(-\epsilon_s)\,\Pr[\mathcal{M}(D') \in S]
                            \;\le\;
                            \Pr[\mathcal{M}(D) \in S]
                            \;\le\;
                            \exp(\epsilon_s)\,\Pr[\mathcal{M}(D') \in S]
                            \]
                        </p>
                    </div>

                    <p>
                        This combines group DP’s extension to multiple records with individual DP’s calibration
                        to the actual dataset. Each source s receives its own privacy parameter \( \epsilon_s \)
                        measuring influence
                        on the actual dataset $D$, not worst-case influence across all possible datasets. A hospital
                        contributing highly unique medical data might have large \( \epsilon_s \) for the actual
                        dataset, while a
                        hospital contributing common patterns might have small \( \epsilon_s \) (without forcing all
                        hospitals to
                        share worst-case bounds).
                    </p>

                    <h3>From Source-Level Privacy to Source-Level Attribution</h3>

                    <p>
                        Just as individual DP extends to attribution regimes (privacy, measurement, attribution),
                        sourcelevel individual DP extends to source-level attribution. We can quantitatively measure
                        each
                        source’s influence on the actual dataset:
                    </p>

                    <div class="definition-box">
                        <h3>Source-Level Individual Differential Attribution</h3>
                        <p>Let $\mathcal{A}$ be a randomized algorithm and let $D$ be a dataset partitioned by sources
                            $D = \bigcup_{s \in S} D_s$. For any source $s$ and prediction $f$, the individual
                            differential attribution of source $s$ on function $f$ is:</p>
                        <p style="text-align: center;">$\text{Attribution}_\alpha(s, f) =
                            D_\alpha^\leftrightarrow(\mathcal{A}(D)\|\mathcal{A}(D^{-s})) =
                            \max\{D_\alpha(\mathcal{A}(D)\|\mathcal{A}(D^{-s})),
                            D_\alpha(\mathcal{A}(D^{-s})\|\mathcal{A}(D))\}$</p>
                        <p>where $D_\alpha$ is the Rényi divergence of order $\alpha$<a href="#sidenote-4"
                                class="sidenote-ref"><sup>4</sup></a>, $D^{-s} = D \setminus D_s$
                            represents the dataset with source $s$ removed, and $\mathcal{A}(D)$ represents the output
                            distribution of algorithm $\mathcal{A}$ on dataset $D$.</p>
                    </div>

                    <p>
                        This measures each source’s attribution relative to the actual dataset \( D \)
                        by quantifying how predictions change when that specific source is included
                        versus excluded. Unlike standard group DP (which provides worst-case
                        \( k\epsilon \) bounds for all groups of size \( k \)), source-level individual
                        attribution calibrates to actual dataset properties: a medical research paper
                        might have high attribution for predictions in its domain (large divergence
                        from \( D_{-s} \)) but negligible attribution for unrelated queries (small
                        divergence).
                    </p>

                    <p>
                        This enables the diverse source-level control ABC requires. Consider the medical AI trained
                        on 100 hospitals: some hospitals can demand privacy (enforce small \( \epsilon_s \) calibrated
                        to their actual
                        data), others can guarantee attribution (enforce large ϵs ensuring measurable influence), others
                        can simply track their contribution (measure \( \epsilon_s \) without bounds). Worst-case group
                        DP cannot
                        accommodate this diversity; it forces uniform \( k\epsilon \) bounds across all groups of size
                        \( k \).
                        Source-level
                        individual attribution provides per-source control adjusted to the actual dataset, precisely
                        what
                        attribution-based control requires.
                    </p>

                    <div class="sidenote" id="sidenote-4">
                        <p>
                            <sup>4</sup>
                            Rényi divergence generalizes both KL divergence
  (\( \alpha \to 1 \)) and max divergence (\( \alpha \to \infty \)),
  providing tractable computation with tunable sensitivity–privacy tradeoffs.
                        </p>
                    </div>

                    <h3>Intelligence Budgets: Implementing Individual Source Control</h3>

                    <p>This source-level individual attribution measure enables a practical control mechanism:
                        <em>intelligence budgets</em>. Rather than simply measuring influence after the fact, we can
                        actively control how much each source influences predictions through architectural routing
                        decisions.
                    </p>

                    <div class="definition-box">
                        <h3>Intelligence Budgets via Forward Pass Weighting</h3>
                        <p>The model has two types of parameterized functions:</p>
                        <p style="text-align: center;">$g_s(\cdot; \Theta_{\text{semantic}}[s])$: source-specific
                            function for source $s$</p>
                        <p style="text-align: center;">$f(\cdot; \Theta_{\text{syntactic}})$: shared function across all
                            sources</p>
                        <p>Information flows through two stages:</p>
                        <p style="text-align: center;">Stage 1 (Semantic): $s_s = g_s(x_s; \Theta_{\text{semantic}}[s])$
                        </p>
                        <p style="text-align: center;">Stage 2 (Syntactic): $h(x) = f\left([x_1, \ldots, x_{|S|},
                            \gamma[1] \cdot s_1, \ldots, \gamma[|S|] \cdot s_{|S|}]; \Theta_{\text{syntactic}}\right)$
                        </p>
                        <p>where $\gamma[s] \in [0,1]$ is a per-source scaling weight and $[\cdot]$ denotes
                            concatenation of all inputs.</p>
                        <p>The intelligence budget $B(s)$ bounds source $s$'s influence:</p>
                        <p style="text-align: center;">$\text{Attribution}_\alpha(s, h) \leq B(s)$</p>
                        <p>Setting $\gamma[s] \approx 0$ enforces small $B(s)$ (privacy regime) by preventing semantic
                            contributions. Setting $\gamma[s] \approx 1$ allows large $B(s)$ (attribution regime) by
                            preserving semantic identity.</p>
                    </div>

                    <h3>From Concatenation and IDP to Deep Voting</h3>
                    <p>The deep voting framework reveals a two-dimensional spectrum in machine learning architectures by
                        introducing a second control parameter $\lambda \in [0,1]$ that governs the overall balance
                        between semantic and syntactic capacity:</p>

                    <div class="definition-box">
                        <h3>Capacity Allocation (λ)</h3>
                        <p>The parameter λ determines what fraction of total model capacity is allocated to each
                            function:</p>
                        <p style="text-align: center;">$|\Theta_{\text{syntactic}}| = \lambda \cdot
                            |\Theta_{\text{total}}|$</p>
                        <p style="text-align: center;">$\sum_{s} |\Theta_{\text{semantic}}[s]| = (1-\lambda) \cdot
                            |\Theta_{\text{total}}|$</p>
                        <p>where $|\Theta|$ denotes parameter count.</p>
                    </div>

                    <div class="callout">
                        <p class="callout-title">Deep Voting Analogy: Individual Differential Privacy via Adaptive
                            Filtering</p>
                        <p>Feldman and Zrnic's individual differential privacy framework [<a href="#ref-17"
                                class="cite">17</a>] provides a concrete example of intelligence budgets implemented
                            through adaptive filtering rather than architectural routing.</p>
                        <p><strong>Architecture:</strong> Their approach uses $\lambda = 1$ (pure syntactic processing)
                            with all capacity in shared parameters $\Theta_{\text{syntactic}}$. No semantic section
                            exists ($\Theta_{\text{semantic}}[s] = \emptyset$), meaning all sources contribute only
                            through raw inputs: $h(x) = f([x_1, \ldots, x_n]; \Theta_{\text{syntactic}})$ where $f$ adds
                            Gaussian noise for privacy.</p>
                        <p><strong>Intelligence Budgets via Filtering:</strong> Rather than controlling $\gamma[s]$
                            continuously, they implement binary filtering. At each time step $t$, compute the individual
                            privacy loss $\rho_t^{(i)} = \frac{\alpha \|\bar{g}_t(X_i)\|_2^2}{2\sigma^2}$ where
                            $\bar{g}_t(X_i)$ is the clipped gradient. Source $i$ remains active while $\sum_{j=1}^t
                            \rho_t^{(i)} \leq B$, then gets dropped (equivalent to setting $\gamma[i] = 0$ for all
                            future steps).</p>
                        <p><strong>Key Insight:</strong> The intelligence budget $B(i)$ is implicitly determined by
                            realized gradient norms. For Lipschitz functions with coordinate sensitivity $L_i$, the
                            bound becomes $B(i) \approx \frac{\alpha L_i^2 \|\phi(X_i)\|^2}{2\sigma^2}$. Sources with
                            small gradients (low sensitivity) can participate longer before exceeding their budget.</p>
                        <p><strong>Contrast with Deep Voting:</strong> Feldman & Zrnic achieve privacy (small ε) by
                            dropping high-influence examples entirely, routing all remaining examples through
                            privacy-constrained shared processing. Deep voting generalizes this by: (1) introducing a
                            semantic section ($\lambda < 1$) that preserves source identity, (2) allowing continuous
                                control ($\gamma[s] \in [0,1]$) rather than binary drop/keep decisions, and (3) enabling
                                attribution regime where large ε is desirable. Individual DP represents the special case
                                where $\lambda=1$, $\gamma[s] \in \{0,1\}$ (binary), and all sources demand privacy.</p>
                                <p><strong>Adaptive Composition:</strong> The Feldman & Zrnic result on adaptive
                                    composition with data-dependent privacy parameters ($\sum_t \rho_t^{(i)} \leq B
                                    \Rightarrow (\alpha, B)$-RDP) directly parallels our intelligence budget
                                    composition: both handle the challenge that influence parameters depend on previous
                                    outputs, enabling provable bounds even under adaptive computation.</p>
                    </div>

                    <p>With these mechanisms in place, we can return to the implications of such a system for providing
                        attribution-based control: the potential to dramatically increase the amount of data and compute
                        available for training AI systems. By providing clear mechanisms for measuring source influence
                        ($\text{Attribution}_\alpha(s, h)$), bounding influence when needed ($\gamma[s] = 0$ for
                        privacy), and guaranteeing influence when required ($\gamma[s] = 1$ for attribution), deep
                        voting might enable the safe use of orders of magnitude more training data.</p>

                    <div class="callout">
                        <p class="callout-title">A Library Analogy</p>
                        <p>Consider a library wherein all of the books have had their covers removed, their table of
                            contents erased, and individual sentences on each page torn out into their own strips. Now
                            imagine that each word in each strip is converted into a number "aardvark = 1", "abernathe =
                            2", and so forth. And then imagine that each of these strips from the whole library is
                            shuffled around, and groups of strips are placed back in the coverless books. Yet, instead
                            of each strip being glued in place, it is first combined with many other strips, adding
                            their respective numbers together. Consequently, when someone wants to answer a specific
                            question, they have to read through the entire library searching for relevant information
                            for their query—first by converting their query into numbers and then attempting to match it
                            to numbers found in the books.</p>
                        <p>Following the analogy, differential attribution ensures that each strip of numbers remains
                            separated (instead of added) into each other strip, preserved within the same book as
                            before, partitioning information in a way which might be indexed by source (or topic). It
                            further provides a staff of librarians who know how to read relevant information and
                            synthesize them, each according to a topic that librarian happens to be familiar with. Taken
                            together, a customer of a library can leverage one librarian to index into the appropriate
                            shelf, identify the right book, and the right snippets of that book, and then ask a subset
                            of the staff of librarians who are experts on that topic to properly interpret those
                            snippets.</p>
                    </div>
                </section>

                <section id="first-hypothesis">
                    <h2>First Hypothesis: ABC and 6+ Orders of Magnitude more Data/Compute</h2>

                    <figure class="full-width">
                        <img src="1411.3146/abc_ch2_8_0_v5.png" alt="First Hypothesis graph">
                        <figcaption>Subset of concept graph highlighting the First Hypothesis and focus of this section.
                        </figcaption>
                    </figure>

                    <p>The deep voting framework reveals a two-dimensional spectrum in machine learning architectures.
                        Consider how different parameter settings affect the model's behavior:</p>

                    <p>At (&lambda;,&gamma;) = (1,0), we find pure deep learning with maximum compression. These
                        systems, like GPT-4, use only shared parameters with basic bounds on attribution. At
                        (&lambda;,&gamma;) = (0,1), we find pure partition-based learning, like federated systems, which
                        maintain group privacy but limit cross-source learning. At (&lambda;,&gamma;) = (0,0), we find
                        pure source-specific learning systems like k-nearest neighbors.</p>

                    <p>The stakes are significant. If deep voting succeeds, it could unlock another 6+ orders of
                        magnitude of training data and compute productivity. If it fails, we may remain constrained by
                        the fundamental limitations of current architectures.</p>
                </section>

                <section id="empirical-evidence">
                    <h2>Empirical Evidence: Does the Pareto-Tradeoff Move?</h2>

                    <figure class="full-width">
                        <img src="1411.3146/abc_ch2_9_0_v4.png" alt="Empirical evidence concept graph">
                        <figcaption>Subset of concept graph highlighting the First Hypothesis and focus of this section.
                        </figcaption>
                    </figure>

                    <h3>The First Crack: RETRO and ATLAS</h3>
                    <p>Recent architectures challenge baseline tradeoffs. RETRO outperforms GPT-3 on the Pile (0.670 vs
                        0.811 bits-per-byte) while using only 7.5B parameters compared to GPT-3's 175B [<a href="#ref-4"
                            class="cite">4</a>]:</p>

                    <div class="table-container">
                        <table class="data-table">
                            <caption>Table: RETRO results on the Pile, measured in bits-per-byte. (From Borgeaud et al.,
                                2022)</caption>
                            <thead>
                                <tr>
                                    <th>Subset</th>
                                    <th>7B Baseline</th>
                                    <th>GPT-3</th>
                                    <th>Jurassic-1</th>
                                    <th>Gopher</th>
                                    <th>7.5B RETRO</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>arxiv</td>
                                    <td>0.742</td>
                                    <td>0.838</td>
                                    <td>0.680</td>
                                    <td><strong>0.641</strong></td>
                                    <td>0.714</td>
                                </tr>
                                <tr>
                                    <td>books3</td>
                                    <td>0.792</td>
                                    <td>0.802</td>
                                    <td>0.835</td>
                                    <td>0.706</td>
                                    <td><strong>0.653</strong></td>
                                </tr>
                                <tr>
                                    <td>dm_mathematics</td>
                                    <td>1.177</td>
                                    <td>1.371</td>
                                    <td><strong>1.037</strong></td>
                                    <td>1.135</td>
                                    <td>1.164</td>
                                </tr>
                                <tr>
                                    <td>freelaw</td>
                                    <td>0.576</td>
                                    <td>0.612</td>
                                    <td>0.514</td>
                                    <td>0.506</td>
                                    <td><strong>0.499</strong></td>
                                </tr>
                                <tr>
                                    <td>github</td>
                                    <td>0.420</td>
                                    <td>0.645</td>
                                    <td>0.358</td>
                                    <td>0.367</td>
                                    <td><strong>0.199</strong></td>
                                </tr>
                                <tr>
                                    <td>gutenberg_pg_19</td>
                                    <td>0.803</td>
                                    <td>1.163</td>
                                    <td>0.890</td>
                                    <td>0.652</td>
                                    <td><strong>0.400</strong></td>
                                </tr>
                                <tr>
                                    <td>hackernews</td>
                                    <td>0.971</td>
                                    <td>0.975</td>
                                    <td>0.869</td>
                                    <td>0.888</td>
                                    <td><strong>0.860</strong></td>
                                </tr>
                                <tr>
                                    <td>nih_exporter</td>
                                    <td>0.650</td>
                                    <td>0.612</td>
                                    <td><strong>0.590</strong></td>
                                    <td>0.590</td>
                                    <td>0.635</td>
                                </tr>
                                <tr>
                                    <td>opensubtitles</td>
                                    <td>0.974</td>
                                    <td>0.932</td>
                                    <td><strong>0.879</strong></td>
                                    <td>0.894</td>
                                    <td>0.930</td>
                                </tr>
                                <tr>
                                    <td>philpapers</td>
                                    <td>0.760</td>
                                    <td>0.723</td>
                                    <td>0.742</td>
                                    <td><strong>0.682</strong></td>
                                    <td>0.699</td>
                                </tr>
                                <tr>
                                    <td>pile_cc</td>
                                    <td>0.771</td>
                                    <td>0.698</td>
                                    <td>0.669</td>
                                    <td>0.688</td>
                                    <td><strong>0.626</strong></td>
                                </tr>
                                <tr>
                                    <td>pubmed_abstracts</td>
                                    <td>0.639</td>
                                    <td>0.625</td>
                                    <td>0.587</td>
                                    <td>0.578</td>
                                    <td><strong>0.542</strong></td>
                                </tr>
                                <tr>
                                    <td>pubmed_central</td>
                                    <td>0.588</td>
                                    <td>0.690</td>
                                    <td>0.579</td>
                                    <td>0.512</td>
                                    <td><strong>0.419</strong></td>
                                </tr>
                                <tr>
                                    <td>stackexchange</td>
                                    <td>0.714</td>
                                    <td>0.773</td>
                                    <td>0.655</td>
                                    <td>0.638</td>
                                    <td><strong>0.624</strong></td>
                                </tr>
                                <tr>
                                    <td>ubuntu_irc</td>
                                    <td>1.200</td>
                                    <td>0.946</td>
                                    <td><strong>0.857</strong></td>
                                    <td>1.081</td>
                                    <td>1.178</td>
                                </tr>
                                <tr>
                                    <td>uspto_backgrounds</td>
                                    <td>0.603</td>
                                    <td>0.566</td>
                                    <td><strong>0.537</strong></td>
                                    <td>0.545</td>
                                    <td>0.583</td>
                                </tr>
                                <tr class="highlight">
                                    <td><strong>Average</strong></td>
                                    <td>0.774</td>
                                    <td>0.811</td>
                                    <td>0.705</td>
                                    <td>0.694</td>
                                    <td><strong>0.670</strong></td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <p>This constitutes a 25x reduction in parameter count while achieving superior performance and
                        maintaining clear attribution paths through its retrieval mechanism.</p>

                    <p>ATLAS demonstrates similar gains: 25-50x parameter efficiency improvements while maintaining or
                        exceeding baseline performance [<a href="#ref-5" class="cite">5</a>]. Both systems achieve these
                        results through a fundamental architectural shift: rather than compressing all knowledge into
                        dense parameters, they maintain explicit connections to source documents through retrieval.</p>

                    <p>Federated RAG systems demonstrate concurrent improvements in attribution and performance:</p>

                    <div class="table-container">
                        <table class="data-table">
                            <caption>Table: Performance Metrics on MMLU Tasks (Federated RAG vs Baseline RAG)</caption>
                            <thead>
                                <tr>
                                    <th>Task</th>
                                    <th>Federated RAG Accuracy (%)</th>
                                    <th>Baseline RAG Accuracy (%)</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Task 1</td>
                                    <td>78</td>
                                    <td>70</td>
                                </tr>
                                <tr>
                                    <td>Task 2</td>
                                    <td>82</td>
                                    <td>75</td>
                                </tr>
                                <tr>
                                    <td>Task 3</td>
                                    <td>74</td>
                                    <td>68</td>
                                </tr>
                                <tr>
                                    <td>Task 4</td>
                                    <td>88</td>
                                    <td>80</td>
                                </tr>
                                <tr>
                                    <td>Task 5</td>
                                    <td>81</td>
                                    <td>76</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <h3>Deep Voting: Formalizing the Pattern</h3>
                    <p>These architectures (RETRO, ATLAS, PATE [<a href="#ref-18" class="cite">18</a>], federated RAG
                        [<a href="#ref-19" class="cite">19</a>], and Git Re-Basin [<a href="#ref-20"
                            class="cite">20</a>]) share a common technical mechanism: they replace addition operations
                        during training with concatenation, deferring synthesis until inference time. We formalize this
                        pattern as <em>deep voting</em>.</p>

                    <figure class="full-width">
                        <img src="1411.3146/deep_learning_to_deep_voting_v3.png" alt="Deep Learning to Deep Voting">
                        <figcaption>Traditional open/closed-source deep learning systems (left) pool all data into a
                            deep learning model (i.e. by adding weight updates) which is later used for predictions,
                            while deep voting systems (right) learn weight parameters which remain partitioned by source
                            (i.e. concatenated), but which are learned in a way that they can be rapidly synthesized on
                            the fly. Partitions (pictured above as pie slices) are rapidly synthesized to form a model
                            used for a specific prediction. Darker lines/slices indicate information being used for a
                            particular prediction. Lighter lines/slices indicate information not being used for a
                            particular prediction. The circle on top of the slices represents a dense model capable of
                            rapidly synthesizing slices in practice. The combination of concatenated weights and
                            concatenated metadata with inference predictions (which tracks the synthesis and subsequent
                            use of slices for an AI prediction) enables ABC.</figcaption>
                    </figure>

                    <p>Deep voting addresses the addition problem that blocks access to 6+ orders of magnitude of data
                        and compute. By preserving source attribution through concatenated representations while
                        enabling cross-source learning through shared components, deep voting architectures demonstrate
                        that the baseline tradeoffs between attribution, efficiency, and performance reflect
                        architectural choices rather than fundamental constraints.</p>

                    <p>However, solving the addition problem reveals a deeper challenge: the copy problem. Even if one
                        achieved perfect attribution through deep voting, data sources cannot enforce how their
                        contributions are used because whoever possesses a copy of the model retains unilateral control.
                        Chapter 2 addresses this challenge, introducing techniques that enable attribution-based
                        <em>control</em> rather than mere attribution-based <em>suggestions</em>.
                    </p>
                </section>

                <section class="references">
                    <h2>References</h2>
                    <ol>
                        <li id="ref-1"><span class="authors">Kaplan, J., et al.</span> (2020). <a
                                href="https://arxiv.org/abs/2001.08361"><span class="title">Scaling Laws for Neural
                                    Language Models.</span></a> <span class="venue">arXiv:2001.08361</span>.</li>
                        <li id="ref-2"><span class="authors">Hoffmann, J., et al.</span> (2022). <a
                                href="https://arxiv.org/abs/2203.15556"><span class="title">Training Compute-Optimal
                                    Large Language Models.</span></a> <span class="venue">arXiv:2203.15556</span>.</li>
                        <li id="ref-3"><span class="authors">Robison, K.</span> (2024). <a
                                href="https://www.theverge.com/2024/12/13/24320811/openai-ilya-sutskever-agi-data-training"><span
                                    class="title">OpenAI cofounder Ilya Sutskever says the way AI is built is about to
                                    change.</span></a> <span class="venue">The Verge</span>.</li>
                        <li id="ref-4"><span class="authors">Borgeaud, S., et al.</span> (2022). <a
                                href="https://arxiv.org/abs/2112.04426"><span class="title">Improving language models by
                                    retrieving from trillions of tokens.</span></a> <span class="venue">ICML
                                2022</span>, 2206–2240.</li>
                        <li id="ref-5"><span class="authors">Izacard, G., et al.</span> (2023). <a
                                href="https://arxiv.org/abs/2208.03299"><span class="title">Atlas: Few-shot Learning
                                    with Retrieval Augmented Language Models.</span></a> <span class="venue">Journal of
                                Machine Learning Research</span>, 24, 1–43.</li>
                        <li id="ref-6"><span class="authors">Guo, Z., et al.</span> (2023). <a
                                href="https://arxiv.org/abs/2310.05773"><span class="title">Towards lossless dataset
                                    distillation via difficulty-aligned trajectory matching.</span></a> <span
                                class="venue">arXiv:2310.05773</span>.</li>
                        <li id="ref-7"><span class="authors">Epoch AI.</span> (2024). <a
                                href="https://epoch.ai/data/notable-ai-models"><span class="title">Data on Notable AI
                                    Models.</span></a> <span class="venue">epoch.ai/data/notable-ai-models</span>.</li>
                        <li id="ref-8"><span class="authors">Goodfellow, I., Bengio, Y., & Courville, A.</span> (2016).
                            <a href="https://www.deeplearningbook.org/"><span class="title">Deep Learning.</span></a>
                            <span class="venue">MIT Press</span>.
                        </li>
                        <li id="ref-9"><span class="authors">Kemker, R., et al.</span> (2018). <a
                                href="https://arxiv.org/abs/1708.02072"><span class="title">Measuring Catastrophic
                                    Forgetting in Neural Networks.</span></a> <span class="venue">AAAI 2018</span>.</li>
                        <li id="ref-10"><span class="authors">Cummins, M.</span> (2024). <a
                                href="https://educatingsilicon.substack.com/p/how-much-llm-training-data-is-there"><span
                                    class="title">How much LLM training data is there, in the limit?</span></a> <span
                                class="venue">Educating Silicon</span>.</li>
                        <li id="ref-11"><span class="authors">Le, Q. V., et al.</span> (2013). <a
                                href="https://arxiv.org/abs/1112.6209"><span class="title">Building high-level features
                                    using large scale unsupervised learning.</span></a> <span class="venue">IEEE
                                Transactions on Pattern Analysis and Machine Intelligence</span>.</li>
                        <li id="ref-12"><span class="authors">Krizhevsky, A., Sutskever, I., & Hinton, G. E.</span>
                            (2012). <a
                                href="https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html"><span
                                    class="title">ImageNet Classification with Deep Convolutional Neural
                                    Networks.</span></a> <span class="venue">NeurIPS 2012</span>.</li>
                        <li id="ref-13"><span class="authors">Zeiler, M. D., & Fergus, R.</span> (2014). <a
                                href="https://arxiv.org/abs/1311.2901"><span class="title">Visualizing and Understanding
                                    Convolutional Networks.</span></a> <span class="venue">ECCV 2014</span>.</li>
                        <li id="ref-14"><span class="authors">Chomsky, N.</span> (2014). <a
                                href="https://mitpress.mit.edu/9780262527408/aspects-of-the-theory-of-syntax/"><span
                                    class="title">Aspects of the Theory of Syntax.</span></a> <span class="venue">MIT
                                Press</span>.</li>
                        <li id="ref-15"><span class="authors">Dwork, C., et al.</span> (2006). <a
                                href="https://doi.org/10.1007/11681878_14"><span class="title">Calibrating noise to
                                    sensitivity in private data analysis.</span></a> <span class="venue">TCC
                                2006</span>.</li>
                        <li id="ref-16"><span class="authors">Abadi, M., et al.</span> (2016). <a
                                href="https://arxiv.org/abs/1607.00133"><span class="title">Deep Learning with
                                    Differential Privacy.</span></a> <span class="venue">CCS 2016</span>.</li>
                        <li id="ref-17"><span class="authors">Feldman, V., & Zrnic, T.</span> (2020). <a
                                href="https://arxiv.org/abs/2008.11193"><span class="title">Individual Privacy
                                    Accounting via a Renyi Filter.</span></a> <span
                                class="venue">arXiv:2008.11193</span>.</li>
                        <li id="ref-18"><span class="authors">Papernot, N., et al.</span> (2018). <a
                                href="https://arxiv.org/abs/1802.08908"><span class="title">Scalable Private Learning
                                    with PATE.</span></a> <span class="venue">ICLR 2018</span>.</li>
                        <li id="ref-19"><span class="authors">Zhao, Y., et al.</span> (2018). <a
                                href="https://arxiv.org/abs/1806.00582"><span class="title">Federated Learning with
                                    Non-IID Data.</span></a> <span class="venue">arXiv:1806.00582</span>.</li>
                        <li id="ref-20"><span class="authors">Ainsworth, S. K., et al.</span> (2022). <a
                                href="https://arxiv.org/abs/2209.04836"><span class="title">Git Re-Basin: Merging Models
                                    modulo Permutation Symmetries.</span></a> <span
                                class="venue">arXiv:2209.04836</span>.</li>
                        <li id="ref-21"><span class="authors">Nguyen, T. T., et al.</span> (2024). <a
                                href="https://arxiv.org/abs/2209.02299"><span class="title">A Survey of Machine
                                    Unlearning.</span></a> <span class="venue">arXiv:2209.02299</span>.</li>
                        <li id="ref-22"><span class="authors">Hochreiter, S., & Schmidhuber, J.</span> (1997). <a
                                href="https://doi.org/10.1162/neco.1997.9.8.1735"><span class="title">Long Short-Term
                                    Memory.</span></a> <span class="venue">Neural Computation</span>, 9(8), 1735–1780.
                        </li>

                        <li id="ref-23">
                            <span class="authors">J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R.
                                Child, S. Gray, A. Radford, J. Wu, and D. Amodei. 2020.</span>

                            <span class="venue">Scaling laws for neural language models. <em>CoRR</em>,
                                abs/2001.08361.</span>
                        </li>

                        <li id="ref-24">
                            <span class="authors">J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E.
                                Rutherford, D. de Las Casas,
                                L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den
                                Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals,
                                and L. Sifre. 2022.</span>

                            <span class="venue">Training compute-optimal large language models.</span>
                        </li>

                        <li id="ref-25">
                            <span class="authors">F. Strati, P. Elvinger, T. Kerimoglu, and A. Klimovic. 2024. </span>

                            <span class="venue">Ml training with cloud gpu
                                shortages: Is cross-region the answer? In <em>Proceedings of the 4th Workshop on Machine
                                    Learning and Systems</em>, EuroMLSys ’24, page 107–116, New York, NY, USA.
                                Association
                                for Computing Machinery.</span>
                        </li>

                        <li id="ref-26">
                            <span class="authors">
                                D. Kaye. 2025.
                            </span>

                            <span class="venue">
                                Nvidia hits new milestone as world’s first $5tn company, 10.
                            </span>
                        </li>

                        <li id="ref-27">
                            <span class="authors">
                                Z. Kachwala and A. Bajwa. 2025.
                            </span>

                            <span class="venue">
                                Nvidia faces revenue threat from new u.s. ai chip export
                                curbs, analysts say. <em>Reuters</em>, January. Updated January 13, 2025 6:31 PM GMT.
                            </span>
                        </li>

                        <li id="ref-28">
                            <span class="authors">
                                D. Howley. 2023.
                            </span>

                            <span class="venue">
                                There’s an ai war, and nvidia is the only arms dealer: Analyst. <em>Yahoo Finance</em>,
                                May. Updated May 25, 2023.
                            </span>
                        </li>

                        <li id="ref-29">
                            <span class="authors">
                                J. You. 2025.
                            </span>

                            <span class="venue">
                                Most of openai’s 2024 compute went to experiments. Accessed: 2025-11-02.
                            </span>
                        </li>

                        <li id="ref-30">
                            <span class="authors">
                                S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. B. Van Den
                                Driessche, J.-B. Lespiau, B. Damoc, A. Clark, et al. 2022.
                            </span>

                            <span class="venue">
                                Improving language models by
                                retrieving from trillions of tokens. In <em>International conference on machine
                                    learning</em>,
                                pages
                                2206–2240. PMLR.
                            </span>
                        </li>

                        <li id="ref-31">
                            <span class="authors">
                                J. Lederer. 2024.
                            </span>

                            <span class="venue">
                                Statistical guarantees for sparse deep learning. <em>AStA Advances in Statistical
                                    Analysis</em>, 108(2):231–258.
                            </span>
                        </li>

                        <li id="ref-32">
                            <span class="authors">
                                D. Dai, C. Deng, C. Zhao, R. Xu, H. Gao, D. Chen, J. Li, W. Zeng, X. Yu, Y. Wu, et al.
                                2024.
                            </span>

                            <span class="venue">
                                Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language
                                models. <em>arXiv preprint arXiv:2401.06066.</em>
                            </span>
                        </li>

                        <li id="ref-33">
                            <span class="authors">
                                Z. Guo, K. Wang, G. Cazenavette, H. Li, K. Zhang, and Y. You. 2023.
                            </span>

                            <span class="venue">
                                Towards lossless dataset
                                distillation via difficulty-aligned trajectory matching. <em>arXiv preprint
                                    arXiv:2310.05773.</em>
                            </span>
                        </li>

                        <li id="ref-34">
                            <span class="authors">
                                Meta AI. 2024.
                            </span>

                            <span class="venue">
                                Introducing llama 3.1: Our most capable models to date. <em>Meta AI Blog</em>, July.
                                Published July 23, 2024.
                            </span>
                        </li>

                        <li id="ref-35">
                            <span class="authors">
                                T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P.
                                Shyam,
                                G. Sastry, A. Askell, et al. 2020.
                            </span>

                            <span class="venue">
                                Language models are few-shot learners. <em>arXiv preprint
                                    arXiv:2005.14165.</em>
                            </span>
                        </li>

                        <li id="ref-36">
                            <span class="authors">
                                K. Wiggers. 2024.
                            </span>

                            <span class="venue">
                                Openai ceo sam altman says lack of compute capacity is delaying the
                                company’s products. <em>TechCrunch</em>, October. Published 12:37 PM PDT, October 31,
                                2024.
                            </span>
                        </li>

                        <li id="ref-37">
                            <span class="authors">
                                Epoch AI. 2025.
                            </span>

                            <span class="venue">
                                Data on ai models, 07. Accessed: 2025-11-02.
                            </span>
                        </li>

                        <li id="ref-38">
                            <span class="authors">
                                I. Goodfellow, Y. Bengio, and A. Courville. 2016.
                            </span>

                            <a href="https://www.deeplearningbook.org/">
                                <em>Deep Learning</em>. MIT Press.
                            </a>
                        </li>

                        <li id="ref-39">
                            <span class="authors">
                                I. Mehta. 2024.
                            </span>

                            <span class="venue">
                                Zuckerberg says Meta will need 10x more computing power to train Llama 4
                                than Llama 3. <em>TechCrunch</em>, aug. 12:53 AM PDT.
                            </span>
                        </li>

                        <li id="ref-40">
                            <span class="authors">
                                OpenAI. 2024.
                            </span>

                            <span class="venue">
                                Learning to reason with LLMs. sep. Introduces OpenAI o1, a new large
                                language model trained with reinforcement learning for complex reasoning
                            </span>
                        </li>

                        <li id="ref-41">
                            <span class="authors">
                                J. Sevilla and E. Roldan. 2024.
                            </span>

                            <span class="venue">
                                Training compute of frontier AI models grows by 4-5x per year. ´
                                <em>Epoch AI Blog</em>, may. Analysis of AI model compute trends showing 4-5x yearly
                                growth
                                from 2010 to 2024.
                            </span>
                        </li>

                        <li id="ref-42">
                            <span class="authors">
                                Weights & Biases. 2025.
                            </span>

                            <a href="https://docs.wandb.ai/models/tutorials/sweeps">
                                Sweeps: An overview. Online tutorial for using W&B Sweeps for hyperparameter
                                optimization.
                            </a>
                        </li>

                        <li id="ref-43">
                            <span class="authors">
                                A. K. Veldanda, S.-X. Zhang, A. Das, S. Chakraborty, S. Rawls, S. Sahu, and M. Naphade.
                                2024.
                            </span>

                            <span class="venue">
                                Llm surgery: Efficient knowledge unlearning and editing in large language models.
                                <em>arXiv preprint arXiv:2409.13054.</em>
                            </span>
                        </li>

                        <li id="ref-44">
                            <span class="authors">
                                I. Bratt. 2025.
                            </span>

                            <a
                                href="https://partners.wsj.com/nasdaq-arm/information-technology/why-there-is-no-ai-without-inference/">
                                Why there is no AI without inference. WSJ Partner Content
                                sponsored by Arm, Vice President of Machine-Learning Technology.
                            </a>
                        </li>

                        <li id="ref-45">
                            <span class="authors">
                                S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. B. Van Den
                                Driessche, J.-B. Lespiau, B. Damoc, A. Clark, et al. 2022.
                            </span>

                            <span class="venue">
                                Improving language models by
                                retrieving from trillions of tokens. In <em>International conference on machine
                                    learning</em>,
                                pages
                                2206–2240. PMLR.
                            </span>
                        </li>

                        <li id="ref-46">
                            <span class="authors">
                                G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. Dwivedi-Yu, A.
                                Joulin,
                                S. Riedel, and E. Grave. 2023
                            </span>

                            <span class="venue">
                                Atlas: Few-shot learning with retrieval augmented language
                                models. <em>Journal of Machine Learning Research</em>, 24(251):1–43.
                            </span>
                        </li>

                        <li id="ref-47">
                            <span class="authors">
                                J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K.
                                Milan,
                                J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, and
                                R. Hadsell. 2017
                            </span>

                            <span class="venue">
                                Overcoming catastrophic forgetting in neural networks. <em>Proceedings of
                                    the National Academy of Sciences</em>, 114(13):3521–3526.
                            </span>
                        </li>

                        <li id="ref-48">
                            <span class="authors">
                                R. Kemker, M. McClure, A. Abitino, T. Hayes, and C. Kanan. 2018.
                            </span>

                            <span class="venue">
                                Measuring catastrophic forgetting in neural networks. In <em>Proceedings of the AAAI
                                    conference on artificial intelligence</em>,
                                volume 32.
                            </span>
                        </li>

                        <li id="ref-49">
                            <span class="authors">
                                K. Robison. 2024.
                            </span>

                            <span class="venue">
                                OpenAI cofounder Ilya Sutskever says the way AI is built is about to change.
                                <em>The Verge</em>, December. Accessed: December 31, 2024.
                            </span>
                        </li>

                        <li id="ref-50">
                            <span class="authors">
                                Epoch AI. 2024.
                            </span>

                            <span class="venue">
                                Data on machine learning hardware. Updated December 30, 2024
                            </span>
                        </li>

                        <li id="ref-51">
                            <span class="authors">
                                Together. 2023.
                            </span>

                            <span class="venue">
                                Redpajama-data-v2: An open dataset with 30 trillion tokens for training large
                                language models, 10. The dataset includes data processing tools for CommonCrawl data,
                                focusing on five languages: English, French, Spanish, German, and Italian. It provides
                                over
                                40 quality annotations for filtering and weighting data, including natural language
                                indicators,
                                repetitiveness measures, and content-based signals.
                            </span>
                        </li>

                        <li id="ref-52">
                            <span class="authors">
                                A. S. Cummings. 2017.
                            </span>

                            <span class="venue">
                                <em>Democracy of sound: Music piracy and the remaking of American
                                    copyright in the twentieth century</em>. Oxford University Press.
                            </span>
                        </li>

                        <li id="ref-53">
                            <span class="authors">
                                M. Cummins. 2024.
                            </span>

                            <span class="venue">
                                How much llm training data is there, in the limit? <em>Educating Silicon</em>,
                                May. A comprehensive analysis of available text data for LLM training, including web
                                data,
                                code, academic publications, books, court documents, social media, transcribed audio,
                                and
                                private communications. Estimates suggest current LLM training sets are approaching the
                                limits of high-quality public text, with approximately 40-90T tokens available in
                                English
                                and 100-200T tokens across all languages
                            </span>
                        </li>

                        <li id="ref-54">
                            <span class="authors">
                                Wikipedia contributors. 2024.
                            </span>

                            <span class="venue">
                                Common Crawl. Last edited on 30 December 2024, at 01:48
                                (UTC).
                            </span>
                        </li>

                        <li id="ref-55">
                            <span class="authors">
                                B. Kahle. 2024.
                            </span>

                            <span class="venue">
                                A message from internet archive founder, brewster kahle. Internet Archive
                                donation page detailing the organization’s mission, impact, and ways to support. The
                                Archive
                                hosts over 99 petabytes of data, including 625 billion webpages, 38 million texts, and
                                14
                                million audio recordings. Federal Tax ID: 94-3242767.
                            </span>
                        </li>

                        <li id="ref-56">
                            <span class="authors">
                                D. Mider. 2024.
                            </span>

                            <span class="venue">
                                Open source intelligence on the internet – categorisation and evaluation of
                                search tools. <em> Internal Security Review</em>, 31:383–412.
                            </span>
                        </li>

                        <li id="ref-57">
                            <span class="authors">
                                P. Taylor. 2024.
                            </span>

                            <span class="venue">
                                Amount of data created, consumed, and stored 2010-2023, with forecasts to
                                2028, 5. Accessed: December 31, 2024.
                            </span>
                        </li>

                        <li id="ref-58">
                            <span class="authors">
                                T. Bogwasi. 2025.
                            </span>

                            <span class="venue">
                                Business facts: Essential business statistics you should know in 2025.
                            </span>
                        </li>

                        <li id="ref-59">
                            <span class="authors">
                                Q. V. Le. 2013.
                            </span>

                            <span class="venue">
                                Building high-level features using large scale unsupervised learning. In <em>2013
                                    IEEE international conference on acoustics, speech and signal processing</em>, pages
                                8595–8598.
                                IEEE.
                            </span>
                        </li>

                        <li id="ref-60">
                            <span class="authors">
                                T. T. Nguyen, T. T. Huynh, Z. Ren, P. L. Nguyen, A. W.-C. Liew, H. Yin, and Q. V. H.
                                Nguyen.
                                2024.
                            </span>

                            <span class="venue">
                                A survey of machine unlearning.
                            </span>
                        </li>

                        <li id="ref-61">
                            <span class="authors">
                                S. Hochreiter. 1998.
                            </span>

                            <span class="venue">
                                The vanishing gradient problem during learning recurrent neural nets and
                                problem solutions. <em>International Journal of Uncertainty, Fuzziness and
                                    Knowledge-Based
                                    Systems</em>, 6(02):107–116.
                            </span>
                        </li>

                        <li id="ref-62">
                            <span class="authors">
                                B. Hanin. 2018.
                            </span>

                            <span class="venue">
                                Which neural net architectures give rise to exploding and vanishing gradients?
                                <em> Advances in neural information processing systems</em>, 31.
                            </span>
                        </li>

                        <li id="ref-63">
                            <span class="authors">
                                A. Krizhevsky, I. Sutskever, and G. E. Hinton. 2012.
                            </span>

                            <span class="venue">
                                Imagenet classification with deep
                                convolutional neural networks. <em>Advances in neural information processing
                                    systems</em>, 25.
                            </span>
                        </li>

                        <li id="ref-64">
                            <span class="authors">
                                C. Dwork, F. McSherry, K. Nissim, and A. Smith. 2006.
                            </span>

                            <span class="venue">
                                Calibrating noise to sensitivity in
                                private data analysis. In <em>Theory of Cryptography: Third Theory of Cryptography
                                    Conference,
                                    TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings</em> 3, pages 265–284.
                                Springer
                            </span>
                        </li>

                        <li id="ref-65">
                            <span class="authors">
                                M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang.
                                2016.
                            </span>

                            <span class="venue">
                                Deep learning with differential privacy. In <em>Proceedings of the 2016 ACM SIGSAC
                                    Conference on Computer and Communications Security</em>, CCS’16. ACM, October
                            </span>
                        </li>

                        <li id="ref-66">
                            <span class="authors">
                                J. Soria-Comas, J. Domingo-Ferrer, D. Sanchez, and D. Meg ´ ´ıas. 2016.
                            </span>

                            <span class="venue">
                                Individual differential privacy: A utility-preserving formulation of differential
                                privacy guarantees. <em>CoRR</em>,
                                abs/1612.02298.
                            </span>
                        </li>

                        <li id="ref-67">
                            <span class="authors">
                                C. Dwork, A. Roth, et al. 2014.
                            </span>

                            <span class="venue">
                                The algorithmic foundations of differential privacy. e, Foundations
                                and Trends® in Theoretical Computer Science, 9(3–4):211–407.
                            </span>
                        </li>
                    </ol>
                </section>

                <nav class="chapter-nav">
                    <a href="index.html" class="prev">Chapter I: Introduction</a>
                    <a href="chapter3.html" class="next">Chapter III: Network-Source AI</a>
                </nav>
            </main>
        </div>

        <footer>
            <p>Andrew Trask &middot; University of Oxford</p>
        </footer>
    </div><!-- /.wrapper -->

    <img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQCa5kJX0IfZqgsgGUg5LB470wWtz3nfi_DuQ&s"
        alt="University of Oxford" class="oxford-logo-fixed">

    <script src='main.js' type='text/javascript' defer></script>
</body>

</html>