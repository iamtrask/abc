<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter I: From Deep Learning to Deep Voting | ABC in AI</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Serif+4:ital,opsz,wght@0,8..60,400;0,8..60,600;1,8..60,400&display=swap" rel="stylesheet">
    <!-- MathJax for LaTeX rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-8HDSX11G9D"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-8HDSX11G9D');
    </script>
    <!-- Privacy-friendly analytics by Plausible -->
    <script async src="https://plausible.io/js/pa-OtFEayY3RhHZVOzX3iV76.js"></script>
    <script>
      window.plausible=window.plausible||function(){(plausible.q=plausible.q||[]).push(arguments)},plausible.init=plausible.init||function(i){plausible.o=i||{}};
      plausible.init()
    </script>
</head>
<body>
    <nav>
        <a href="index.html">I. Introduction</a>
        <a href="chapter2.html" class="active">II. Deep Voting</a>
        <a href="chapter3.html">III. Network-Source AI</a>
        <a href="chapter4.html">IV. Broad Listening</a>
        <a href="chapter5.html">V. Conclusion</a>
        <a href="https://andrewtrask.com">About</a>
        <a href="appendix1.html">Appendix I</a>
        <a href="appendix2.html">Appendix II</a>
    </nav>

    <div class="page-container">
        <aside class="toc-sidebar">
            <h4>On This Page</h4>
            <ul>
                <li class="toc-h3"><a href="#chapter-summary">Chapter Summary</a></li>
                <li class="toc-h3"><a href="#symptom">The Symptom: Underutilization</a></li>
                <li class="toc-h4"><a href="#underutilized-compute">Underutilized Compute</a></li>
                <li class="toc-h4"><a href="#siloed-data">Siloed Data</a></li>
                <li class="toc-h4"><a href="#root-causes">Root Causes (Three Whys)</a></li>
                <li class="toc-h3"><a href="#first-why">First Why: ABC</a></li>
                <li class="toc-h3"><a href="#second-why">Second Why: Feature Mixing</a></li>
                <li class="toc-h3"><a href="#third-why">Third Why: Addition</a></li>
                <li class="toc-h3"><a href="#third-hypothesis">Third Hypothesis: Concatenation</a></li>
                <li class="toc-h3"><a href="#second-hypothesis">Second Hypothesis: Deep Voting</a></li>
                <li class="toc-h3"><a href="#first-hypothesis">First Hypothesis: 6+ OOM</a></li>
                <li class="toc-h3"><a href="#empirical-evidence">Empirical Evidence</a></li>
            </ul>
            <div class="ascii-decoration">
  ‚ñà‚ñà‚ñà‚ñà‚ñì‚ñì‚ñí‚ñí‚ñë‚ñë

   gradients
    adding
   erasing
   sources
            </div>
        </aside>

        <!-- Floating margin art -->
        <div class="ascii-margin">
            <div class="art-piece">
   neurons
   voting:

  ‚ï≠‚îÄ‚î¨‚îÄ‚î¨‚îÄ‚ïÆ
  ‚îÇ1‚îÇ0‚îÇ1‚îÇ
  ‚îÇ0‚îÇ1‚îÇ1‚îÇ
  ‚îÇ1‚îÇ1‚îÇ0‚îÇ
  ‚ï∞‚îÄ‚î¥‚îÄ‚î¥‚îÄ‚ïØ
            </div>
            <div class="art-piece">
   sparse
     vs
   dense

  ¬∑ ¬∑ ¬∑ ¬∑
    vs
  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
            </div>
            <div class="art-piece">
  üìö‚Üí[  ]‚Üíüí°
     ‚Üë
  retrieval
   beats
  memorize
            </div>
        </div>

        <main>
            <!-- Margin ASCII Art - positioned absolutely, scrolls with content -->
            <div class="ascii-art float" style="top: 50px;">
   DEEP LEARNING
      ‚îå‚îÄ‚îê‚îå‚îÄ‚îê‚îå‚îÄ‚îê
      ‚îÇA‚îÇ‚îÇB‚îÇ‚îÇC‚îÇ
      ‚îî‚î¨‚îò‚îî‚î¨‚îò‚îî‚î¨‚îò
       ‚îî‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îò
          ‚ñº
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ A+B+C ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     sources lost
<span class="caption">addition erases</span>
            </div>

            <div class="ascii-art pulse" style="top: 700px;">
   DEEP VOTING
      ‚îå‚îÄ‚îê‚îå‚îÄ‚îê‚îå‚îÄ‚îê
      ‚îÇA‚îÇ‚îÇB‚îÇ‚îÇC‚îÇ
      ‚îî‚îÇ‚îò‚îî‚îÇ‚îò‚îî‚îÇ‚îò
       ‚îÇ  ‚îÇ  ‚îÇ
       ‚ñº  ‚ñº  ‚ñº
    ‚îå‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îê
    ‚îÇA ‚îÇB ‚îÇC ‚îÇ
    ‚îî‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îò
    sources kept
<span class="caption">concatenate preserves</span>
            </div>

            <div class="ascii-art breathe" style="top: 1400px;">
     ¬∑‚îÄ‚îÄ‚îÄ¬∑‚îÄ‚îÄ‚îÄ¬∑‚îÄ‚îÄ‚îÄ¬∑
     ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ
     ¬∑‚îÄ‚îÄ‚îÄ¬∑‚îÄ‚îÄ‚îÄ¬∑‚îÄ‚îÄ‚îÄ¬∑
     sparse params

    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
    dense params

   96% wasted!
<span class="caption">retrieval wins</span>
            </div>

            <div class="ascii-art float" style="top: 2100px;">
  üìä Available Data:
  ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì

  üìä Data Used:
  ‚ñë

  &lt;0.0001% used
<span class="caption">6 OOM untapped</span>
            </div>

            <div class="ascii-art pulse" style="top: 2800px;">
   Œµ small ‚Üí privacy
   Œµ measured ‚Üí track
   Œµ large ‚Üí attribute

    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ differential‚îÇ
    ‚îÇ attribution ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
<span class="caption">DP repurposed</span>
            </div>

            <div class="ascii-art breathe" style="top: 3500px;">
   intelligence
     budgets:

   Œ≥[s]‚âà0 ‚Üí hide
   Œ≥[s]‚âà1 ‚Üí show

   ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
   ‚îÇsource control‚îÇ
   ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ
<span class="caption">fine-grained control</span>
            </div>

            <div class="ascii-art float" style="top: 4200px;">
   RETRO: 25x fewer
   ATLAS: 50x fewer
       params!

    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ retrieval  ‚îÇ
    ‚îÇ   beats    ‚îÇ
    ‚îÇ memorizing ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
<span class="caption">the evidence</span>
            </div>

            <div class="chapter-header">
                <p class="chapter-number">Chapter II</p>
                <h1>From Deep Learning to Deep Voting</h1>
            </div>

            <figure class="full-width">
                <img src="1411.3146/abc_ch2_2_v8.png" alt="The lack of ABC creates problems for data-owning institutions">
                <figcaption>The lack of ABC creates problems for data-owning institutions, problems which avert their sharing of data and compute for AI training, problems which are underpinned&mdash;at their core&mdash;by the overuse of addition within deep learning systems.</figcaption>
            </figure>

            <section id="chapter-summary" class="abstract">
                <h2>Chapter Summary</h2>
                <p>Estimates below suggest that models are trained using less than 1/1,000,000th of the world's data and AI compute productivity. Consequently, following AI's scaling laws, AI models possess capabilities which are insignificant compared to what existing data, compute, and algorithms could create.</p>
                <p>Yet, if AI models (and their capabilities) are the lifeblood of the AI industry, why are data and compute so underutilized? Why is AI capability so constrained? This chapter unpacks the cause of such a drastic resource under-utilization. It begins by linking resource utilization to attribution-based control (ABC). It then breaks attribution-based control into problems with attribution and control, which are themselves underpinned by deep learning's core philosophy of mixing dense features. This mixing is only problematic because of a specific technical choice: the use of addition to update model weights, which erases provenance information during gradient descent.</p>
            </section>

            <figure class="full-width">
                <img src="1411.3146/deep_learning_to_deep_voting_v3.png" alt="Deep Learning vs Deep Voting">
                <figcaption>Traditional open/closed-source deep learning systems (left) pool all data into a deep learning model (i.e. by adding weight updates) which is later used for predictions, while deep voting systems (right) learn weight parameters which remain partitioned by source (i.e. concatenated), but which are learned in a way that they can be rapidly synthesized on the fly.</figcaption>
            </figure>

            <section>
                <p>The chapter then explores alternatives to addition during the training process, revealing a fundamental trade off between three factors: AI capability (driven by unrestricted feature learning), attribution (tracking where features came from), and computational complexity (tracking the path of feature mixing). It proposes a key innovation, a re-purposing of differential privacy for attribution: <em>differential attribution</em>, using the natural boundaries of training documents to identify which concepts must mix freely and vice versa, thereby pushing this Pareto frontier by providing a data-driven approach to balance addition and concatenation.</p>
                <p>Building on this insight, the chapter develops a specific form of concatenation to replace addition in key sections of the deep learning training process. This transformation&mdash;from deep learning to <strong>deep voting</strong>&mdash;cascades upward through the aforementioned hierarchy of problems, reducing the need for dense feature mixing across data sources, enabling attribution-based control, and unlocking a viable path towards another 6+ orders of magnitude of training data and compute productivity.</p>
            </section>

            <section id="symptom">
                <h2>The Symptom: Data/Compute Underutilization</h2>

                <figure class="full-width">
                    <img src="1411.3146/abc_ch2_2_v9.png" alt="Problem tree">
                    <figcaption>The lack of ABC creates problems for data-owning institutions, problems which avert their sharing of data and compute for AI training, problems which are underpinned by the overuse of addition within deep learning systems.</figcaption>
                </figure>

                <p>As of NeurIPS 2024, leading AI researchers have reported that available compute and data reserves are approaching saturation, creating constraints on both computational resources and pre-training scale. However, this assessment overlooks approximately six orders of magnitude of underutilized compute productivity and siloed data. Rather than absolute scarcity, the industry faces structural problems of data and compute access and productivity.</p>

                <h3 id="underutilized-compute">6+ OOM: Underutilized Training Compute Productivity</h3>

                <figure class="full-width">
                    <img src="1411.3146/abc_ch2_2_1_v8.png" alt="Compute inefficiency tree">
                    <figcaption>A tree of sub-problems regarding inefficient training compute productivity.</figcaption>
                </figure>

                <p>The AI industry's computational requirements have driven significant economic and geopolitical consequences, including NVIDIA's rise to become the world's most valuable company, U.S. export restrictions on AI chips to China, and intense competition for latest-generation hardware among startups, enterprises, and major technology firms. However, recent evidence suggests that current AI training and inference processes utilize less than 0.0002% of available compute productivity, indicating that perceived compute scarcity may reflect inefficiency rather than absolute resource limits.</p>

                <h4>2-3 OOM: Inefficient AI inference</h4>

                <div class="callout">
                    <p class="callout-title">A Library Analogy</p>
                    <p>Consider a library. When someone asks a librarian about the rules of chess, the librarian doesn't subsequently read <em>every book in the library</em> to find the answer. Instead, they use the catalog system to find a relevant bookshelf, the titles of books on that shelf to find the relevant book, and the table of contents of that book to find the relevant section. This practice stands in stark contrast to how AI systems process information. To make an AI prediction with a model like GPT-3, AI users forward propagate <em>through the entire model and all of its knowledge</em> (i.e., read every book in the library). And in the case of large language models, they don't just do this once per answer, they do this <em>for every token they predict</em>.</p>
                </div>

                <p>DeepMind's RETRO achieves comparable performance to GPT-3 while using 1/25th of the parameters through retrieval from a large-scale vector database. Similarly, Meta's ATLAS demonstrates that models can be reduced to 1/50th their original size while maintaining or exceeding baseline performance through database-augmented inference.</p>

                <p>Recent work demonstrates that current architectures contain redundant and underutilized parameters. Guo et al. achieve 5-10x reduction in parameter count through lossless compression while maintaining accuracy:</p>

                <div class="table-container">
                    <table class="data-table">
                        <caption>Table: Comparison with previous dataset distillation methods on CIFAR-10, CIFAR-100 and Tiny ImageNet. Highlighted results indicate lossless distillation. (From Guo et al., 2023)</caption>
                        <thead>
                            <tr>
                                <th>Dataset</th>
                                <th colspan="5">CIFAR-10</th>
                                <th colspan="4">CIFAR-100</th>
                                <th colspan="3">Tiny ImageNet</th>
                            </tr>
                            <tr>
                                <th>IPC</th>
                                <th>1</th><th>10</th><th>50</th><th>500</th><th>1000</th>
                                <th>1</th><th>10</th><th>50</th><th>100</th>
                                <th>1</th><th>10</th><th>50</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td>Random</td><td>15.4</td><td>31.0</td><td>50.6</td><td>73.2</td><td>78.4</td><td>4.2</td><td>14.6</td><td>33.4</td><td>42.8</td><td>1.4</td><td>5.0</td><td>15.0</td></tr>
                            <tr><td>DC</td><td>28.3</td><td>44.9</td><td>53.9</td><td>72.1</td><td>76.6</td><td>12.8</td><td>25.2</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr>
                            <tr><td>DM</td><td>26.0</td><td>48.9</td><td>63.0</td><td>75.1</td><td>78.8</td><td>11.4</td><td>29.7</td><td>43.6</td><td>-</td><td>3.9</td><td>12.9</td><td>24.1</td></tr>
                            <tr><td>MTT</td><td>46.2</td><td>65.4</td><td>71.6</td><td>-</td><td>-</td><td>24.3</td><td>39.7</td><td>47.7</td><td>49.2</td><td>8.8</td><td>23.2</td><td>28.0</td></tr>
                            <tr><td>RCIG</td><td>53.9</td><td>69.1</td><td>73.5</td><td>-</td><td>-</td><td>39.3</td><td>44.1</td><td>46.7</td><td>-</td><td>25.6</td><td>29.4</td><td>-</td></tr>
                            <tr class="highlight"><td><strong>DATM (Ours)</strong></td><td>46.9</td><td><strong>66.8</strong></td><td><strong>76.1</strong></td><td><strong>83.5</strong></td><td class="lossless"><strong>85.5</strong></td><td><strong>27.9</strong></td><td><strong>47.2</strong></td><td><strong>55.0</strong></td><td class="lossless"><strong>57.5</strong></td><td><strong>17.1</strong></td><td><strong>31.1</strong></td><td class="lossless"><strong>39.7</strong></td></tr>
                            <tr class="baseline"><td>Full Dataset</td><td colspan="5">84.8</td><td colspan="4">56.2</td><td colspan="3">37.6</td></tr>
                        </tbody>
                    </table>
                </div>

                <p>We adopt RETRO/ATLAS-style parameter efficiency as a conservative lower bound on current compute waste. These results suggest that at least 96-98% of parameters activated during dense inference are unnecessary for individual queries.</p>

                <h4>6 OOM: Underutilized and Inefficient Compute in AI Learning</h4>

                <div class="callout">
                    <p class="callout-title">A Library Analogy</p>
                    <p>As before, consider a library. When a library adds or removes a significant number of books to/from their collection, they don't <em>rebuild the entire building and re-print all of their books from scratch</em>, they simply add/remove books, shelves, or rooms. These practices stand in stark contrast to how AI systems process information. To add or remove a significant portion of knowledge from a deep learning system, AI researchers <em>retrain them from scratch</em> (i.e., tear down the entire library, burn all the books, rebuild the library, and re-print all the books from scratch).</p>
                </div>

                <p>Analysis of the largest AI firms reveals that pre-training their most capable models consumes less than 1% of quarterly compute budgets (see table below). Yet these same firms continue expanding computational infrastructure to support larger models, suggesting that remaining compute capacity is allocated to other training activities rather than final model production.</p>

                <div class="table-container">
                    <table class="data-table">
                        <caption>Table: Model Compute Summary. The far right columns present the estimated compute usage by large AI models relative to their parent organization's capacity.</caption>
                        <thead>
                            <tr>
                                <th>Model</th>
                                <th>Organization</th>
                                <th>Training FLOPs</th>
                                <th>Parent Org Peak Annual FLOPs</th>
                                <th>Model/Peak Annual (%)</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td>Gemini 1.0 Ultra</td><td>Google DeepMind</td><td>$5.00 \times 10^{25}$</td><td>$3.87 \times 10^{28}$</td><td>0.129</td></tr>
                            <tr><td>Claude 3.5 Sonnet</td><td>Anthropic</td><td>$4.98 \times 10^{25}$</td><td>$2.27 \times 10^{28}$</td><td>0.220</td></tr>
                            <tr><td>GPT-4o</td><td>OpenAI</td><td>$3.81 \times 10^{25}$</td><td>$4.35 \times 10^{28}$</td><td>0.088</td></tr>
                            <tr><td>Llama 3.1-405B</td><td>Meta AI</td><td>$3.80 \times 10^{25}$</td><td>$5.65 \times 10^{28}$</td><td>0.067</td></tr>
                            <tr><td>GPT-4</td><td>OpenAI</td><td>$2.10 \times 10^{25}$</td><td>$4.35 \times 10^{28}$</td><td>0.048</td></tr>
                            <tr><td>Gemini 1.0 Pro</td><td>Google DeepMind</td><td>$1.83 \times 10^{25}$</td><td>$3.87 \times 10^{28}$</td><td>0.047</td></tr>
                            <tr><td>Claude 3 Opus</td><td>Anthropic</td><td>$1.64 \times 10^{25}$</td><td>$2.27 \times 10^{28}$</td><td>0.072</td></tr>
                            <tr><td>Llama 3-70B</td><td>Meta AI</td><td>$7.86 \times 10^{24}$</td><td>$5.65 \times 10^{28}$</td><td>0.014</td></tr>
                            <tr><td>GPT-4o mini</td><td>OpenAI</td><td>$7.36 \times 10^{24}$</td><td>$4.35 \times 10^{28}$</td><td>0.017</td></tr>
                            <tr><td>PaLM 2</td><td>Google</td><td>$7.34 \times 10^{24}$</td><td>$3.87 \times 10^{28}$</td><td>0.019</td></tr>
                        </tbody>
                    </table>
                </div>

                <div class="table-container">
                    <table class="data-table">
                        <caption>Table: Estimated Total Worldwide AI Computing Capacity (Q4 2024)</caption>
                        <thead>
                            <tr>
                                <th>Category</th>
                                <th>Computing Power (FLOP/s)</th>
                                <th>Share (%)</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td colspan="3"><em>Cloud/AI Providers</em></td></tr>
                            <tr><td>Meta</td><td>$1.79 \times 10^{21}$</td><td>5.57</td></tr>
                            <tr><td>Microsoft/OpenAI</td><td>$1.38 \times 10^{21}$</td><td>4.29</td></tr>
                            <tr><td>Google/DeepMind</td><td>$1.23 \times 10^{21}$</td><td>3.81</td></tr>
                            <tr><td>Amazon/Anthropic</td><td>$7.19 \times 10^{20}$</td><td>2.23</td></tr>
                            <tr><td colspan="3"><em>Consumer Computing</em></td></tr>
                            <tr><td>Smartphones</td><td>$7.48 \times 10^{21}$</td><td>23.23</td></tr>
                            <tr><td>PC CPUs/GPUs</td><td>$2.23 \times 10^{21}$</td><td>6.92</td></tr>
                            <tr><td>Game Consoles</td><td>$8.64 \times 10^{20}$</td><td>2.68</td></tr>
                            <tr><td>Other Cloud/Pre-2023</td><td>$1.65 \times 10^{22}$</td><td>51.28</td></tr>
                            <tr class="baseline"><td><strong>Total</strong></td><td>$3.22 \times 10^{22}$</td><td>100.00</td></tr>
                        </tbody>
                    </table>
                </div>

                <div class="callout">
                    <p class="callout-title">A Full Picture of Compute Waste</p>
                    <p>Consider first how an AI system would operate as a librarian. When someone asks about the rules of chess, this librarian doesn't merely consult the games section. Instead, they read <em>every single book in the library</em>. Not just once, they do this for <em>every single query</em>. When this AI librarian needs to add a new book to their collection, they don't simply locate an appropriate shelf using a catalog system. Instead they first read <em>every book in the library</em>, then displace existing books onto the floor to make space, then must <em>re-read everything</em> to determine where to relocate those displaced books. This process repeats, sometimes <em>trillions of times</em>, until the library reaches a new equilibrium.</p>
                    <p>Now consider how human librarians process information. When someone inquires about chess, they navigate directly to the games section, select a relevant text, and locate the rules. When adding a new book, they utilize the Dewey Decimal system to identify the appropriate shelf and place it there. The process is direct, efficient, and purposeful.</p>
                </div>

                <div class="table-container">
                    <table class="data-table">
                        <caption>Table: Summary of Major AI System Inefficiencies</caption>
                        <thead>
                            <tr>
                                <th>Inefficiency Type</th>
                                <th>Range</th>
                                <th>Evidence</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td colspan="3"><em>Inference Inefficiencies:</em></td></tr>
                            <tr><td>Full Forward Propagation</td><td>25-50x+</td><td>RETRO/ATLAS</td></tr>
                            <tr><td>Parameter Redundancy</td><td>5-10x+</td><td>Compression</td></tr>
                            <tr><td>Catastrophic Forgetting</td><td>40x</td><td>Size Heuristic</td></tr>
                            <tr><td colspan="3"><em>Training Inefficiencies:</em></td></tr>
                            <tr><td>Re-training from Scratch</td><td>68x+</td><td>Industry Analysis</td></tr>
                            <tr><td>Full Forward Propagation</td><td>25-50x+</td><td>RETRO/ATLAS</td></tr>
                            <tr><td>Parameter Redundancy</td><td>5-10x+</td><td>Compression</td></tr>
                            <tr><td>Siloed Compute</td><td>~17.95x</td><td>Global Compute</td></tr>
                            <tr><td>Catastrophic Forgetting</td><td>40x</td><td>Size Heuristic</td></tr>
                            <tr><td colspan="3"><em>Combined Effects:</em></td></tr>
                            <tr class="highlight"><td>Inference Total</td><td>5,000-20,000x+</td><td>Multiplicative</td></tr>
                            <tr class="highlight"><td>Training Total</td><td>6,103,000-24,412,000x+</td><td>Multiplicative</td></tr>
                        </tbody>
                    </table>
                </div>

                <h3 id="siloed-data">6+ OOM: Siloed Data</h3>

                <figure class="full-width">
                    <img src="1411.3146/abc_ch2_2_2_v5.png" alt="Data siloing tree">
                    <figcaption>A tree of sub-problems regarding data siloing.</figcaption>
                </figure>

                <p>Following growing rumors across the AI research community that data is becoming a major bottleneck, OpenAI's former chief scientist, Ilya Sutskever, announced during his test of time award speech at NeurIPS 2024 that data for training AI has peaked, "We've achieved peak data and there'll be no more". However, while this may be true for the AI industry, Ilya's statement does not reflect the reality of what data exists in the world.</p>

                <div class="callout">
                    <p class="callout-title">A Library Analogy</p>
                    <p>Consider a world where libraries could only acquire books through anonymous donations left on their doorstep. No matter how many valuable books exist in private collections, university archives, or government repositories, libraries would be limited to what people voluntarily abandon. In such a world, librarians might reasonably conclude they're "running out of books", even while surrounded by vast, inaccessible collections within surrounding businesses and homes.</p>
                    <p>This mirrors the current state of AI training. When frontier models like GPT-4 (trained on 6.5 trillion tokens), and Qwen2.5-72B (18 trillion tokens), LLama 4 (30 trillion tokens) report hitting data limits, they're really hitting access limits. They're not running out of data, they're running out of data they can freely collect.</p>
                </div>

                <p>The scale of untapped data is staggering:</p>

                <div class="table-container">
                    <table class="data-table">
                        <caption>Table: Estimated Volume of Public Digital Text Content (relative size uses Llama 3 = 1 as reference)</caption>
                        <thead>
                            <tr>
                                <th>Category &amp; Source</th>
                                <th>Words (T)</th>
                                <th>Tokens (T)</th>
                                <th>Rel. Size</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td colspan="4"><em>Web Data</em></td></tr>
                            <tr><td>FineWeb</td><td>11</td><td>15</td><td>1.0</td></tr>
                            <tr><td>Non-English Common Crawl (high quality)</td><td>13.5</td><td>18</td><td>1.0</td></tr>
                            <tr><td>All high quality web text</td><td>45-120</td><td>60-160</td><td>4.0-11.0</td></tr>
                            <tr><td colspan="4"><em>Code</em></td></tr>
                            <tr><td>Public code</td><td>-</td><td>0.78</td><td>0.05</td></tr>
                            <tr><td>Private Code</td><td>-</td><td>20</td><td>1.3</td></tr>
                            <tr><td colspan="4"><em>Academic &amp; Legal</em></td></tr>
                            <tr><td>Academic articles</td><td>0.8</td><td>1</td><td>0.07</td></tr>
                            <tr><td>Patents</td><td>0.15</td><td>0.2</td><td>0.01</td></tr>
                            <tr><td colspan="4"><em>Books</em></td></tr>
                            <tr><td>Google Books</td><td>3.6</td><td>4.8</td><td>0.3</td></tr>
                            <tr><td>Anna's Archive</td><td>2.8</td><td>3.9</td><td>0.25</td></tr>
                            <tr><td>Every unique book</td><td>16</td><td>21</td><td>1.4</td></tr>
                            <tr><td>US federal court documents</td><td>2</td><td>2.7</td><td>0.2</td></tr>
                        </tbody>
                    </table>
                </div>

                <div class="table-container">
                    <table class="data-table">
                        <caption>Table: Estimated Volume of Social Media, Audio, and Private Data (relative size uses Llama 3 = 1 as reference)</caption>
                        <thead>
                            <tr>
                                <th>Category &amp; Source</th>
                                <th>Words (T)</th>
                                <th>Tokens (T)</th>
                                <th>Rel. Size</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td colspan="4"><em>Social Media</em></td></tr>
                            <tr><td>Twitter / X</td><td>8</td><td>11</td><td>0.7</td></tr>
                            <tr><td>Weibo</td><td>29</td><td>38</td><td>2.5</td></tr>
                            <tr><td>Facebook</td><td>105</td><td>140</td><td>10.0</td></tr>
                            <tr><td colspan="4"><em>Audio (Transcribed)</em></td></tr>
                            <tr><td>YouTube</td><td>5.2</td><td>7</td><td>0.5</td></tr>
                            <tr><td>TikTok</td><td>3.7</td><td>4.9</td><td>0.3</td></tr>
                            <tr><td>All podcasts</td><td>0.56</td><td>0.75</td><td>0.05</td></tr>
                            <tr><td colspan="4"><em>Private Data</em></td></tr>
                            <tr class="highlight"><td>All stored instant messages</td><td>500</td><td>650</td><td>45.0</td></tr>
                            <tr class="highlight"><td>All stored email</td><td>900</td><td>1200</td><td>80.0</td></tr>
                            <tr><td colspan="4"><em>Total Human Communication</em></td></tr>
                            <tr><td>Daily</td><td>115</td><td>150</td><td>10</td></tr>
                            <tr><td>Since 1800</td><td>3,000,000</td><td>4,000,000</td><td>$10^5$</td></tr>
                            <tr><td>All time</td><td>6,000,000</td><td>8,000,000</td><td>$10^5$</td></tr>
                        </tbody>
                    </table>
                </div>

                <p>Stored email and instant messages alone contain over 1,850 trillion tokens, approximately 60 times the largest known training dataset. Daily human communication generates approximately 150 trillion tokens, accumulating to roughly 55 quadrillion tokens annually.</p>

                <p>Yet even this vast sea of text represents merely a drop in the ocean of total digital data. While frontier AI models train on curated web scrapes such as Common Crawl (454 TB as of December 2023), the Internet Archive's Wayback Machine alone stores approximately 100 petabytes. Meanwhile, global digital data is projected to reach 180 zettabytes by 2025, six orders of magnitude larger than The Internet Archive and nine orders of magnitude larger than the largest known training datasets.</p>
            </section>

            <section id="root-causes">
                <h3>The Search for Root Causes (Three "Whys")</h3>

                <figure class="full-width">
                    <img src="1411.3146/abc_ch2_2_3_v6.png" alt="Cascade of causes">
                    <figcaption>The cascade of causes between the addition problem and ABC.</figcaption>
                </figure>

                <p>The previous section revealed a paradox: despite widespread beliefs of data and compute scarcity, AI systems access less than one millionth of digital resources. This under-utilization raises a critical question: if more data and compute directly improves AI capabilities through scaling laws, why do AI systems use such a tiny fraction of what's available? The answer lies in a cascade of technical and institutional barriers:</p>

                <ul>
                    <li><strong>First Why:</strong> Attribution-based Control</li>
                    <li><strong>Second Why:</strong> Deep Learning's Feature Mixing Precludes Partitioning</li>
                    <li><strong>Third Why (Root Cause):</strong> Addition of Source-Separable Concepts</li>
                </ul>
            </section>

            <section id="first-why">
                <h2>First Why: Attribution-based Control</h2>

                <figure class="full-width">
                    <img src="1411.3146/abc_ch2_3_0_v5.png" alt="First Why graph">
                    <figcaption>Subset of conceptual graph regarding Section 2.3</figcaption>
                </figure>

                <p>The previous section revealed significant inefficiencies in the training of AI systems: 6+ orders of magnitude in underutilized data and compute. While there may be multiple contributing factors to these constraints, this thesis examines one particular root cause: AI's inability to provide attribution-based control (ABC). An AI model possesses attribution-based control when two properties are true: data sources control which AI predictions they support, AI users control which data sources they rely upon for an AI prediction.</p>

                <p>ABC implies certain architectural properties as novel requirements:</p>

                <ul>
                    <li><strong>Source-Partitionable Representations:</strong> Knowledge within an AI system is partition-able by source</li>
                    <li><strong>Rapid Partition Synthesis:</strong> Partitions are rapidly synthesize-able during inference</li>
                </ul>

                <h3>ABC and Compute Productivity (6+ OOM)</h3>
                <p>ABC would address compute productivity issues along two dimensions: access and learning structure. Regarding structure, successful ABC would necessarily provide a means to structure the learning and representation process, reducing re-training, forward propagation, redundancy, and catastrophic forgetting.</p>

                <p>RETRO and ATLAS demonstrate the minimum scale of such a breakthrough. By maintaining source-based partitions through their database architecture, they achieve equal performance while activating only 2-4% of the parameters of similarly performant dense models.</p>

                <h3>How Failing ABC Siloes Data (6 OOM)</h3>
                <p>Recall that current AI models can only train on data they can access. Consequently, AI models almost certainly train on less than 1/1,000,000th of the digitized information in the world because they cannot access the other 99.9999%, which remains hidden amongst the world's 360 million companies, 8+ billion citizens, etc.</p>

                <p>Successful ABC would necessarily enable one particularly compelling form of data sharing. The ability for a data source to decide which AI predictions to support is (almost tautologically) the ability for a data source to enable uses while averting mis-uses. One could argue that truly successful ABC would constitute an incentive shift attracting all of the world's data to be pulled into at least some AI predictions.</p>
            </section>

            <section id="second-why">
                <h2>Second Why: Deep Learning's Feature Mixing Precludes Partitioning</h2>

                <figure class="full-width">
                    <img src="1411.3146/abc_ch2_4_0_v5.png" alt="Second Why graph">
                    <figcaption>Subset of conceptual graph highlighting the Second Why and focus of this section.</figcaption>
                </figure>

                <p>The previous section revealed how solving attribution-based control would necessarily enable massive data and compute gains in AI systems. Yet this raises a deeper question: why do current AI systems fail to maintain attribution in the first place? The answer lies in deep learning's foundational premise: algorithms should learn everything from scratch through layers of (largely) unrestricted feature mixing on raw data.</p>

                <p>This commitment to unrestricted learning manifests in how neural networks fundamentally process information. Through operations that combine and mix information at every step (from layer computations to weight updates to knowledge accumulation) neural networks create increasingly complex representations of patterns in their training data. While this flexibility enables powerful pattern recognition, it creates a fundamental problem: features become stored in a disorganized, obfuscated way within the deep learning model... a black box.</p>

                <div class="callout">
                    <p class="callout-title">A Library Analogy</p>
                    <p>Consider a library wherein all of the books have had their covers removed, their table of contents erased, and their chapters torn out and shuffled amongst all the books. Consequently, when someone wants to answer a specific question, they have to read through the entire library searching for relevant information for their query.</p>
                    <p>Deep learning stores information in a similar way, with so-called <em>distributed representations</em> spreading concepts across many neurons... each of which is unlabeled (i.e. "hidden"). Far from an accident, this form of learning is at the center of deep learning's core philosophy, the unrestricted learning of dense, hidden features.</p>
                </div>
            </section>

            <section id="third-why">
                <h2>Third Why (Root Cause): Addition of Source-Separable Concepts</h2>

                <figure class="full-width">
                    <img src="1411.3146/abc_ch2_5_0_v5.png" alt="Third Why graph">
                    <figcaption>Subset of conceptual graph highlighting the Third Why and the focus of this section.</figcaption>
                </figure>

                <p>The previous section revealed how deep learning's feature mixing precludes the partitioning required for attribution-based control. Yet this raises our final "why": what makes this mixing fundamentally irreversible? The answer lies in deep learning's most basic mathematical operation: addition.</p>

                <p>Addition might seem like an implementation detail, but it fundamentally prevents recovering source information. When values combine through addition, the result does not uniquely determine its inputs&mdash;multiple distinct source combinations produce identical outputs:</p>

                <div class="definition-box">
                    <h3>Non-Injectivity of Addition</h3>
                    <p>Addition is not injective: for any sum y, there exist infinitely many distinct pairs (x<sub>1</sub>, x<sub>2</sub>) and (x'<sub>1</sub>, x'<sub>2</sub>) such that x<sub>1</sub> + x<sub>2</sub> = y = x'<sub>1</sub> + x'<sub>2</sub> where (x<sub>1</sub>, x<sub>2</sub>) &ne; (x'<sub>1</sub>, x'<sub>2</sub>).</p>
                </div>

                <div class="definition-box">
                    <h3>Concatenation vs Addition</h3>
                    <p><strong>Concatenation preserves sources:</strong><br>
                    "1" &oplus; "6" = "16"<br>
                    "2" &oplus; "5" = "25"<br>
                    (distinct inputs &rarr; distinct outputs)</p>
                    <p><strong>Addition erases them:</strong><br>
                    1 + 6 = 7<br>
                    2 + 5 = 7<br>
                    (distinct inputs &rarr; identical outputs)</p>
                </div>

                <p>When partitions are known, concatenation of numbers is injective; different inputs produce different outputs, allowing source recovery. Addition is not; the output 7 could arise from 1+6, 2+5, 3+4, 0+7, or infinitely many other combinations. This non-injectivity is the mechanism through which deep neural networks erase attribution information: once gradients from different sources are summed into shared parameters, no function of those parameters can recover which sources contributed what information.</p>

                <p><strong>The Root Problem:</strong> Without the ability to track sources through training, we cannot provide the attribution that ABC requires. Without attribution, we cannot enable the partitioned sharing and use of data and compute that could unlock orders of magnitude more AI resources. Addition itself blocks the very data and compute gains described earlier in this chapter.</p>
            </section>

            <section id="third-hypothesis">
                <h2>Third Hypothesis: Concatenating Along Natural Boundaries Enables Attribution</h2>

                <figure class="full-width">
                    <img src="1411.3146/abc_ch2_6_0_v6.png" alt="Third Hypothesis graph">
                    <figcaption>Subset of concept graph highlighting the Third Hypothesis and focus of this section.</figcaption>
                </figure>

                <p>The previous sections revealed how addition in deep learning creates a fundamental barrier to attribution. Yet examining why addition fails suggests a testable hypothesis: can we significantly reduce the use of addition, perhaps swapping it with concatenation?</p>

                <p>Some information patterns appear ubiquitously: basic rules of grammar that structure language, logical operations that appear in reasoning, morphological patterns which make up words, edges and corners in images, etc. Such dense patterns suggest unrestricted mixing through addition may be appropriate for a core subset of features. Their ubiquity also makes attribution less critical; they represent shared computational tools rather than source-specific claims about the world. While perhaps not formally stated, noted researcher Andrej Karpathy recently suggested a similar concept when referring to a future LLM "cognitive core":</p>

                <div class="callout" style="background: #f5f5f5; border-left: 3px solid #888;">
                    <p style="font-style: italic;">The race for LLM "cognitive core"‚Äîa few billion param model that maximally sacrifices encyclopedic knowledge for capability. It lives always-on and by default on every computer as the kernel of LLM personal computing. Its features are slowly crystalizing:</p>
                    <ul style="font-style: italic;">
                        <li>Natively multimodal text/vision/audio at both input and output.</li>
                        <li>Matryoshka-style architecture allowing a dial of capability up and down at test time.</li>
                        <li>Reasoning, also with a dial. (system 2)</li>
                        <li>Aggressively tool-using.</li>
                        <li>On-device finetuning LoRA slots for test-time training, personalization and customization.</li>
                        <li>Delegates and double checks just the right parts with the oracles in the cloud if internet is available.</li>
                    </ul>
                    <p style="font-style: italic;">It doesn't know that William the Conqueror's reign ended in September 9 1087, but it vaguely recognizes the name and can look up the date. It can't recite the SHA-256 of empty string as e3b0c442..., but it can calculate it quickly should you really want it.</p>
                    <p style="font-style: italic;">What LLM personal computing lacks in broad world knowledge and top tier problem-solving capability it will make up in super low interaction latency (especially as multimodal matures), direct / private access to data and state, offline continuity, sovereignty ("not your weights not your brain"). i.e. many of the same reasons we like, use and buy personal computers instead of having thin clients access a cloud via remote desktop or so.</p>
                    <p style="text-align: right;">‚Äî Andrej Karpathy<sup><a href="https://x.com/karpathy/status/1938626382248149433">[link]</a></sup></p>
                </div>

                <p>In contrast, perhaps most information is encyclopedic and appears sparsely: specific facts about the world, domain expertise in particular fields, claims made by individual sources, etc. The capital of France, the rules of chess, statistics about pizza... each appears in distinct contexts with limited overlap.</p>

                <p>A key insight of this chapter is that techniques from privacy-preserving machine learning, particularly differential privacy (DP), provide a principled way to measure and control which features benefit from dense mixing versus sparse representation. Differential privacy quantifies how much a model's outputs depend on any individual training example:</p>

                <div class="definition-box">
                    <h3>(Œµ, Œ¥)-Differential Privacy</h3>
                    <p>A randomized mechanism $\mathcal{M}: \mathcal{D} \to \mathcal{R}$ satisfies (Œµ, Œ¥)-differential privacy if for all adjacent datasets $D, D' \in \mathcal{D}$ (differing in one example) and all subsets of outputs $S \subseteq \mathcal{R}$:</p>
                    <p style="text-align: center;">$\Pr[\mathcal{M}(D) \in S] \leq e^\epsilon \cdot \Pr[\mathcal{M}(D') \in S] + \delta$</p>
                    <p>where small Œµ indicates strong privacy‚Äîoutputs barely depend on any individual example.</p>
                </div>

                <p>The parameter Œµ provides a quantitative measure of individual example influence on outputs. This same measure can serve three distinct control objectives:</p>

                <div class="definition-box">
                    <h3>Three Regimes of Influence Control</h3>
                    <ul>
                        <li><strong>Privacy (constrain influence):</strong> Enforce &epsilon;<sub>e</sub> &lt; &tau;<sub>min</sub> for all examples</li>
                        <li><strong>Measurement (track influence):</strong> Compute &epsilon;<sub>e</sub> for each example</li>
                        <li><strong>Attribution (ensure influence):</strong> Enforce &epsilon;<sub>e</sub> &gt; &tau;<sub>max</sub> for specified examples</li>
                    </ul>
                </div>
            </section>

            <section id="second-hypothesis">
                <h2>Second Hypothesis: Deep Voting with Intelligence Budgets</h2>

                <figure class="full-width">
                    <img src="1411.3146/abc_ch2_7_0_v5.png" alt="Second Hypothesis graph">
                    <figcaption>Subset of concept graph highlighting the Second Hypothesis and focus of this section.</figcaption>
                </figure>

                <p>The previous section revealed how privacy mechanisms might naturally separate dense from sparse information patterns. Yet this theoretical insight raises a practical question: how do we actually <em>implement</em> the measurement and control regimes we defined? The definitions in the previous section assumed worst-case bounds (requiring that Œµ constraints hold for <em>all</em> pairs of neighboring datasets). But attribution-based control requires the opposite: not uniform bounds across all sources, but <em>source-specific</em> control where each source can have different influence levels matching different stakeholder needs.</p>

                <h3>From Worst-Case to Individual Differential Privacy</h3>
                <p>Standard differential privacy enforces worst-case bounds across all possible pairs of neighboring datasets. Consider a dataset where most individuals' data appears in common patterns, but one individual has highly unique data. Worst-case differential privacy must constrain the entire mechanism based on that one outlier, reducing utility for everyone‚Äîeven though the mechanism only ever operates on the <em>actual</em> dataset, not all possible datasets. Individual differential privacy provides a more nuanced approach by focusing privacy guarantees on the <em>actual</em> dataset rather than all possible datasets:</p>

                <div class="definition-box">
                    <h3>Individual Differential Privacy</h3>
                    <p>Given a dataset $D$, a response mechanism $\mathcal{M}(\cdot)$ satisfies Œµ-individual differential privacy (Œµ-iDP) if, for any dataset $D'$ that is a neighbor of $D$ (differing in one example), and any $S \subset \text{Range}(\mathcal{M})$:</p>
                    <p style="text-align: center;">$\exp(-\epsilon) \cdot \Pr[\mathcal{M}(D') \in S] \leq \Pr[\mathcal{M}(D) \in S] \leq \exp(\epsilon) \cdot \Pr[\mathcal{M}(D') \in S]$</p>
                </div>

                <p>The crucial difference from standard DP: $D$ refers to the <em>actual</em> dataset being protected, while $D'$ ranges over $D$'s neighbors. Standard DP requires indistinguishability for <em>any</em> pair of neighbors; individual DP requires indistinguishability only between the actual dataset and its neighbors.</p>

                <h3>From Individual Privacy to Individual Attribution</h3>
                <p>Just as we extended standard differential privacy to define attribution regimes in the previous section, we can extend individual differential privacy to define individual attribution. The key insight remains the same: privacy and attribution are opposite ends of the same Œµ spectrum, now applied to the actual dataset rather than worst-case bounds.</p>

                <div class="definition-box">
                    <h3>Individual Differential Privacy vs. Attribution</h3>
                    <p>For a mechanism $\mathcal{M}$, actual dataset $D$, and threshold $\tau > 0$:</p>
                    <ul>
                        <li><strong>Individual Privacy:</strong> Enforce $\epsilon < \tau$ for all neighbors $D'$ of $D$, guaranteeing that any individual's data in the dataset cannot be distinguished through its influence on outputs.</li>
                        <li><strong>Individual Measurement:</strong> Compute $\epsilon$ for the actual dataset, enabling quantification of influence without enforcing bounds.</li>
                        <li><strong>Individual Attribution:</strong> Enforce $\epsilon > \tau$ for the actual dataset, guaranteeing that individuals in the actual dataset have measurable influence on outputs.</li>
                    </ul>
                </div>

                <h3>From Examples to Sources: Group Differential Privacy</h3>
                <p>But attribution-based control requires more than individual-level bounds, it requires <em>source-level</em> control. Individual differential privacy protects single examples in the actual dataset, but data sources typically contribute many examples. Consider a medical AI trained on data from 100 hospitals: each hospital contributes thousands of patient records. ABC needs to measure and control influence at the hospital level, not just the individual patient level. The differential privacy literature addresses this through group differential privacy, which extends privacy guarantees from individuals to groups of records:</p>

                <div class="definition-box">
                    <h3>Group Differential Privacy</h3>
                    <p>A randomized mechanism $\mathcal{M}: \mathcal{D} \to \mathcal{R}$ satisfies Œµ-group differential privacy for groups of size $k$ if for all datasets $D, D'$ differing in at most $k$ records and all subsets of outputs $S \subseteq \mathcal{R}$:</p>
                    <p style="text-align: center;">$\Pr[\mathcal{M}(D) \in S] \leq e^\epsilon \cdot \Pr[\mathcal{M}(D') \in S]$</p>
                </div>

                <p>We can combine group differential privacy with individual differential privacy to obtain source-level control calibrated to the actual dataset. When we partition a dataset by sources $D = \bigcup_{s \in S} D_s$, we treat each source as a group and apply individual DP at the group level:</p>

                <div class="definition-box">
                    <h3>Source-Level Individual Differential Privacy</h3>
                    <p>Given a dataset $D$ partitioned by sources $D = \bigcup_{s \in S} D_s$, a response mechanism $\mathcal{M}(\cdot)$ satisfies $\epsilon_s$-individual differential privacy for source $s$ if, for any dataset $D'$ differing from $D$ only in source $s$'s data (i.e., $D' = D_{-s} \cup D'_s$ where $|D'_s| = |D_s|$), and any $S \subset \text{Range}(\mathcal{M})$:</p>
                    <p style="text-align: center;">$\exp(-\epsilon_s) \cdot \Pr[\mathcal{M}(D') \in S] \leq \Pr[\mathcal{M}(D) \in S] \leq \exp(\epsilon_s) \cdot \Pr[\mathcal{M}(D') \in S]$</p>
                </div>

                <h3>From Source-Level Privacy to Source-Level Attribution</h3>
                <p>Just as individual DP extends to attribution regimes (privacy, measurement, attribution), source-level individual DP extends to source-level attribution. We can quantitatively measure each source's influence on the actual dataset:</p>

                <div class="definition-box">
                    <h3>Source-Level Individual Differential Attribution</h3>
                    <p>Let $\mathcal{A}$ be a randomized algorithm and let $D$ be a dataset partitioned by sources $D = \bigcup_{s \in S} D_s$. For any source $s$ and prediction $f$, the individual differential attribution of source $s$ on function $f$ is:</p>
                    <p style="text-align: center;">$\text{Attribution}_\alpha(s, f) = D_\alpha^\leftrightarrow(\mathcal{A}(D)\|\mathcal{A}(D^{-s})) = \max\{D_\alpha(\mathcal{A}(D)\|\mathcal{A}(D^{-s})), D_\alpha(\mathcal{A}(D^{-s})\|\mathcal{A}(D))\}$</p>
                    <p>where $D_\alpha$ is the R√©nyi divergence of order $\alpha$, $D^{-s} = D \setminus D_s$ represents the dataset with source $s$ removed, and $\mathcal{A}(D)$ represents the output distribution of algorithm $\mathcal{A}$ on dataset $D$.</p>
                </div>

                <p>This measures each source's attribution relative to the actual dataset $D$ by quantifying how predictions change when that specific source is included versus excluded. This enables the diverse source-level control ABC requires.</p>

                <h3>Intelligence Budgets: Implementing Individual Source Control</h3>
                <p>This source-level individual attribution measure enables a practical control mechanism: <em>intelligence budgets</em>. Rather than simply measuring influence after the fact, we can actively control how much each source influences predictions through architectural routing decisions.</p>

                <div class="definition-box">
                    <h3>Intelligence Budgets via Forward Pass Weighting</h3>
                    <p>The model has two types of parameterized functions:</p>
                    <p style="text-align: center;">$g_s(\cdot; \Theta_{\text{semantic}}[s])$: source-specific function for source $s$</p>
                    <p style="text-align: center;">$f(\cdot; \Theta_{\text{syntactic}})$: shared function across all sources</p>
                    <p>Information flows through two stages:</p>
                    <p style="text-align: center;">Stage 1 (Semantic): $s_s = g_s(x_s; \Theta_{\text{semantic}}[s])$</p>
                    <p style="text-align: center;">Stage 2 (Syntactic): $h(x) = f\left([x_1, \ldots, x_{|S|}, \gamma[1] \cdot s_1, \ldots, \gamma[|S|] \cdot s_{|S|}]; \Theta_{\text{syntactic}}\right)$</p>
                    <p>where $\gamma[s] \in [0,1]$ is a per-source scaling weight and $[\cdot]$ denotes concatenation of all inputs.</p>
                    <p>The intelligence budget $B(s)$ bounds source $s$'s influence:</p>
                    <p style="text-align: center;">$\text{Attribution}_\alpha(s, h) \leq B(s)$</p>
                    <p>Setting $\gamma[s] \approx 0$ enforces small $B(s)$ (privacy regime) by preventing semantic contributions. Setting $\gamma[s] \approx 1$ allows large $B(s)$ (attribution regime) by preserving semantic identity.</p>
                </div>

                <h3>From Concatenation and IDP to Deep Voting</h3>
                <p>The deep voting framework reveals a two-dimensional spectrum in machine learning architectures by introducing a second control parameter $\lambda \in [0,1]$ that governs the overall balance between semantic and syntactic capacity:</p>

                <div class="definition-box">
                    <h3>Capacity Allocation (Œª)</h3>
                    <p>The parameter Œª determines what fraction of total model capacity is allocated to each function:</p>
                    <p style="text-align: center;">$|\Theta_{\text{syntactic}}| = \lambda \cdot |\Theta_{\text{total}}|$</p>
                    <p style="text-align: center;">$\sum_{s} |\Theta_{\text{semantic}}[s]| = (1-\lambda) \cdot |\Theta_{\text{total}}|$</p>
                    <p>where $|\Theta|$ denotes parameter count.</p>
                </div>

                <div class="callout">
                    <p class="callout-title">Deep Voting Analogy: Individual Differential Privacy via Adaptive Filtering</p>
                    <p>Feldman and Zrnic's individual differential privacy framework provides a concrete example of intelligence budgets implemented through adaptive filtering rather than architectural routing.</p>
                    <p><strong>Architecture:</strong> Their approach uses $\lambda = 1$ (pure syntactic processing) with all capacity in shared parameters $\Theta_{\text{syntactic}}$. No semantic section exists ($\Theta_{\text{semantic}}[s] = \emptyset$), meaning all sources contribute only through raw inputs: $h(x) = f([x_1, \ldots, x_n]; \Theta_{\text{syntactic}})$ where $f$ adds Gaussian noise for privacy.</p>
                    <p><strong>Intelligence Budgets via Filtering:</strong> Rather than controlling $\gamma[s]$ continuously, they implement binary filtering. At each time step $t$, compute the individual privacy loss $\rho_t^{(i)} = \frac{\alpha \|\bar{g}_t(X_i)\|_2^2}{2\sigma^2}$ where $\bar{g}_t(X_i)$ is the clipped gradient. Source $i$ remains active while $\sum_{j=1}^t \rho_t^{(i)} \leq B$, then gets dropped (equivalent to setting $\gamma[i] = 0$ for all future steps).</p>
                    <p><strong>Key Insight:</strong> The intelligence budget $B(i)$ is implicitly determined by realized gradient norms. For Lipschitz functions with coordinate sensitivity $L_i$, the bound becomes $B(i) \approx \frac{\alpha L_i^2 \|\phi(X_i)\|^2}{2\sigma^2}$. Sources with small gradients (low sensitivity) can participate longer before exceeding their budget.</p>
                    <p><strong>Contrast with Deep Voting:</strong> Feldman & Zrnic achieve privacy (small Œµ) by dropping high-influence examples entirely, routing all remaining examples through privacy-constrained shared processing. Deep voting generalizes this by: (1) introducing a semantic section ($\lambda < 1$) that preserves source identity, (2) allowing continuous control ($\gamma[s] \in [0,1]$) rather than binary drop/keep decisions, and (3) enabling attribution regime where large Œµ is desirable. Individual DP represents the special case where $\lambda = 1$, $\gamma[s] \in \{0,1\}$ (binary), and all sources demand privacy.</p>
                    <p><strong>Adaptive Composition:</strong> The Feldman & Zrnic result on adaptive composition with data-dependent privacy parameters ($\sum_t \rho_t^{(i)} \leq B \Rightarrow (\alpha, B)$-RDP) directly parallels our intelligence budget composition: both handle the challenge that influence parameters depend on previous outputs, enabling provable bounds even under adaptive computation.</p>
                </div>

                <p>With these mechanisms in place, we can return to the implications of such a system for providing attribution-based control: the potential to dramatically increase the amount of data and compute available for training AI systems. By providing clear mechanisms for measuring source influence ($\text{Attribution}_\alpha(s, h)$), bounding influence when needed ($\gamma[s] = 0$ for privacy), and guaranteeing influence when required ($\gamma[s] = 1$ for attribution), deep voting might enable the safe use of orders of magnitude more training data.</p>

                <div class="callout">
                    <p class="callout-title">A Library Analogy</p>
                    <p>Consider a library wherein all of the books have had their covers removed, their table of contents erased, and individual sentences on each page torn out into their own strips. Now imagine that each word in each strip is converted into a number "aardvark = 1", "abernathe = 2", and so forth. And then imagine that each of these strips from the whole library is shuffled around, and groups of strips are placed back in the coverless books. Yet, instead of each strip being glued in place, it is first combined with many other strips, adding their respective numbers together. Consequently, when someone wants to answer a specific question, they have to read through the entire library searching for relevant information for their query‚Äîfirst by converting their query into numbers and then attempting to match it to numbers found in the books.</p>
                    <p>Following the analogy, differential attribution ensures that each strip of numbers remains separated (instead of added) into each other strip, preserved within the same book as before, partitioning information in a way which might be indexed by source (or topic). It further provides a staff of librarians who know how to read relevant information and synthesize them, each according to a topic that librarian happens to be familiar with. Taken together, a customer of a library can leverage one librarian to index into the appropriate shelf, identify the right book, and the right snippets of that book, and then ask a subset of the staff of librarians who are experts on that topic to properly interpret those snippets.</p>
                </div>
            </section>

            <section id="first-hypothesis">
                <h2>First Hypothesis: ABC and 6+ Orders of Magnitude more Data/Compute</h2>

                <figure class="full-width">
                    <img src="1411.3146/abc_ch2_8_0_v5.png" alt="First Hypothesis graph">
                    <figcaption>Subset of concept graph highlighting the First Hypothesis and focus of this section.</figcaption>
                </figure>

                <p>The deep voting framework reveals a two-dimensional spectrum in machine learning architectures. Consider how different parameter settings affect the model's behavior:</p>

                <p>At (&lambda;,&gamma;) = (1,0), we find pure deep learning with maximum compression. These systems, like GPT-4, use only shared parameters with basic bounds on attribution. At (&lambda;,&gamma;) = (0,1), we find pure partition-based learning, like federated systems, which maintain group privacy but limit cross-source learning. At (&lambda;,&gamma;) = (0,0), we find pure source-specific learning systems like k-nearest neighbors.</p>

                <p>The stakes are significant. If deep voting succeeds, it could unlock another 6+ orders of magnitude of training data and compute productivity. If it fails, we may remain constrained by the fundamental limitations of current architectures.</p>
            </section>

            <section id="empirical-evidence">
                <h2>Empirical Evidence: Does the Pareto-Tradeoff Move?</h2>

                <figure class="full-width">
                    <img src="1411.3146/abc_ch2_9_0_v4.png" alt="Empirical evidence concept graph">
                    <figcaption>Subset of concept graph highlighting the First Hypothesis and focus of this section.</figcaption>
                </figure>

                <h3>The First Crack: RETRO and ATLAS</h3>
                <p>Recent architectures challenge baseline tradeoffs. RETRO outperforms GPT-3 on the Pile (0.670 vs 0.811 bits-per-byte) while using only 7.5B parameters compared to GPT-3's 175B:</p>

                <div class="table-container">
                    <table class="data-table">
                        <caption>Table: RETRO results on the Pile, measured in bits-per-byte. (From Borgeaud et al., 2022)</caption>
                        <thead>
                            <tr>
                                <th>Subset</th>
                                <th>7B Baseline</th>
                                <th>GPT-3</th>
                                <th>Jurassic-1</th>
                                <th>Gopher</th>
                                <th>7.5B RETRO</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td>arxiv</td><td>0.742</td><td>0.838</td><td>0.680</td><td><strong>0.641</strong></td><td>0.714</td></tr>
                            <tr><td>books3</td><td>0.792</td><td>0.802</td><td>0.835</td><td>0.706</td><td><strong>0.653</strong></td></tr>
                            <tr><td>dm_mathematics</td><td>1.177</td><td>1.371</td><td><strong>1.037</strong></td><td>1.135</td><td>1.164</td></tr>
                            <tr><td>freelaw</td><td>0.576</td><td>0.612</td><td>0.514</td><td>0.506</td><td><strong>0.499</strong></td></tr>
                            <tr><td>github</td><td>0.420</td><td>0.645</td><td>0.358</td><td>0.367</td><td><strong>0.199</strong></td></tr>
                            <tr><td>gutenberg_pg_19</td><td>0.803</td><td>1.163</td><td>0.890</td><td>0.652</td><td><strong>0.400</strong></td></tr>
                            <tr><td>hackernews</td><td>0.971</td><td>0.975</td><td>0.869</td><td>0.888</td><td><strong>0.860</strong></td></tr>
                            <tr><td>nih_exporter</td><td>0.650</td><td>0.612</td><td><strong>0.590</strong></td><td>0.590</td><td>0.635</td></tr>
                            <tr><td>opensubtitles</td><td>0.974</td><td>0.932</td><td><strong>0.879</strong></td><td>0.894</td><td>0.930</td></tr>
                            <tr><td>philpapers</td><td>0.760</td><td>0.723</td><td>0.742</td><td><strong>0.682</strong></td><td>0.699</td></tr>
                            <tr><td>pile_cc</td><td>0.771</td><td>0.698</td><td>0.669</td><td>0.688</td><td><strong>0.626</strong></td></tr>
                            <tr><td>pubmed_abstracts</td><td>0.639</td><td>0.625</td><td>0.587</td><td>0.578</td><td><strong>0.542</strong></td></tr>
                            <tr><td>pubmed_central</td><td>0.588</td><td>0.690</td><td>0.579</td><td>0.512</td><td><strong>0.419</strong></td></tr>
                            <tr><td>stackexchange</td><td>0.714</td><td>0.773</td><td>0.655</td><td>0.638</td><td><strong>0.624</strong></td></tr>
                            <tr><td>ubuntu_irc</td><td>1.200</td><td>0.946</td><td><strong>0.857</strong></td><td>1.081</td><td>1.178</td></tr>
                            <tr><td>uspto_backgrounds</td><td>0.603</td><td>0.566</td><td><strong>0.537</strong></td><td>0.545</td><td>0.583</td></tr>
                            <tr class="highlight"><td><strong>Average</strong></td><td>0.774</td><td>0.811</td><td>0.705</td><td>0.694</td><td><strong>0.670</strong></td></tr>
                        </tbody>
                    </table>
                </div>

                <p>This constitutes a 25x reduction in parameter count while achieving superior performance and maintaining clear attribution paths through its retrieval mechanism.</p>

                <p>ATLAS demonstrates similar gains: 25-50x parameter efficiency improvements while maintaining or exceeding baseline performance. Both systems achieve these results through a fundamental architectural shift: rather than compressing all knowledge into dense parameters, they maintain explicit connections to source documents through retrieval.</p>

                <p>Federated RAG systems demonstrate concurrent improvements in attribution and performance:</p>

                <div class="table-container">
                    <table class="data-table">
                        <caption>Table: Performance Metrics on MMLU Tasks (Federated RAG vs Baseline RAG)</caption>
                        <thead>
                            <tr>
                                <th>Task</th>
                                <th>Federated RAG Accuracy (%)</th>
                                <th>Baseline RAG Accuracy (%)</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td>Task 1</td><td>78</td><td>70</td></tr>
                            <tr><td>Task 2</td><td>82</td><td>75</td></tr>
                            <tr><td>Task 3</td><td>74</td><td>68</td></tr>
                            <tr><td>Task 4</td><td>88</td><td>80</td></tr>
                            <tr><td>Task 5</td><td>81</td><td>76</td></tr>
                        </tbody>
                    </table>
                </div>

                <h3>Deep Voting: Formalizing the Pattern</h3>
                <p>These architectures (RETRO, ATLAS, PATE, federated RAG, and Git Re-Basin) share a common technical mechanism: they replace addition operations during training with concatenation, deferring synthesis until inference time. We formalize this pattern as <em>deep voting</em>.</p>

                <figure class="full-width">
                    <img src="1411.3146/deep_learning_to_deep_voting_v3.png" alt="Deep Learning to Deep Voting">
                    <figcaption>Traditional open/closed-source deep learning systems (left) pool all data into a deep learning model (i.e. by adding weight updates) which is later used for predictions, while deep voting systems (right) learn weight parameters which remain partitioned by source (i.e. concatenated), but which are learned in a way that they can be rapidly synthesized on the fly.</figcaption>
                </figure>

                <p>Deep voting addresses the addition problem that blocks access to 6+ orders of magnitude of data and compute. By preserving source attribution through concatenated representations while enabling cross-source learning through shared components, deep voting architectures demonstrate that the baseline tradeoffs between attribution, efficiency, and performance reflect architectural choices rather than fundamental constraints.</p>

                <p>However, solving the addition problem reveals a deeper challenge: the copy problem. Even if one achieved perfect attribution through deep voting, data sources cannot enforce how their contributions are used because whoever possesses a copy of the model retains unilateral control. Chapter 2 addresses this challenge, introducing techniques that enable attribution-based <em>control</em> rather than mere attribution-based <em>suggestions</em>.</p>
            </section>

            <section class="references">
                <h2>References</h2>
                <ol>
                    <li><span class="authors">Kaplan, J., et al.</span> (2020). <a href="https://arxiv.org/abs/2001.08361"><span class="title">Scaling Laws for Neural Language Models.</span></a> <span class="venue">arXiv:2001.08361</span>.</li>
                    <li><span class="authors">Hoffmann, J., et al.</span> (2022). <a href="https://arxiv.org/abs/2203.15556"><span class="title">Training Compute-Optimal Large Language Models.</span></a> <span class="venue">arXiv:2203.15556</span>.</li>
                    <li><span class="authors">Robison, K.</span> (2024). <a href="https://www.theverge.com/2024/12/13/24320811/openai-ilya-sutskever-agi-data-training"><span class="title">OpenAI cofounder Ilya Sutskever says the way AI is built is about to change.</span></a> <span class="venue">The Verge</span>.</li>
                    <li><span class="authors">Borgeaud, S., et al.</span> (2022). <a href="https://arxiv.org/abs/2112.04426"><span class="title">Improving language models by retrieving from trillions of tokens.</span></a> <span class="venue">ICML 2022</span>, 2206‚Äì2240.</li>
                    <li><span class="authors">Izacard, G., et al.</span> (2023). <a href="https://arxiv.org/abs/2208.03299"><span class="title">Atlas: Few-shot Learning with Retrieval Augmented Language Models.</span></a> <span class="venue">Journal of Machine Learning Research</span>, 24, 1‚Äì43.</li>
                    <li><span class="authors">Guo, Z., et al.</span> (2023). <a href="https://arxiv.org/abs/2310.05773"><span class="title">Towards lossless dataset distillation via difficulty-aligned trajectory matching.</span></a> <span class="venue">arXiv:2310.05773</span>.</li>
                    <li><span class="authors">Epoch AI.</span> (2024). <a href="https://epoch.ai/data/notable-ai-models"><span class="title">Data on Notable AI Models.</span></a> <span class="venue">epoch.ai/data/notable-ai-models</span>.</li>
                    <li><span class="authors">Goodfellow, I., Bengio, Y., & Courville, A.</span> (2016). <a href="https://www.deeplearningbook.org/"><span class="title">Deep Learning.</span></a> <span class="venue">MIT Press</span>.</li>
                    <li><span class="authors">Kemker, R., et al.</span> (2018). <a href="https://arxiv.org/abs/1708.02072"><span class="title">Measuring Catastrophic Forgetting in Neural Networks.</span></a> <span class="venue">AAAI 2018</span>.</li>
                    <li><span class="authors">Cummins, M.</span> (2024). <a href="https://educatingsilicon.substack.com/p/how-much-llm-training-data-is-there"><span class="title">How much LLM training data is there, in the limit?</span></a> <span class="venue">Educating Silicon</span>.</li>
                    <li><span class="authors">Le, Q. V., et al.</span> (2013). <a href="https://arxiv.org/abs/1112.6209"><span class="title">Building high-level features using large scale unsupervised learning.</span></a> <span class="venue">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>.</li>
                    <li><span class="authors">Krizhevsky, A., Sutskever, I., & Hinton, G. E.</span> (2012). <a href="https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html"><span class="title">ImageNet Classification with Deep Convolutional Neural Networks.</span></a> <span class="venue">NeurIPS 2012</span>.</li>
                    <li><span class="authors">Zeiler, M. D., & Fergus, R.</span> (2014). <a href="https://arxiv.org/abs/1311.2901"><span class="title">Visualizing and Understanding Convolutional Networks.</span></a> <span class="venue">ECCV 2014</span>.</li>
                    <li><span class="authors">Chomsky, N.</span> (2014). <a href="https://mitpress.mit.edu/9780262527408/aspects-of-the-theory-of-syntax/"><span class="title">Aspects of the Theory of Syntax.</span></a> <span class="venue">MIT Press</span>.</li>
                    <li><span class="authors">Dwork, C., et al.</span> (2006). <a href="https://doi.org/10.1007/11681878_14"><span class="title">Calibrating noise to sensitivity in private data analysis.</span></a> <span class="venue">TCC 2006</span>.</li>
                    <li><span class="authors">Abadi, M., et al.</span> (2016). <a href="https://arxiv.org/abs/1607.00133"><span class="title">Deep Learning with Differential Privacy.</span></a> <span class="venue">CCS 2016</span>.</li>
                    <li><span class="authors">Feldman, V., & Zrnic, T.</span> (2020). <a href="https://arxiv.org/abs/2008.11193"><span class="title">Individual Privacy Accounting via a Renyi Filter.</span></a> <span class="venue">arXiv:2008.11193</span>.</li>
                    <li><span class="authors">Papernot, N., et al.</span> (2018). <a href="https://arxiv.org/abs/1802.08908"><span class="title">Scalable Private Learning with PATE.</span></a> <span class="venue">ICLR 2018</span>.</li>
                    <li><span class="authors">Zhao, Y., et al.</span> (2018). <a href="https://arxiv.org/abs/1806.00582"><span class="title">Federated Learning with Non-IID Data.</span></a> <span class="venue">arXiv:1806.00582</span>.</li>
                    <li><span class="authors">Ainsworth, S. K., et al.</span> (2022). <a href="https://arxiv.org/abs/2209.04836"><span class="title">Git Re-Basin: Merging Models modulo Permutation Symmetries.</span></a> <span class="venue">arXiv:2209.04836</span>.</li>
                    <li><span class="authors">Nguyen, T. T., et al.</span> (2024). <a href="https://arxiv.org/abs/2209.02299"><span class="title">A Survey of Machine Unlearning.</span></a> <span class="venue">arXiv:2209.02299</span>.</li>
                    <li><span class="authors">Hochreiter, S., & Schmidhuber, J.</span> (1997). <a href="https://doi.org/10.1162/neco.1997.9.8.1735"><span class="title">Long Short-Term Memory.</span></a> <span class="venue">Neural Computation</span>, 9(8), 1735‚Äì1780.</li>
                </ol>
            </section>

            <nav class="chapter-nav">
                <a href="index.html" class="prev">Chapter I: Introduction</a>
                <a href="chapter3.html" class="next">Chapter III: Network-Source AI</a>
            </nav>
        </main>
    </div>

    <footer>
        <p>Andrew Trask &middot; University of Oxford</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const tocLinks = document.querySelectorAll('.toc-sidebar a');
            const sections = [];
            tocLinks.forEach(link => {
                const id = link.getAttribute('href').substring(1);
                const section = document.getElementById(id);
                if (section) sections.push({ id, element: section, link });
            });
            function updateActiveLink() {
                const scrollPos = window.scrollY + 100;
                let currentSection = sections[0];
                for (const section of sections) {
                    if (section.element.offsetTop <= scrollPos) currentSection = section;
                }
                tocLinks.forEach(link => link.classList.remove('active'));
                if (currentSection) currentSection.link.classList.add('active');
            }
            window.addEventListener('scroll', updateActiveLink);
            updateActiveLink();
        });
    </script>
</body>
</html>
