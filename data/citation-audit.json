{
  "meta": {
    "version": "1.0",
    "generated": "2026-02-27",
    "total_citations": 264,
    "total_unique_refs": 173
  },
  "citations": [
    {
      "id": "index:ref-1:0",
      "chapter": "index",
      "ref_num": 1,
      "bibtex_key": "dziri_origin_2022",
      "cite_label": "Dziri et al., 2022",
      "claim_context": "The lack of ABC presents AI users with a basic authenticity problem, discussed publicly using phrases like hallucination, deepfake, and disinformation ([Dziri et al., 2022]; [Vaccari and Chadwick 2020]). When an AI generates a response, users often cannot verify its sources; unlike a Google search where one can examine original documents ([Zuccon et al., 2023]; [Schmidt and Rosenberg 2014]).",
      "section_id": "individual-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title directly addresses hallucinations in conversational models, matching the claim about hallucination as an authenticity problem, but the abstract is only bibliographic metadata with no content to confirm specific claims.",
        "checked_at": "2026-02-27T22:56:14.160204+00:00"
      }
    },
    {
      "id": "index:ref-2:0",
      "chapter": "index",
      "ref_num": 2,
      "bibtex_key": "vaccari_deepfakes_2020",
      "cite_label": "Vaccari and Chadwick 2020",
      "claim_context": "The lack of ABC presents AI users with a basic authenticity problem, discussed publicly using phrases like hallucination, deepfake, and disinformation ([Dziri et al., 2022]; [Vaccari and Chadwick 2020]). When an AI generates a response, users often cannot verify its sources; unlike a Google search where one can examine original documents ([Zuccon et al., 2023]; [Schmidt and Rosenberg 2014]).",
      "section_id": "individual-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Deepfakes and Disinformation' directly matches the claim about deepfakes and disinformation as authenticity problems, but source inaccessible (HTTP 403) and no abstract available.",
        "checked_at": "2026-02-27T22:56:14.160405+00:00"
      }
    },
    {
      "id": "index:ref-3:0",
      "chapter": "index",
      "ref_num": 3,
      "bibtex_key": "zuccon_hallucination_attribution",
      "cite_label": "Zuccon et al., 2023",
      "claim_context": "The lack of ABC presents AI users with a basic authenticity problem, discussed publicly using phrases like hallucination, deepfake, and disinformation ([Dziri et al., 2022]; [Vaccari and Chadwick 2020]). When an AI generates a response, users often cannot verify its sources; unlike a Google search where one can examine original documents ([Zuccon et al., 2023]; [Schmidt and Rosenberg 2014]). Consider a medical student using AI to research rare diseases.",
      "section_id": "individual-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'ChatGPT Hallucinates when Attributing Answers' directly supports the claim about users being unable to verify AI sources, but source inaccessible (HTTP 403) and no abstract available.",
        "checked_at": "2026-02-27T22:56:14.160408+00:00"
      }
    },
    {
      "id": "index:ref-21:0",
      "chapter": "index",
      "ref_num": 21,
      "bibtex_key": "ai_bioweapon",
      "cite_label": "Schmidt\nand Rosenberg 2014",
      "claim_context": "The lack of ABC presents AI users with a basic authenticity problem, discussed publicly using phrases like hallucination, deepfake, and disinformation ([Dziri et al., 2022]; [Vaccari and Chadwick 2020]). When an AI generates a response, users often cannot verify its sources; unlike a Google search where one can examine original documents ([Zuccon et al., 2023]; [Schmidt and Rosenberg 2014]). Consider a medical student using AI to research rare diseases. They cannot tell whether an AI’s detailed ",
      "section_id": "individual-consequences",
      "verification": {
        "status": "mismatch",
        "reasoning": "The citation label says 'Schmidt and Rosenberg 2014' referencing Google search verification, but the actual reference is Mouton et al. 2023 about AI risks in biological attacks, which is unrelated to examining original documents in search results.",
        "checked_at": "2026-02-27T22:56:14.160410+00:00"
      }
    },
    {
      "id": "index:ref-4:0",
      "chapter": "index",
      "ref_num": 4,
      "bibtex_key": "GRAVEL2023226",
      "cite_label": "Gravel et al., 2023",
      "claim_context": "Consider a medical student using AI to research rare diseases. They cannot tell whether an AI’s detailed symptom description comes from high-quality sources (e.g., peer-reviewed journals) or less qualified sources ([Gravel et al., 2023]; [Bhattacharyya et al., 2023]). Meanwhile, the researchers who authored medical papers feeding the AI lose control over how their work is used, dis-incentivizing them to share information ([Devriendt et al., 2021]).This mutual blindness creates the opportunity for hallucination and disinformation",
      "section_id": "individual-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Learning to Fake It: Limited Responses and Fabricated References Provided by ChatGPT for Medical Questions' directly relates to the claim about medical AI source quality, but no abstract available to confirm specifics.",
        "checked_at": "2026-02-27T22:56:14.160411+00:00"
      }
    },
    {
      "id": "index:ref-22:0",
      "chapter": "index",
      "ref_num": 22,
      "bibtex_key": "bhattacharyya2023high",
      "cite_label": "Bhattacharyya et al., 2023",
      "claim_context": "Consider a medical student using AI to research rare diseases. They cannot tell whether an AI’s detailed symptom description comes from high-quality sources (e.g., peer-reviewed journals) or less qualified sources ([Gravel et al., 2023]; [Bhattacharyya et al., 2023]). Meanwhile, the researchers who authored medical papers feeding the AI lose control over how their work is used, dis-incentivizing them to share information ([Devriendt et al., 2021]).This mutual blindness creates the opportunity for hallucination and disinformation",
      "section_id": "individual-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms 47% of ChatGPT-generated medical references were fabricated and 46% inaccurate, directly supporting the claim that users cannot tell whether AI medical content comes from high-quality sources.",
        "checked_at": "2026-02-27T22:56:14.160412+00:00"
      }
    },
    {
      "id": "index:ref-23:0",
      "chapter": "index",
      "ref_num": 23,
      "bibtex_key": "health_sharing_incentives",
      "cite_label": "Devriendt et al., 2021",
      "claim_context": "They cannot tell whether an AI’s detailed symptom description comes from high-quality sources (e.g., peer-reviewed journals) or less qualified sources ([Gravel et al., 2023]; [Bhattacharyya et al., 2023]). Meanwhile, the researchers who authored medical papers feeding the AI lose control over how their work is used, dis-incentivizing them to share information ([Devriendt et al., 2021]).This mutual blindness creates the opportunity for hallucination and disinformation",
      "section_id": "individual-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Data Sharing in Biomedical Sciences: A Systematic Review of Incentives' is topically relevant to the claim about researchers losing control over their work dis-incentivizing sharing, but source inaccessible (HTTP 403) and no abstract available.",
        "checked_at": "2026-02-27T22:56:14.160414+00:00"
      }
    },
    {
      "id": "index:ref-6:0",
      "chapter": "index",
      "ref_num": 6,
      "bibtex_key": "yu-etal-2024-mechanistic",
      "cite_label": "Yu et al., 2024",
      "claim_context": "Hallucination: To see ABC’s contribution to the problem of hallucination, consider the mechanics of an AI hallucination. In some cases, an AI model makes a prediction regarding a subject either not properly covered in its training data or prompt, or not properly recalled during inference ([Yu et al., 2024]). But since an AI model’s predictions are necessarily based on data, the AI will attempt to use insights from less related documents 1 to generate a plausible-sounding response: a hallucination.",
      "section_id": "individual-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations' directly matches the claim about AI hallucination mechanics, but abstract is only bibliographic metadata with no substantive content.",
        "checked_at": "2026-02-27T22:56:14.160415+00:00"
      }
    },
    {
      "id": "index:ref-24:0",
      "chapter": "index",
      "ref_num": 24,
      "bibtex_key": "RLHF",
      "cite_label": "Ouyang et al., 2022",
      "claim_context": "Proposed Remedies: Within the context of LLMs, recent work has attempted to address this through a myriad of methods such as, data cleaning, fine-tuning more data into AI models (e.g., RLHF, InstructGPT), direct oversight of deployed AI (e.g., HITL, auditing), measures of AI confidence (i.e., self checking), improving prompts (e.g., chain of thought, RAG), source attribution methods to reverse engineer the source-prediction relationship (i.e., influence functions), and others ([Ouyang et al., 2022]; [Roozbahani 2025]; [Ahmad et al., 2023]; [Manakul et al., 2023]).",
      "section_id": "individual-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Training language models to follow instructions with human feedback' directly matches the claim referencing RLHF/InstructGPT as a remedy for hallucination, but no abstract available to confirm specifics.",
        "checked_at": "2026-02-27T22:56:14.160416+00:00"
      }
    },
    {
      "id": "index:ref-25:0",
      "chapter": "index",
      "ref_num": 25,
      "bibtex_key": "roozbahani2025review",
      "cite_label": "Roozbahani 2025",
      "claim_context": "Proposed Remedies: Within the context of LLMs, recent work has attempted to address this through a myriad of methods such as, data cleaning, fine-tuning more data into AI models (e.g., RLHF, InstructGPT), direct oversight of deployed AI (e.g., HITL, auditing), measures of AI confidence (i.e., self checking), improving prompts (e.g., chain of thought, RAG), source attribution methods to reverse engineer the source-prediction relationship (i.e., influence functions), and others ([Ouyang et al., 2022]; [Roozbahani 2025]; [Ahmad et al., 2023]; [Manakul et al., 2023]).",
      "section_id": "individual-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses hallucination reduction methods including reinforcement learning approaches, directly supporting the claim about various methods to address hallucination in LLMs.",
        "checked_at": "2026-02-27T22:56:14.160417+00:00"
      }
    },
    {
      "id": "index:ref-26:0",
      "chapter": "index",
      "ref_num": 26,
      "bibtex_key": "ahmad2023creatingtrustworthyllmsdealing",
      "cite_label": "Ahmad et al., 2023",
      "claim_context": "Proposed Remedies: Within the context of LLMs, recent work has attempted to address this through a myriad of methods such as, data cleaning, fine-tuning more data into AI models (e.g., RLHF, InstructGPT), direct oversight of deployed AI (e.g., HITL, auditing), measures of AI confidence (i.e., self checking), improving prompts (e.g., chain of thought, RAG), source attribution methods to reverse engineer the source-prediction relationship (i.e., influence functions), and others ([Ouyang et al., 2022]; [Roozbahani 2025]; [Ahmad et al., 2023]; [Manakul et al., 2023]).",
      "section_id": "individual-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses quantification, validation, and mitigation of hallucinations in healthcare LLMs, directly supporting the claim about methods to create trustworthy LLMs and address hallucination.",
        "checked_at": "2026-02-27T22:56:14.160419+00:00"
      }
    },
    {
      "id": "index:ref-27:0",
      "chapter": "index",
      "ref_num": 27,
      "bibtex_key": "manakul2023selfcheckgptzeroresourceblackboxhallucination",
      "cite_label": "Manakul et al., 2023",
      "claim_context": "Proposed Remedies: Within the context of LLMs, recent work has attempted to address this through a myriad of methods such as, data cleaning, fine-tuning more data into AI models (e.g., RLHF, InstructGPT), direct oversight of deployed AI (e.g., HITL, auditing), measures of AI confidence (i.e., self checking), improving prompts (e.g., chain of thought, RAG), source attribution methods to reverse engineer the source-prediction relationship (i.e., influence functions), and others ([Ouyang et al., 2022]; [Roozbahani 2025]; [Ahmad et al., 2023]; [Manakul et al., 2023]).",
      "section_id": "individual-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes SelfCheckGPT, a zero-resource hallucination detection method for LLMs, directly supporting the claim about measures of AI confidence (self-checking) as a proposed remedy.",
        "checked_at": "2026-02-27T22:56:14.160420+00:00"
      }
    },
    {
      "id": "index:ref-5:0",
      "chapter": "index",
      "ref_num": 5,
      "bibtex_key": "xu2024hallucinationinevitableinnatelimitation",
      "cite_label": "Xu et al., 2024",
      "claim_context": "These efforts, however, face two fundamental limitations which prevent them from solving the issue. First, it has been shown that LLMs will always hallucinate because they cannot learn all computable functions between a computable LLM and a computable ground truth function ([Xu et al., 2024]) (a proof which focuses on LLMs but requires no specific tie to language or transformers). And second, since AI will always hallucinate to some extent, the question is whether or not detection is possible and addressable.",
      "section_id": "individual-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract proves that hallucination is inevitable in LLMs because they cannot learn all computable functions, directly supporting the claim that LLMs will always hallucinate.",
        "checked_at": "2026-02-27T22:56:14.160421+00:00"
      }
    },
    {
      "id": "index:ref-7:0",
      "chapter": "index",
      "ref_num": 7,
      "bibtex_key": "st",
      "cite_label": "Trask et al., 2020",
      "claim_context": "The lack of ABC cre`ates a basic incentive problem at the institutional level. Data-owning institutions can either share data to create AI, or they can decline and maintain control over their data, a dilemma discussed publicly using words like copyright, privacy, transparency, and misuse ([Trask et al., 2020]; [Golodny 2023]). Leading cancer centers, for example, struggle to train AI to detect breast cancer using more than 3-4 million mammograms (the largest and most diverse dataset known to this author being ([Jeong et al., 2022]).",
      "section_id": "institutional-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses the 'copy problem' where sharing information means losing control over its use, creating a dilemma that inhibits collaboration, directly supporting the claim about institutions facing a choice between sharing data and maintaining control.",
        "checked_at": "2026-02-27T22:56:14.160422+00:00"
      }
    },
    {
      "id": "index:ref-28:0",
      "chapter": "index",
      "ref_num": 28,
      "bibtex_key": "Steptoe",
      "cite_label": "Golodny 2023",
      "claim_context": "The lack of ABC cre`ates a basic incentive problem at the institutional level. Data-owning institutions can either share data to create AI, or they can decline and maintain control over their data, a dilemma discussed publicly using words like copyright, privacy, transparency, and misuse ([Trask et al., 2020]; [Golodny 2023]). Leading cancer centers, for example, struggle to train AI to detect breast cancer using more than 3-4 million mammograms (the largest and most diverse dataset known to this author being ([Jeong et al., 2022]).",
      "section_id": "institutional-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes a Senate AI forum focusing on intellectual property, transparency, and copyright issues in AI, directly supporting the claim about the data-sharing dilemma being discussed publicly using words like copyright and transparency.",
        "checked_at": "2026-02-27T22:56:14.160424+00:00"
      }
    },
    {
      "id": "index:ref-29:0",
      "chapter": "index",
      "ref_num": 29,
      "bibtex_key": "jeong2022emorybreastimagingdataset",
      "cite_label": "Jeong et al., 2022",
      "claim_context": "Data-owning institutions can either share data to create AI, or they can decline and maintain control over their data, a dilemma discussed publicly using words like copyright, privacy, transparency, and misuse ([Trask et al., 2020]; [Golodny 2023]). Leading cancer centers, for example, struggle to train AI to detect breast cancer using more than 3-4 million mammograms (the largest and most diverse dataset known to this author being ([Jeong et al., 2022]). Yet hundreds of millions of mammograms are produced annually worldwide ([Trieu et al., 2023]; [Onega et al., 2014]).",
      "section_id": "institutional-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes the EMBED dataset of 3.65 million mammograms, directly supporting the claim that the largest known breast imaging dataset contains approximately 3.5 million mammograms.",
        "checked_at": "2026-02-27T22:56:14.160425+00:00"
      }
    },
    {
      "id": "index:ref-30:0",
      "chapter": "index",
      "ref_num": 30,
      "bibtex_key": "global_cancer_screening",
      "cite_label": "Trieu et al., 2023",
      "claim_context": "Leading cancer centers, for example, struggle to train AI to detect breast cancer using more than 3-4 million mammograms (the largest and most diverse dataset known to this author being ([Jeong et al., 2022]). Yet hundreds of millions of mammograms are produced annually worldwide ([Trieu et al., 2023]; [Onega et al., 2014]).",
      "section_id": "institutional-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Abstract discusses mammography screening globally and mentions screening is not available in all countries with increasing cancer cases, but does not provide a specific figure for hundreds of millions of annual mammograms worldwide.",
        "checked_at": "2026-02-27T22:56:14.160427+00:00"
      }
    },
    {
      "id": "index:ref-31:0",
      "chapter": "index",
      "ref_num": 31,
      "bibtex_key": "us_cancer_screening",
      "cite_label": "Onega et al., 2014",
      "claim_context": "Leading cancer centers, for example, struggle to train AI to detect breast cancer using more than 3-4 million mammograms (the largest and most diverse dataset known to this author being ([Jeong et al., 2022]). Yet hundreds of millions of mammograms are produced annually worldwide ([Trieu et al., 2023]; [Onega et al., 2014]).",
      "section_id": "institutional-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Breast Cancer Screening in an Era of Personalized Regimens' relates to mammography screening at a population level, but source inaccessible (HTTP 403) and no abstract available to confirm annual mammogram volume.",
        "checked_at": "2026-02-27T22:56:14.160429+00:00"
      }
    },
    {
      "id": "index:ref-7:1",
      "chapter": "index",
      "ref_num": 7,
      "bibtex_key": "st",
      "cite_label": "Trask et al., 2020",
      "claim_context": "This gap exists because medical facilities face an impossible decision: either withhold their data to protect patient privacy and maintain control (i.e. protect privacy, IP, security, legal risk, etc.), or share it and lose all ability to control how it’s used ([Trask et al., 2020]). In 2023, multiple major medical centers explicitly cited this dilemma when declining to participate in AI research ([Youssef et al., 2023]).",
      "section_id": "institutional-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses how sharing information means losing control (the copy problem), creating a dilemma that inhibits collaboration, directly supporting the claim about medical facilities facing the impossible decision of withholding vs. sharing data.",
        "checked_at": "2026-02-27T22:56:14.160430+00:00"
      }
    },
    {
      "id": "index:ref-8:0",
      "chapter": "index",
      "ref_num": 8,
      "bibtex_key": "privacy_blocks_medical_sharing",
      "cite_label": "Youssef et al., 2023",
      "claim_context": "This gap exists because medical facilities face an impossible decision: either withhold their data to protect patient privacy and maintain control (i.e. protect privacy, IP, security, legal risk, etc.), or share it and lose all ability to control how it’s used ([Trask et al., 2020]). In 2023, multiple major medical centers explicitly cited this dilemma when declining to participate in AI research ([Youssef et al., 2023]). Yet their complaint is not unique to medical information, it exists for any data owner who has some incentive to care about when and how their data is used ([Gould 2015]).",
      "section_id": "institutional-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Organizational Factors in Clinical Data Sharing for Artificial Intelligence in Health Care' directly relates to the claim about medical centers declining AI research participation, but source inaccessible (HTTP 403) and no abstract available.",
        "checked_at": "2026-02-27T22:56:14.160431+00:00"
      }
    },
    {
      "id": "index:ref-32:0",
      "chapter": "index",
      "ref_num": 32,
      "bibtex_key": "data_sharing_doesnt_happen",
      "cite_label": "Gould 2015",
      "claim_context": "In 2023, multiple major medical centers explicitly cited this dilemma when declining to participate in AI research ([Youssef et al., 2023]). Yet their complaint is not unique to medical information, it exists for any data owner who has some incentive to care about when and how their data is used ([Gould 2015]).",
      "section_id": "institutional-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Data sharing: Why it doesn't happen' and page title confirm the topic directly supports the claim about data owners having incentives to care about how their data is used, but no abstract available to confirm specifics.",
        "checked_at": "2026-02-27T22:56:14.160432+00:00"
      }
    },
    {
      "id": "index:ref-33:0",
      "chapter": "index",
      "ref_num": 33,
      "bibtex_key": "grybauskas2023twitter",
      "cite_label": "Grybauskas 2023",
      "claim_context": "Fighting Against Data and Compute for AI—Copyright, Siloing, and Rate-limiting: The resistance against AI training is widespread and highly vocal. It can be found regarding websites as big as Reddit, the New York Times, and Twitter, and as small as individual bloggers who are fighting against onerous scrapers ([Grybauskas 2023]; [O’Brien 2025]; [Samuelson 2023]; [Grynbaum and Mac 2023]). Resistance can be found across media and entertainment, but also in the siloing of the world’s structured data ( [Patel 2019]).",
      "section_id": "institutional-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses Twitter's rate limits as a response to data scraping, directly supporting the claim about resistance against AI data scraping on major platforms like Twitter.",
        "checked_at": "2026-02-27T22:56:14.160433+00:00"
      }
    },
    {
      "id": "index:ref-34:0",
      "chapter": "index",
      "ref_num": 34,
      "bibtex_key": "obrien2025reddit",
      "cite_label": "O’Brien 2025",
      "claim_context": "Fighting Against Data and Compute for AI—Copyright, Siloing, and Rate-limiting: The resistance against AI training is widespread and highly vocal. It can be found regarding websites as big as Reddit, the New York Times, and Twitter, and as small as individual bloggers who are fighting against onerous scrapers ([Grybauskas 2023]; [O’Brien 2025]; [Samuelson 2023]; [Grynbaum and Mac 2023]). Resistance can be found across media and entertainment, but also in the siloing of the world’s structured data ( [Patel 2019]).",
      "section_id": "institutional-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms Reddit sued Perplexity AI for scraping user comments, directly supporting the claim about resistance against AI scraping on Reddit.",
        "checked_at": "2026-02-27T22:56:14.160435+00:00"
      }
    },
    {
      "id": "index:ref-35:0",
      "chapter": "index",
      "ref_num": 35,
      "bibtex_key": "samuelson2023generative",
      "cite_label": "Samuelson 2023",
      "claim_context": "Fighting Against Data and Compute for AI—Copyright, Siloing, and Rate-limiting: The resistance against AI training is widespread and highly vocal. It can be found regarding websites as big as Reddit, the New York Times, and Twitter, and as small as individual bloggers who are fighting against onerous scrapers ([Grybauskas 2023]; [O’Brien 2025]; [Samuelson 2023]; [Grynbaum and Mac 2023]). Resistance can be found across media and entertainment, but also in the siloing of the world’s structured data ( [Patel 2019]).",
      "section_id": "institutional-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Generative AI meets copyright' directly relates to the claim about copyright resistance against AI training, but source inaccessible (HTTP 403) and no abstract available.",
        "checked_at": "2026-02-27T22:56:14.160436+00:00"
      }
    },
    {
      "id": "index:ref-36:0",
      "chapter": "index",
      "ref_num": 36,
      "bibtex_key": "grynbaum2023times",
      "cite_label": "Grynbaum and Mac 2023",
      "claim_context": "Fighting Against Data and Compute for AI—Copyright, Siloing, and Rate-limiting: The resistance against AI training is widespread and highly vocal. It can be found regarding websites as big as Reddit, the New York Times, and Twitter, and as small as individual bloggers who are fighting against onerous scrapers ([Grybauskas 2023]; [O’Brien 2025]; [Samuelson 2023]; [Grynbaum and Mac 2023]). Resistance can be found across media and entertainment, but also in the siloing of the world’s structured data ( [Patel 2019]).",
      "section_id": "institutional-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'The Times Sues OpenAI and Microsoft Over A.I. Use of Copyrighted Work' directly supports the claim about New York Times fighting against AI data scraping, but source inaccessible (HTTP 403) and no abstract available.",
        "checked_at": "2026-02-27T22:56:14.160437+00:00"
      }
    },
    {
      "id": "index:ref-37:0",
      "chapter": "index",
      "ref_num": 37,
      "bibtex_key": "patel2019bridging",
      "cite_label": "Patel 2019",
      "claim_context": "It can be found regarding websites as big as Reddit, the New York Times, and Twitter, and as small as individual bloggers who are fighting against onerous scrapers ([Grybauskas 2023]; [O’Brien 2025]; [Samuelson 2023]; [Grynbaum and Mac 2023]). Resistance can be found across media and entertainment, but also in the siloing of the world’s structured data ( [Patel 2019]). This resistance has cultivated a profound contradiction (unpacked in Chapter 2): the AI community feels there is a lack of data and compute (enough for NVIDIA chips to be in short supply and Ilya Sutzskever to announce ”peak ...",
      "section_id": "institutional-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Bridging data silos using big data integration' directly relates to the claim about siloing of the world's structured data, but no abstract available to confirm specifics.",
        "checked_at": "2026-02-27T22:56:14.160438+00:00"
      }
    },
    {
      "id": "index:ref-38:0",
      "chapter": "index",
      "ref_num": 38,
      "bibtex_key": "robison2024openai",
      "cite_label": "Robison 2024",
      "claim_context": "Resistance can be found across media and entertainment, but also in the siloing of the world’s structured data ( [Patel 2019]). This resistance has cultivated a profound contradiction (unpacked in Chapter 2): the AI community feels there is a lack of data and compute (enough for NVIDIA chips to be in short supply and Ilya Sutzskever to announce ”peak data”) while around 1,000,000x more of each remains untapped ([Robison 2024]).",
      "section_id": "institutional-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract quotes 'We've achieved peak data and there'll be no more,' directly supporting the claim about Ilya Sutskever announcing 'peak data' while vast resources remain untapped.",
        "checked_at": "2026-02-27T22:56:14.160439+00:00"
      }
    },
    {
      "id": "index:ref-9:0",
      "chapter": "index",
      "ref_num": 9,
      "bibtex_key": "fl",
      "cite_label": "McMahan et al., 2016",
      "claim_context": "Recent work has suggested that privacy-enhancing technologies (PETs) can solve this collective action problem, suggesting technologies like federated learning, secure multi-party computation, and synthetic data ([McMahan et al., 2016]; [Rieke et al., 2020]; [Zhou et al., 2024]; [Jadon and Kumar 2023]). These efforts, however, face a fundamental limitation: they may enable data owners to retain control over the only copy of their information, but each of these technologies alone do not enable data owners to obtain fine-grained control over which AI predictions they wish to support.",
      "section_id": "institutional-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes federated learning as keeping training data distributed on devices and learning a shared model through aggregated updates, directly supporting the claim about PETs like federated learning solving collective action problems.",
        "checked_at": "2026-02-27T22:56:14.160440+00:00"
      }
    },
    {
      "id": "index:ref-10:0",
      "chapter": "index",
      "ref_num": 10,
      "bibtex_key": "Rieke_2020",
      "cite_label": "Rieke et al., 2020",
      "claim_context": "Recent work has suggested that privacy-enhancing technologies (PETs) can solve this collective action problem, suggesting technologies like federated learning, secure multi-party computation, and synthetic data ([McMahan et al., 2016]; [Rieke et al., 2020]; [Zhou et al., 2024]; [Jadon and Kumar 2023]). These efforts, however, face a fundamental limitation: they may enable data owners to retain control over the only copy of their information, but each of these technologies alone do not enable data owners to obtain fine-grained control over which AI predictions they wish to support.",
      "section_id": "institutional-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'The future of digital health with federated learning' directly relates to the claim about federated learning as a privacy-enhancing technology, but source timed out and no abstract available.",
        "checked_at": "2026-02-27T22:56:14.160441+00:00"
      }
    },
    {
      "id": "index:ref-39:0",
      "chapter": "index",
      "ref_num": 39,
      "bibtex_key": "smpc_ml",
      "cite_label": "Zhou et al., 2024",
      "claim_context": "Recent work has suggested that privacy-enhancing technologies (PETs) can solve this collective action problem, suggesting technologies like federated learning, secure multi-party computation, and synthetic data ([McMahan et al., 2016]; [Rieke et al., 2020]; [Zhou et al., 2024]; [Jadon and Kumar 2023]). These efforts, however, face a fundamental limitation: they may enable data owners to retain control over the only copy of their information, but each of these technologies alone do not enable data owners to obtain fine-grained control over which AI predictions they wish to support.",
      "section_id": "institutional-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract surveys secure multi-party computation for machine learning and discusses how SMPC champions data privacy through encrypted computations and secret sharing, directly supporting the claim about SMPC as a PET for solving collective action problems.",
        "checked_at": "2026-02-27T22:56:14.160443+00:00"
      }
    },
    {
      "id": "index:ref-40:0",
      "chapter": "index",
      "ref_num": 40,
      "bibtex_key": "synthetic_data_privacy",
      "cite_label": "Jadon and Kumar 2023",
      "claim_context": "Recent work has suggested that privacy-enhancing technologies (PETs) can solve this collective action problem, suggesting technologies like federated learning, secure multi-party computation, and synthetic data ([McMahan et al., 2016]; [Rieke et al., 2020]; [Zhou et al., 2024]; [Jadon and Kumar 2023]). These efforts, however, face a fundamental limitation: they may enable data owners to retain control over the only copy of their information, but each of these technologies alone do not enable data owners to obtain fine-grained control over which AI predictions they wish to support.",
      "section_id": "institutional-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses using generative AI for synthetic data generation in healthcare to balance data access and patient privacy, directly supporting the claim about synthetic data as a PET.",
        "checked_at": "2026-02-27T22:56:14.160444+00:00"
      }
    },
    {
      "id": "index:ref-9:1",
      "chapter": "index",
      "ref_num": 9,
      "bibtex_key": "fl",
      "cite_label": "McMahan et al., 2017",
      "claim_context": "Federated learning and synthetic data, for example, avoid the need to share raw data, enabling them to retain control over that data, but they don’t enable data providers to collectively control how an AI model they create is used because they do not provide a user-specific (i.e. attributionbased) control mechanism ([McMahan et al., 2017]; [Rubin 1993]). Secure multi-party computation schemes (secure enclaves, SPDZ, etc.) might allow for joint control over an AI model, but they do not inherently provide the metadata or control necessary for data providers to add or withdraw their support fo...",
      "section_id": "institutional-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes federated learning keeping data distributed on devices, directly supporting the claim that FL avoids sharing raw data but does not provide user-specific attribution-based control mechanisms.",
        "checked_at": "2026-02-27T22:56:14.160445+00:00"
      }
    },
    {
      "id": "index:ref-41:0",
      "chapter": "index",
      "ref_num": 41,
      "bibtex_key": "rubin1993statistical",
      "cite_label": "Rubin 1993",
      "claim_context": "Federated learning and synthetic data, for example, avoid the need to share raw data, enabling them to retain control over that data, but they don’t enable data providers to collectively control how an AI model they create is used because they do not provide a user-specific (i.e. attributionbased) control mechanism ([McMahan et al., 2017]; [Rubin 1993]). Secure multi-party computation schemes (secure enclaves, SPDZ, etc.) might allow for joint control over an AI model, but they do not inherently provide the metadata or control necessary for data providers to add or withdraw their support fo...",
      "section_id": "institutional-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Statistical disclosure limitation' relates to synthetic data and privacy-preserving data release, which is topically relevant to the claim about synthetic data not providing attribution-based control, but no abstract available.",
        "checked_at": "2026-02-27T22:56:14.160446+00:00"
      }
    },
    {
      "id": "index:ref-42:0",
      "chapter": "index",
      "ref_num": 42,
      "bibtex_key": "Yao1982ProtocolsFS",
      "cite_label": "Yao 1982b",
      "claim_context": "Federated learning and synthetic data, for example, avoid the need to share raw data, enabling them to retain control over that data, but they don’t enable data providers to collectively control how an AI model they create is used because they do not provide a user-specific (i.e. attributionbased) control mechanism ([McMahan et al., 2017]; [Rubin 1993]). Secure multi-party computation schemes (secure enclaves, SPDZ, etc.) might allow for joint control over an AI model, but they do not inherently provide the metadata or control necessary for data providers to add or withdraw their support fo...",
      "section_id": "institutional-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Protocols for secure computations' is the foundational SMPC paper by Yao, directly relevant to the claim about SMPC not inherently providing attribution metadata, but no abstract available to confirm specifics.",
        "checked_at": "2026-02-27T22:56:14.160447+00:00"
      }
    },
    {
      "id": "index:ref-43:0",
      "chapter": "index",
      "ref_num": 43,
      "bibtex_key": "lobo2023right",
      "cite_label": "Lobo et al., 2023",
      "claim_context": "Secure multi-party computation schemes (secure enclaves, SPDZ, etc.) might allow for joint control over an AI model, but they do not inherently provide the metadata or control necessary for data providers to add or withdraw their support for specific AI predictions (i.e. attribution) ([Yao 1982b]). To use SMPC for ABC, one would need to re-train AI models whenever a part of an AI model’s pre-training data needs to be removed (such as for compliance with right to be forgotten laws ([Lobo et al., 2023]).",
      "section_id": "institutional-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses the right to be forgotten in AI, including the challenge of erasing personal information from learned models and the need for retraining, directly supporting the claim about needing to re-train models for right-to-be-forgotten compliance.",
        "checked_at": "2026-02-27T22:56:14.160448+00:00"
      }
    },
    {
      "id": "index:ref-11:0",
      "chapter": "index",
      "ref_num": 11,
      "bibtex_key": "nguyen2024surveymachineunlearning",
      "cite_label": "Nguyen et al., 2024",
      "claim_context": "Thus, the complementary challenge is attribution (i.e., the ability to enable each data participant to efficiently elect to support or not support specific AI predictions) without needing to relinquish or economically bundle control within AI models. This attribution task is also referred to as machine unlearning, which at the time of writing remains an open problem in the literature ([Nguyen et al., 2024]).",
      "section_id": "institutional-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms machine unlearning is an unsolved problem due to lack of common frameworks, and surveys methods for making ML models forget data, directly supporting the claim that machine unlearning remains an open problem.",
        "checked_at": "2026-02-27T22:56:14.160449+00:00"
      }
    },
    {
      "id": "index:ref-12:0",
      "chapter": "index",
      "ref_num": 12,
      "bibtex_key": "amodei2016concreteproblemsaisafety",
      "cite_label": "Amodei et al., 2016",
      "claim_context": "The lack of ABC creates a governance crisis unprecedented in scale at the societal level. This crisis manifests publicly as concerns about AI safety, value alignment, and algorithmic bias ([Amodei et al., 2016]; [Gabriel 2020]; [Ntoutsi et al., 2020]). Crisis arises because AI systems can derive their capabilities from millions of contributors yet remain controlled by whomever possesses a copy of an AI.",
      "section_id": "societal-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses concrete AI safety problems including unintended harmful behavior from poor design, directly supporting the claim about AI safety concerns at the societal level.",
        "checked_at": "2026-02-27T22:56:14.160450+00:00"
      }
    },
    {
      "id": "index:ref-13:0",
      "chapter": "index",
      "ref_num": 13,
      "bibtex_key": "gabriel2020artificial",
      "cite_label": "Gabriel 2020",
      "claim_context": "The lack of ABC creates a governance crisis unprecedented in scale at the societal level. This crisis manifests publicly as concerns about AI safety, value alignment, and algorithmic bias ([Amodei et al., 2016]; [Gabriel 2020]; [Ntoutsi et al., 2020]). Crisis arises because AI systems can derive their capabilities from millions of contributors yet remain controlled by whomever possesses a copy of an AI.",
      "section_id": "societal-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses AI alignment, values, and the challenge of identifying fair principles for alignment, directly supporting the claim about value alignment as a societal concern.",
        "checked_at": "2026-02-27T22:56:14.160452+00:00"
      }
    },
    {
      "id": "index:ref-14:0",
      "chapter": "index",
      "ref_num": 14,
      "bibtex_key": "ntoutsi2020bias",
      "cite_label": "Ntoutsi et al., 2020",
      "claim_context": "The lack of ABC creates a governance crisis unprecedented in scale at the societal level. This crisis manifests publicly as concerns about AI safety, value alignment, and algorithmic bias ([Amodei et al., 2016]; [Gabriel 2020]; [Ntoutsi et al., 2020]). Crisis arises because AI systems can derive their capabilities from millions of contributors yet remain controlled by whomever possesses a copy of an AI.",
      "section_id": "societal-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Bias in data-driven artificial intelligence systems - An introductory survey' directly relates to the claim about algorithmic bias as a societal concern, but source inaccessible (HTTP 403) and no abstract available.",
        "checked_at": "2026-02-27T22:56:14.160453+00:00"
      }
    },
    {
      "id": "index:ref-21:1",
      "chapter": "index",
      "ref_num": 21,
      "bibtex_key": "ai_bioweapon",
      "cite_label": "Mouton et al., 2023",
      "claim_context": "This control structure sparks concern that either the AI or its owner will use the AI’s intelligence in ways that conflict with society’s values or interests (Gabriel et al., 2024). An LLM trained on chemistry research papers, for example, could be fine-tuned to generate instructions for bioweapons, with no way for the training data creators to prevent this misuse ([Mouton et al., 2023]).",
      "section_id": "societal-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses risks of AI (LLMs) being misused for biological attacks and generating concerning text, directly supporting the claim about LLMs being fine-tuned for bioweapons with no way for training data creators to prevent misuse.",
        "checked_at": "2026-02-27T22:56:14.160454+00:00"
      }
    },
    {
      "id": "index:ref-44:0",
      "chapter": "index",
      "ref_num": 44,
      "bibtex_key": "industryAustraliaAI2024",
      "cite_label": "Australian Government Department of Industry and Resources 2024",
      "claim_context": "Recent work has proposed various oversight mechanisms and safety guidelines in response ([Australian Government Department of Industry and Resources 2024]; [Commission 2024b]; [Commission 2024a]; [Commission 2020]; [on Ethics of Autonomous and Systems 2021]; [of Life Institute 2024]; [Now 2023]; [Institute 2024]; [Declaration 2024]; [Forum 2023]; [for Internet Society 2020]; [Institute 2019]; [Google 2024]; [Microsoft 2024]; [IBM 2018]). However, these efforts face a limitation history has shown before: attempting to solve the problem of serving many by empowering few ([Riesen 2023]; [Pope ...",
      "section_id": "societal-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Australia's AI Ethics Principles' directly matches the claim about various oversight mechanisms and safety guidelines, but source timed out and no abstract available.",
        "checked_at": "2026-02-27T22:56:14.160455+00:00"
      }
    },
    {
      "id": "index:ref-45:0",
      "chapter": "index",
      "ref_num": 45,
      "bibtex_key": "ecGermanyAI2024",
      "cite_label": "Commission 2024b",
      "claim_context": "Recent work has proposed various oversight mechanisms and safety guidelines in response ([Australian Government Department of Industry and Resources 2024]; [Commission 2024b]; [Commission 2024a]; [Commission 2020]; [on Ethics of Autonomous and Systems 2021]; [of Life Institute 2024]; [Now 2023]; [Institute 2024]; [Declaration 2024]; [Forum 2023]; [for Internet Society 2020]; [Institute 2019]; [Google 2024]; [Microsoft 2024]; [IBM 2018]). However, these efforts face a limitation history has shown before: attempting to solve the problem of serving many by empowering few ([Riesen 2023]; [Pope ...",
      "section_id": "societal-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title and page title confirm this is Germany's AI Strategy Report, supporting the claim about various oversight mechanisms proposed in response to AI governance concerns, but abstract contains only the title with no further detail.",
        "checked_at": "2026-02-27T22:56:14.160456+00:00"
      }
    },
    {
      "id": "index:ref-46:0",
      "chapter": "index",
      "ref_num": 46,
      "bibtex_key": "ecFranceAI2024",
      "cite_label": "Commission 2024a",
      "claim_context": "Recent work has proposed various oversight mechanisms and safety guidelines in response ([Australian Government Department of Industry and Resources 2024]; [Commission 2024b]; [Commission 2024a]; [Commission 2020]; [on Ethics of Autonomous and Systems 2021]; [of Life Institute 2024]; [Now 2023]; [Institute 2024]; [Declaration 2024]; [Forum 2023]; [for Internet Society 2020]; [Institute 2019]; [Google 2024]; [Microsoft 2024]; [IBM 2018]). However, these efforts face a limitation history has shown before: attempting to solve the problem of serving many by empowering few ([Riesen 2023]; [Pope ...",
      "section_id": "societal-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title and page title confirm this is France's AI Strategy Report, supporting the claim about various oversight mechanisms proposed in response to AI governance concerns, but abstract contains only the title with no further detail.",
        "checked_at": "2026-02-27T22:56:14.160457+00:00"
      }
    },
    {
      "id": "index:ref-47:0",
      "chapter": "index",
      "ref_num": 47,
      "bibtex_key": "ecWhitePaperAI2020",
      "cite_label": "Commission 2020",
      "claim_context": "Recent work has proposed various oversight mechanisms and safety guidelines in response ([Australian Government Department of Industry and Resources 2024]; [Commission 2024b]; [Commission 2024a]; [Commission 2020]; [on Ethics of Autonomous and Systems 2021]; [of Life Institute 2024]; [Now 2023]; [Institute 2024]; [Declaration 2024]; [Forum 2023]; [for Internet Society 2020]; [Institute 2019]; [Google 2024]; [Microsoft 2024]; [IBM 2018]). However, these efforts face a limitation history has shown before: attempting to solve the problem of serving many by empowering few ([Riesen 2023]; [Pope ...",
      "section_id": "societal-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'White Paper on Artificial Intelligence: A European Approach to Excellence and Trust' confirms this is an AI oversight/guidelines document, supporting the claim about proposed oversight mechanisms, but abstract contains only the title.",
        "checked_at": "2026-02-27T22:56:14.160458+00:00"
      }
    },
    {
      "id": "index:ref-48:0",
      "chapter": "index",
      "ref_num": 48,
      "bibtex_key": "ieeeEthicsAI2021",
      "cite_label": "on Ethics of Autonomous and Systems 2021",
      "claim_context": "Recent work has proposed various oversight mechanisms and safety guidelines in response ([Australian Government Department of Industry and Resources 2024]; [Commission 2024b]; [Commission 2024a]; [Commission 2020]; [on Ethics of Autonomous and Systems 2021]; [of Life Institute 2024]; [Now 2023]; [Institute 2024]; [Declaration 2024]; [Forum 2023]; [for Internet Society 2020]; [Institute 2019]; [Google 2024]; [Microsoft 2024]; [IBM 2018]). However, these efforts face a limitation history has shown before: attempting to solve the problem of serving many by empowering few ([Riesen 2023]; [Pope ...",
      "section_id": "societal-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Ethically Aligned Design: A Vision for Prioritizing Human Well-being with Artificial Intelligence and Autonomous Systems' directly matches the claim about AI safety guidelines, but no abstract available to confirm specifics.",
        "checked_at": "2026-02-27T22:56:14.160460+00:00"
      }
    },
    {
      "id": "index:ref-49:0",
      "chapter": "index",
      "ref_num": 49,
      "bibtex_key": "futureOfLifeAIPrinciples2024",
      "cite_label": "of Life Institute 2024",
      "claim_context": "Recent work has proposed various oversight mechanisms and safety guidelines in response ([Australian Government Department of Industry and Resources 2024]; [Commission 2024b]; [Commission 2024a]; [Commission 2020]; [on Ethics of Autonomous and Systems 2021]; [of Life Institute 2024]; [Now 2023]; [Institute 2024]; [Declaration 2024]; [Forum 2023]; [for Internet Society 2020]; [Institute 2019]; [Google 2024]; [Microsoft 2024]; [IBM 2018]). However, these efforts face a limitation history has shown before: attempting to solve the problem of serving many by empowering few ([Riesen 2023]; [Pope ...",
      "section_id": "societal-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms AI scientists developed the Asilomar AI governance principles, directly supporting the claim about proposed oversight mechanisms and safety guidelines for AI.",
        "checked_at": "2026-02-27T22:56:14.160461+00:00"
      }
    },
    {
      "id": "index:ref-50:0",
      "chapter": "index",
      "ref_num": 50,
      "bibtex_key": "accessNowTorontoDeclaration2023",
      "cite_label": "Now 2023",
      "claim_context": "Recent work has proposed various oversight mechanisms and safety guidelines in response ([Australian Government Department of Industry and Resources 2024]; [Commission 2024b]; [Commission 2024a]; [Commission 2020]; [on Ethics of Autonomous and Systems 2021]; [of Life Institute 2024]; [Now 2023]; [Institute 2024]; [Declaration 2024]; [Forum 2023]; [for Internet Society 2020]; [Institute 2019]; [Google 2024]; [Microsoft 2024]; [IBM 2018]). However, these efforts face a limitation history has shown before: attempting to solve the problem of serving many by empowering few ([Riesen 2023]; [Pope ...",
      "section_id": "societal-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes the Toronto Declaration on protecting rights to equality and non-discrimination in ML systems, endorsed by human rights organizations, directly supporting the claim about proposed AI oversight mechanisms.",
        "checked_at": "2026-02-27T22:56:14.160462+00:00"
      }
    },
    {
      "id": "index:ref-51:0",
      "chapter": "index",
      "ref_num": 51,
      "bibtex_key": "aiNowAlgorithmicAccountability2024",
      "cite_label": "Institute 2024",
      "claim_context": "Recent work has proposed various oversight mechanisms and safety guidelines in response ([Australian Government Department of Industry and Resources 2024]; [Commission 2024b]; [Commission 2024a]; [Commission 2020]; [on Ethics of Autonomous and Systems 2021]; [of Life Institute 2024]; [Now 2023]; [Institute 2024]; [Declaration 2024]; [Forum 2023]; [for Internet Society 2020]; [Institute 2019]; [Google 2024]; [Microsoft 2024]; [IBM 2018]). However, these efforts face a limitation history has shown before: attempting to solve the problem of serving many by empowering few ([Riesen 2023]; [Pope ...",
      "section_id": "societal-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes an algorithmic accountability policy toolkit with guidance on identifying and interrogating AI systems in government, directly supporting the claim about proposed AI oversight mechanisms.",
        "checked_at": "2026-02-27T22:56:14.160463+00:00"
      }
    },
    {
      "id": "index:ref-52:0",
      "chapter": "index",
      "ref_num": 52,
      "bibtex_key": "montrealDeclaration2024",
      "cite_label": "Declaration 2024",
      "claim_context": "Recent work has proposed various oversight mechanisms and safety guidelines in response ([Australian Government Department of Industry and Resources 2024]; [Commission 2024b]; [Commission 2024a]; [Commission 2020]; [on Ethics of Autonomous and Systems 2021]; [of Life Institute 2024]; [Now 2023]; [Institute 2024]; [Declaration 2024]; [Forum 2023]; [for Internet Society 2020]; [Institute 2019]; [Google 2024]; [Microsoft 2024]; [IBM 2018]). However, these efforts face a limitation history has shown before: attempting to solve the problem of serving many by empowering few ([Riesen 2023]; [Pope ...",
      "section_id": "societal-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes the Montreal Declaration as a collective effort for responsible AI development, directly supporting the claim about proposed AI oversight mechanisms and safety guidelines.",
        "checked_at": "2026-02-27T22:56:14.160464+00:00"
      }
    },
    {
      "id": "index:ref-53:0",
      "chapter": "index",
      "ref_num": 53,
      "bibtex_key": "weforumAIGovernance2023",
      "cite_label": "Forum 2023",
      "claim_context": "Recent work has proposed various oversight mechanisms and safety guidelines in response ([Australian Government Department of Industry and Resources 2024]; [Commission 2024b]; [Commission 2024a]; [Commission 2020]; [on Ethics of Autonomous and Systems 2021]; [of Life Institute 2024]; [Now 2023]; [Institute 2024]; [Declaration 2024]; [Forum 2023]; [for Internet Society 2020]; [Institute 2019]; [Google 2024]; [Microsoft 2024]; [IBM 2018]). However, these efforts face a limitation history has shown before: attempting to solve the problem of serving many by empowering few ([Riesen 2023]; [Pope ...",
      "section_id": "societal-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'AI Governance: A Holistic Approach to Implement Ethics into AI' directly matches the claim about proposed AI oversight mechanisms, but source inaccessible (HTTP 403) and no abstract available.",
        "checked_at": "2026-02-27T22:56:14.160465+00:00"
      }
    },
    {
      "id": "index:ref-54:0",
      "chapter": "index",
      "ref_num": 54,
      "bibtex_key": "harvardPrincipledAI2020",
      "cite_label": "for Internet Society 2020",
      "claim_context": "Recent work has proposed various oversight mechanisms and safety guidelines in response ([Australian Government Department of Industry and Resources 2024]; [Commission 2024b]; [Commission 2024a]; [Commission 2020]; [on Ethics of Autonomous and Systems 2021]; [of Life Institute 2024]; [Now 2023]; [Institute 2024]; [Declaration 2024]; [Forum 2023]; [for Internet Society 2020]; [Institute 2019]; [Google 2024]; [Microsoft 2024]; [IBM 2018]). However, these efforts face a limitation history has shown before: attempting to solve the problem of serving many by empowering few ([Riesen 2023]; [Pope ...",
      "section_id": "societal-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes comparing thirty-six prominent AI principles documents, directly supporting the claim about proposed oversight mechanisms and safety guidelines for AI.",
        "checked_at": "2026-02-27T22:56:14.160466+00:00"
      }
    },
    {
      "id": "index:ref-55:0",
      "chapter": "index",
      "ref_num": 55,
      "bibtex_key": "turingAIethics2019",
      "cite_label": "Institute 2019",
      "claim_context": "Recent work has proposed various oversight mechanisms and safety guidelines in response ([Australian Government Department of Industry and Resources 2024]; [Commission 2024b]; [Commission 2024a]; [Commission 2020]; [on Ethics of Autonomous and Systems 2021]; [of Life Institute 2024]; [Now 2023]; [Institute 2024]; [Declaration 2024]; [Forum 2023]; [for Internet Society 2020]; [Institute 2019]; [Google 2024]; [Microsoft 2024]; [IBM 2018]). However, these efforts face a limitation history has shown before: attempting to solve the problem of serving many by empowering few ([Riesen 2023]; [Pope ...",
      "section_id": "societal-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Understanding Artificial Intelligence Ethics and Safety' from the Alan Turing Institute directly matches the claim about proposed AI safety guidelines, but no abstract available to confirm specifics.",
        "checked_at": "2026-02-27T22:56:14.160468+00:00"
      }
    },
    {
      "id": "index:ref-75:0",
      "chapter": "index",
      "ref_num": 75,
      "bibtex_key": "googleAIPrinciples2024",
      "cite_label": "Google 2024",
      "claim_context": "Recent work has proposed various oversight mechanisms and safety guidelines in response ([Australian Government Department of Industry and Resources 2024]; [Commission 2024b]; [Commission 2024a]; [Commission 2020]; [on Ethics of Autonomous and Systems 2021]; [of Life Institute 2024]; [Now 2023]; [Institute 2024]; [Declaration 2024]; [Forum 2023]; [for Internet Society 2020]; [Institute 2019]; [Google 2024]; [Microsoft 2024]; [IBM 2018]). However, these efforts face a limitation history has shown before: attempting to solve the problem of serving many by empowering few ([Riesen 2023]; [Pope ...",
      "section_id": "societal-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes Google's guiding framework for responsible AI development and use, directly supporting the claim about proposed AI oversight mechanisms and safety guidelines from tech companies.",
        "checked_at": "2026-02-27T22:56:14.160469+00:00"
      }
    },
    {
      "id": "index:ref-56:0",
      "chapter": "index",
      "ref_num": 56,
      "bibtex_key": "microsoftAIPrinciples2024",
      "cite_label": "Microsoft 2024",
      "claim_context": "Recent work has proposed various oversight mechanisms and safety guidelines in response ([Australian Government Department of Industry and Resources 2024]; [Commission 2024b]; [Commission 2024a]; [Commission 2020]; [on Ethics of Autonomous and Systems 2021]; [of Life Institute 2024]; [Now 2023]; [Institute 2024]; [Declaration 2024]; [Forum 2023]; [for Internet Society 2020]; [Institute 2019]; [Google 2024]; [Microsoft 2024]; [IBM 2018]). However, these efforts face a limitation history has shown before: attempting to solve the problem of serving many by empowering few ([Riesen 2023]; [Pope ...",
      "section_id": "societal-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes Microsoft's AI governance solutions and responsible AI practices, directly supporting the claim about proposed AI oversight mechanisms from tech companies.",
        "checked_at": "2026-02-27T22:56:14.160470+00:00"
      }
    },
    {
      "id": "index:ref-57:0",
      "chapter": "index",
      "ref_num": 57,
      "bibtex_key": "ibmAIPrinciples2018",
      "cite_label": "IBM 2018",
      "claim_context": "Recent work has proposed various oversight mechanisms and safety guidelines in response ([Australian Government Department of Industry and Resources 2024]; [Commission 2024b]; [Commission 2024a]; [Commission 2020]; [on Ethics of Autonomous and Systems 2021]; [of Life Institute 2024]; [Now 2023]; [Institute 2024]; [Declaration 2024]; [Forum 2023]; [for Internet Society 2020]; [Institute 2019]; [Google 2024]; [Microsoft 2024]; [IBM 2018]). However, these efforts face a limitation history has shown before: attempting to solve the problem of serving many by empowering few ([Riesen 2023]; [Pope ...",
      "section_id": "societal-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Abstract is a general IBM policy page description rather than the specific principles document, but the page title 'IBM Policy' and the reference title confirm it relates to IBM's AI trust principles, supporting the claim about tech company safety guidelines.",
        "checked_at": "2026-02-27T22:56:14.160471+00:00"
      }
    },
    {
      "id": "index:ref-58:0",
      "chapter": "index",
      "ref_num": 58,
      "bibtex_key": "riesen2023imagine",
      "cite_label": "Riesen 2023",
      "claim_context": "Recent work has proposed various oversight mechanisms and safety guidelines in response ([Australian Government Department of Industry and Resources 2024]; [Commission 2024b]; [Commission 2024a]; [Commission 2020]; [on Ethics of Autonomous and Systems 2021]; [of Life Institute 2024]; [Now 2023]; [Institute 2024]; [Declaration 2024]; [Forum 2023]; [for Internet Society 2020]; [Institute 2019]; [Google 2024]; [Microsoft 2024]; [IBM 2018]). However, these efforts face a limitation history has shown before: attempting to solve the problem of serving many by empowering few ([Riesen 2023]; [Pope ...",
      "section_id": "societal-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Imagine A World: What if global challenges led to more centralization?' is relevant to the claim about the historical limitation of solving problems by empowering few, but no abstract available to confirm specific content.",
        "checked_at": "2026-02-27T22:56:14.160472+00:00"
      }
    },
    {
      "id": "index:ref-59:0",
      "chapter": "index",
      "ref_num": 59,
      "bibtex_key": "pope2023centralizing",
      "cite_label": "Pope 2023",
      "claim_context": "Recent work has proposed various oversight mechanisms and safety guidelines in response ([Australian Government Department of Industry and Resources 2024]; [Commission 2024b]; [Commission 2024a]; [Commission 2020]; [on Ethics of Autonomous and Systems 2021]; [of Life Institute 2024]; [Now 2023]; [Institute 2024]; [Declaration 2024]; [Forum 2023]; [for Internet Society 2020]; [Institute 2019]; [Google 2024]; [Microsoft 2024]; [IBM 2018]). However, these efforts face a limitation history has shown before: attempting to solve the problem of serving many by empowering few ([Riesen 2023]; [Pope ...",
      "section_id": "societal-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Title and abstract snippet about AI being easier to control through centralization directly supports the claim about the limitation of empowering few to serve many in AI governance.",
        "checked_at": "2026-02-27T22:56:14.160473+00:00"
      }
    },
    {
      "id": "index:ref-60:0",
      "chapter": "index",
      "ref_num": 60,
      "bibtex_key": "crispweed2024alignment",
      "cite_label": "crispweed 2024",
      "claim_context": "Recent work has proposed various oversight mechanisms and safety guidelines in response ([Australian Government Department of Industry and Resources 2024]; [Commission 2024b]; [Commission 2024a]; [Commission 2020]; [on Ethics of Autonomous and Systems 2021]; [of Life Institute 2024]; [Now 2023]; [Institute 2024]; [Declaration 2024]; [Forum 2023]; [for Internet Society 2020]; [Institute 2019]; [Google 2024]; [Microsoft 2024]; [IBM 2018]). However, these efforts face a limitation history has shown before: attempting to solve the problem of serving many by empowering few ([Riesen 2023]; [Pope ...",
      "section_id": "societal-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Title 'The Alignment Trap: AI Safety as Path to Power' and abstract about AI safety discussions around keeping AI under human control directly support the claim about the historical limitation of empowering few to serve many.",
        "checked_at": "2026-02-27T22:56:14.160474+00:00"
      }
    },
    {
      "id": "index:ref-61:0",
      "chapter": "index",
      "ref_num": 61,
      "bibtex_key": "summerfield2025impact",
      "cite_label": "Summerfield et al., 2025",
      "claim_context": "Recent work has proposed various oversight mechanisms and safety guidelines in response ([Australian Government Department of Industry and Resources 2024]; [Commission 2024b]; [Commission 2024a]; [Commission 2020]; [on Ethics of Autonomous and Systems 2021]; [of Life Institute 2024]; [Now 2023]; [Institute 2024]; [Declaration 2024]; [Forum 2023]; [for Internet Society 2020]; [Institute 2019]; [Google 2024]; [Microsoft 2024]; [IBM 2018]). However, these efforts face a limitation history has shown before: attempting to solve the problem of serving many by empowering few ([Riesen 2023]; [Pope ...",
      "section_id": "societal-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'The impact of advanced AI systems on democracy' is directly relevant to the claim about AI oversight and democratic values, but source timed out and no abstract available.",
        "checked_at": "2026-02-27T22:56:14.160476+00:00"
      }
    },
    {
      "id": "index:ref-62:0",
      "chapter": "index",
      "ref_num": 62,
      "bibtex_key": "welle2025aligning",
      "cite_label": "Welle 2025",
      "claim_context": "When individuals, small groups, or tech companies train AI models on global internet data, decisions about content filtering and model behavior end up being made by AI models or small teams (at most thousands attempting to represent billions). AI governance therefore becomes an exercise in altruism, operating under the hope that AI models, IT staff, or institutions controlling them will forego selfish incentives to serve humanity’s broader interests ([Welle 2025]). These hopes may be disappointed: despite grand claims, leading AI labs may not be successfully evading their selfish incentives...",
      "section_id": "societal-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Aligning those who align AI' relates to the claim about AI governance being an exercise in altruism, but the abstract only mentions a satirical website about AI alignment without substantive content to confirm the specific claim.",
        "checked_at": "2026-02-27T22:56:14.160477+00:00"
      }
    },
    {
      "id": "index:ref-63:0",
      "chapter": "index",
      "ref_num": 63,
      "bibtex_key": "Samuel_2024",
      "cite_label": "Samuel 2024",
      "claim_context": "AI governance therefore becomes an exercise in altruism, operating under the hope that AI models, IT staff, or institutions controlling them will forego selfish incentives to serve humanity’s broader interests ([Welle 2025]). These hopes may be disappointed: despite grand claims, leading AI labs may not be successfully evading their selfish incentives ([Samuel 2024]; [Hu and Cai 2024]; [Morales 2024]).",
      "section_id": "societal-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract states 'Company insiders explain why safety-conscious employees are leaving' OpenAI, directly supporting the claim that AI labs may not be successfully evading selfish incentives despite grand claims.",
        "checked_at": "2026-02-27T22:56:14.160478+00:00"
      }
    },
    {
      "id": "index:ref-64:0",
      "chapter": "index",
      "ref_num": 64,
      "bibtex_key": "Hu_Cai_2024",
      "cite_label": "Hu and Cai 2024",
      "claim_context": "AI governance therefore becomes an exercise in altruism, operating under the hope that AI models, IT staff, or institutions controlling them will forego selfish incentives to serve humanity’s broader interests ([Welle 2025]). These hopes may be disappointed: despite grand claims, leading AI labs may not be successfully evading their selfish incentives ([Samuel 2024]; [Hu and Cai 2024]; [Morales 2024]).",
      "section_id": "societal-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'OpenAI to remove non-profit control and give Sam Altman equity' directly supports the claim about AI labs not evading selfish incentives, but source inaccessible (HTTP 401) and no abstract available.",
        "checked_at": "2026-02-27T22:56:14.160479+00:00"
      }
    },
    {
      "id": "index:ref-65:0",
      "chapter": "index",
      "ref_num": 65,
      "bibtex_key": "Morales_2024",
      "cite_label": "Morales 2024",
      "claim_context": "AI governance therefore becomes an exercise in altruism, operating under the hope that AI models, IT staff, or institutions controlling them will forego selfish incentives to serve humanity’s broader interests ([Welle 2025]). These hopes may be disappointed: despite grand claims, leading AI labs may not be successfully evading their selfish incentives ([Samuel 2024]; [Hu and Cai 2024]; [Morales 2024]).",
      "section_id": "societal-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title about Musk's concerns over 'AI dictatorship' at Google DeepMind directly relates to the claim about AI labs not evading selfish incentives, but source inaccessible (HTTP 403) and no abstract available.",
        "checked_at": "2026-02-27T22:56:14.160480+00:00"
      }
    },
    {
      "id": "index:ref-15:0",
      "chapter": "index",
      "ref_num": 15,
      "bibtex_key": "scaling_laws_2020",
      "cite_label": "Kaplan et al., 2020",
      "claim_context": "This problem stems from a fundamental decoupling: AI capability and AI alignment operate independently. Due to scaling laws, an AI model becomes more capable when its owner can centralize more data and compute ([Kaplan et al., 2020]; [Hoffmann et al., 2022]). Yet its alignment with human values depends on the AI model and/or its supervisors choosing to forego selfish incentives, two forces that are not naturally correlated ([crispweed 2024])).",
      "section_id": "societal-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract demonstrates that model performance scales as a power-law with model size, dataset size, and compute, directly supporting the claim that AI becomes more capable with more centralized data and compute.",
        "checked_at": "2026-02-27T22:56:14.160481+00:00"
      }
    },
    {
      "id": "index:ref-16:0",
      "chapter": "index",
      "ref_num": 16,
      "bibtex_key": "hoffmann2022trainingcomputeoptimallargelanguage",
      "cite_label": "Hoffmann et al., 2022",
      "claim_context": "This problem stems from a fundamental decoupling: AI capability and AI alignment operate independently. Due to scaling laws, an AI model becomes more capable when its owner can centralize more data and compute ([Kaplan et al., 2020]; [Hoffmann et al., 2022]). Yet its alignment with human values depends on the AI model and/or its supervisors choosing to forego selfish incentives, two forces that are not naturally correlated ([crispweed 2024])).",
      "section_id": "societal-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract shows that for compute-optimal training, model size and training tokens should scale equally, directly supporting the claim about scaling laws linking AI capability to data and compute.",
        "checked_at": "2026-02-27T22:56:14.160482+00:00"
      }
    },
    {
      "id": "index:ref-60:1",
      "chapter": "index",
      "ref_num": 60,
      "bibtex_key": "crispweed2024alignment",
      "cite_label": "crispweed 2024",
      "claim_context": "Due to scaling laws, an AI model becomes more capable when its owner can centralize more data and compute ([Kaplan et al., 2020]; [Hoffmann et al., 2022]). Yet its alignment with human values depends on the AI model and/or its supervisors choosing to forego selfish incentives, two forces that are not naturally correlated ([crispweed 2024])). This decoupling creates a dangerous asymmetry: unlike democratic systems, where a leader’s power ideally scales with public support, AI systems concentrate power without requiring ongoing public consent.",
      "section_id": "societal-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'The Alignment Trap: AI Safety as Path to Power' and abstract snippet about AI safety discussions are relevant to the claim that alignment depends on supervisors foregoing selfish incentives, but the truncated abstract does not confirm the specific claim.",
        "checked_at": "2026-02-27T22:56:14.160484+00:00"
      }
    },
    {
      "id": "index:ref-59:1",
      "chapter": "index",
      "ref_num": 59,
      "bibtex_key": "pope2023centralizing",
      "cite_label": "Pope 2023",
      "claim_context": "reatening democratic values through greater autonomy, adoption, and complexity. This represents not merely a policy problem, but an architectural one rooted in how AI systems process information ([Pope 2023]). As this thesis shows, when neural networks irreversibly: 1) combine data through addition operations and 2) decouple models from source data through copy operations, and 3) route the selection of trusted sources and users through ultra-high branching intermediaries, they sever the connection between data contributors and AI users, shifting control over an AI model to the model itself ...",
      "section_id": "societal-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Title 'AI is centralizing by default; let's not make it worse' directly supports the claim that AI centralization represents an architectural problem rooted in how AI systems process information.",
        "checked_at": "2026-02-27T22:56:14.160485+00:00"
      }
    },
    {
      "id": "index:ref-7:2",
      "chapter": "index",
      "ref_num": 7,
      "bibtex_key": "st",
      "cite_label": "(Trask et al., 2020)",
      "claim_context": "As this thesis shows, when neural networks irreversibly: 1) combine data through addition operations and 2) decouple models from source data through copy operations, and 3) route the selection of trusted sources and users through ultra-high branching intermediaries, they sever the connection between data contributors and AI users, shifting control over an AI model to the model itself and/or its owner. This architectural design means no amount of after-the-fact regulation can restore the lost capability for continuous, granular, public consent over how AI systems use society’s knowledge (con...",
      "section_id": "societal-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses the copy problem and how sharing information means losing control, directly supporting the claim about neural networks severing the connection between data contributors and AI users through copy operations.",
        "checked_at": "2026-02-27T22:56:14.160486+00:00"
      }
    },
    {
      "id": "index:ref-66:0",
      "chapter": "index",
      "ref_num": 66,
      "bibtex_key": "mirvish2011hathaway",
      "cite_label": "Mirvish 2017",
      "claim_context": "As an early example of this, in 2011 the Huffington Post broke a story about the Hathaway Effect, that when famous actress Anne Hathaway experiences a significant career moment, similarly-named Berkshire Hathaway stock rises as a result of algorithmic agents failing to disambiguate in online sentiment analysis ([Mirvish 2017]). The setup is a direct comparison to LLMs.",
      "section_id": "societal-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes automated stock trading programs picking up internet chatter about 'Hathaway' and applying it to Berkshire Hathaway stock, directly supporting the claim about the Hathaway Effect and algorithmic disambiguation failures.",
        "checked_at": "2026-02-27T22:56:14.160628+00:00"
      }
    },
    {
      "id": "index:ref-15:1",
      "chapter": "index",
      "ref_num": 15,
      "bibtex_key": "scaling_laws_2020",
      "cite_label": "Kaplan et al., 2020",
      "claim_context": "The lack of ABC creates tension between AI capability and democratic values at the geopolitical level. AI systems become more powerful with more data and compute, as described by so-called ”Scaling Laws” ([Kaplan et al., 2020];[Hoffmann et al., 2022]) and implied by the ”Bitter Lesson” ([Sutton 2019]). But without ABC, acquiring data (and compute) means acquiring a copy of it within a company, and acquiring a copy means centralizing unilateral control... over the data, over the compute, and over the AI systems this centralization creates.",
      "section_id": "geopolitical-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract demonstrates scaling laws showing performance improves with model size, dataset size, and compute, directly supporting the claim about AI becoming more powerful with more data and compute.",
        "checked_at": "2026-02-27T22:56:14.160630+00:00"
      }
    },
    {
      "id": "index:ref-16:1",
      "chapter": "index",
      "ref_num": 16,
      "bibtex_key": "hoffmann2022trainingcomputeoptimallargelanguage",
      "cite_label": "Hoffmann et al., 2022",
      "claim_context": "The lack of ABC creates tension between AI capability and democratic values at the geopolitical level. AI systems become more powerful with more data and compute, as described by so-called ”Scaling Laws” ([Kaplan et al., 2020];[Hoffmann et al., 2022]) and implied by the ”Bitter Lesson” ([Sutton 2019]). But without ABC, acquiring data (and compute) means acquiring a copy of it within a company, and acquiring a copy means centralizing unilateral control... over the data, over the compute, and over the AI systems this centralization creates.",
      "section_id": "geopolitical-consequences",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract shows compute-optimal training requires scaling model size and training tokens equally, directly supporting the claim about scaling laws linking AI power to data and compute.",
        "checked_at": "2026-02-27T22:56:14.160631+00:00"
      }
    },
    {
      "id": "index:ref-17:0",
      "chapter": "index",
      "ref_num": 17,
      "bibtex_key": "sutton2019bitter",
      "cite_label": "Sutton 2019",
      "claim_context": "The lack of ABC creates tension between AI capability and democratic values at the geopolitical level. AI systems become more powerful with more data and compute, as described by so-called ”Scaling Laws” ([Kaplan et al., 2020];[Hoffmann et al., 2022]) and implied by the ”Bitter Lesson” ([Sutton 2019]). But without ABC, acquiring data (and compute) means acquiring a copy of it within a company, and acquiring a copy means centralizing unilateral control... over the data, over the compute, and over the AI systems this centralization creates.",
      "section_id": "geopolitical-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'The bitter lesson' is the well-known Sutton essay about compute scaling in AI, directly relevant to the claim about the 'Bitter Lesson' implying AI power scales with compute, but no abstract available to confirm specifics.",
        "checked_at": "2026-02-27T22:56:14.160633+00:00"
      }
    },
    {
      "id": "index:ref-67:0",
      "chapter": "index",
      "ref_num": 67,
      "bibtex_key": "Tong_Martina_2024",
      "cite_label": "Tong and Martina 2024",
      "claim_context": "Some policymakers have responded to this crisis not by addressing the underlying ABC problem, but by proposing a ”Manhattan Project” for AI ([Tong and Martina 2024]). These proposals suggest competing with China’s centralization by out-centralizing them... meeting authoritarian AI capability by building a larger, even more centralized AI capability domestically.",
      "section_id": "geopolitical-consequences",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'US Government Commission pushes Manhattan Project-style AI Initiative' directly supports the claim about policymakers proposing a Manhattan Project for AI, but source inaccessible (HTTP 401) and no abstract available.",
        "checked_at": "2026-02-27T22:56:14.160634+00:00"
      }
    },
    {
      "id": "index:ref-18:0",
      "chapter": "index",
      "ref_num": 18,
      "bibtex_key": "Goodfellow-et-al-2016",
      "cite_label": "Goodfellow et al., 2016",
      "claim_context": "In the same way, when a deep learning AI model is trained, each datapoint is translated into a ”gradient update” which is added into the previous weights of the model, losing the ability to know which aspects of each weight were derived from which datapoints and thus the ability to later remove specific datapoints’ influence ([Goodfellow et al., 2016]). Thus, despite recent work, when an AI model makes a prediction, no-one can verify which source datapoints are informing that prediction (i.e. influence functions), nor can they cleanly remove problematic datapoints’ influence (i.e. ”unlearni...",
      "section_id": "addition-problem",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Deep Learning' (Goodfellow et al., 2016) is the canonical deep learning textbook that covers gradient-based training, relevant to the claim about gradient updates destroying per-datapoint attribution, but no abstract available (book source).",
        "checked_at": "2026-02-27T22:56:14.160635+00:00"
      }
    },
    {
      "id": "index:ref-11:1",
      "chapter": "index",
      "ref_num": 11,
      "bibtex_key": "nguyen2024surveymachineunlearning",
      "cite_label": "Nguyen et al., 2024",
      "claim_context": "Thus, despite recent work, when an AI model makes a prediction, no-one can verify which source datapoints are informing that prediction (i.e. influence functions), nor can they cleanly remove problematic datapoints’ influence (i.e. ”unlearning”) without retraining the entire model. Despite attempts, influence function based unlearning remains an unsolved challenge ([Nguyen et al., 2024]), blocking an ABC solution... because addition fundamentally destroys information which cannot then be recovered. Addition blocks attribution, which blocks attribution-based control in AI systems.",
      "section_id": "addition-problem",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms machine unlearning remains unsolved due to lack of common frameworks, directly supporting the claim that influence function-based unlearning remains an unsolved challenge blocking ABC.",
        "checked_at": "2026-02-27T22:56:14.160636+00:00"
      }
    },
    {
      "id": "index:ref-7:3",
      "chapter": "index",
      "ref_num": 7,
      "bibtex_key": "st",
      "cite_label": "Trask et al., 2020",
      "claim_context": "When a person makes a copy of their information and gives it to another, they lose the ability to enforce how that information might be used ([Trask et al., 2020]). And upon this core flaw, even if AI attribution was solved (e.g. unlearning and influence functions were solved), certain dominoes would still fall and avert ABC within AI systems.",
      "section_id": "copy-problem",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract explicitly discusses the copy problem: once information is copied and shared, the sender can no longer control how the recipient uses it, directly supporting the claim about losing control over copied information.",
        "checked_at": "2026-02-27T22:56:14.160637+00:00"
      }
    },
    {
      "id": "index:ref-19:0",
      "chapter": "index",
      "ref_num": 19,
      "bibtex_key": "dunbar1993coevolution",
      "cite_label": "Dunbar 1993",
      "claim_context": "Similarly, when a person needs to broadcast information to a small number of recipients, they can often evaluate each recipient’s trustworthiness before sharing, and in-so-doing protect their privacy, further their interests, and broadly avert what they might consider to be information mis-use. However, when a person needs to synthesize information from billions of sources or broadcast to billions of recipients, they necessarily cannot evaluate each party individually ([Dunbar 1993]) they are forced to delegate evaluation to intermediaries who scale trust evaluation on their behalf. And upo...",
      "section_id": "branching-problem",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses Dunbar's number and the limits on group size that humans can maintain, directly supporting the claim that individuals cannot evaluate billions of parties individually and must delegate to intermediaries.",
        "checked_at": "2026-02-27T22:56:14.160638+00:00"
      }
    },
    {
      "id": "index:ref-68:0",
      "chapter": "index",
      "ref_num": 68,
      "bibtex_key": "2f928592-a19d-38f4-91e4-45f12ea471a0",
      "cite_label": "BURT 1992",
      "claim_context": "Similarly, when users query high-branching AI systems (where a single model branches out to billions of users), they lose the ability to evaluate which of billions of sources inform their predictions... thereby losing any enforceable control over whether those sources are reliable, independent, or free from conflicts of interest. Taken together, because of the overuse of high-branching aggregation, AI concentrates source and user selection at whichever entities facilitate high-branching operations: the platforms connecting billions of sources to billions of users ([BURT 1992]; [Burt 2003]).",
      "section_id": "branching-problem",
      "verification": {
        "status": "unverifiable",
        "reasoning": "Reference has no title, no authors, no URL, no abstract, and no access type - the bibliographic entry appears to be empty or broken.",
        "checked_at": "2026-02-27T22:56:14.160640+00:00"
      }
    },
    {
      "id": "index:ref-76:0",
      "chapter": "index",
      "ref_num": 76,
      "bibtex_key": "burt2003social",
      "cite_label": "Burt 2003",
      "claim_context": "Similarly, when users query high-branching AI systems (where a single model branches out to billions of users), they lose the ability to evaluate which of billions of sources inform their predictions... thereby losing any enforceable control over whether those sources are reliable, independent, or free from conflicts of interest. Taken together, because of the overuse of high-branching aggregation, AI concentrates source and user selection at whichever entities facilitate high-branching operations: the platforms connecting billions of sources to billions of users ([BURT 1992]; [Burt 2003]).",
      "section_id": "branching-problem",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'The social structure of competition' by Burt relates to network theory and structural holes in information brokerage, which is relevant to the claim about high-branching platforms concentrating source/user selection, but source inaccessible (HTTP 403) and no abstract available.",
        "checked_at": "2026-02-27T22:56:14.160641+00:00"
      }
    },
    {
      "id": "index:ref-20:0",
      "chapter": "index",
      "ref_num": 20,
      "bibtex_key": "freeman1977set",
      "cite_label": "Freeman 1977",
      "claim_context": "While platforms may attempt to perform this selection faithfully, the branching problem underpins a structural limitation: high-branching operations mathematically necessitate centralized selection authority at information bottlenecks ([Freeman 1977]). And the overuse of high-branching aggregation doesn’t just create systems which can have centralized selection, it creates systems which must have centralized selection, systems which explicitly prohibit distributed bilateral authority over source and user selection.",
      "section_id": "branching-problem",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'A set of measures of centrality based on betweenness' by Freeman is a foundational network centrality paper, relevant to the claim about high-branching operations necessitating centralized selection at bottlenecks, but only bibliographic JSTOR metadata available without the actual abstract.",
        "checked_at": "2026-02-27T22:56:14.160642+00:00"
      }
    },
    {
      "id": "index:ref-18:1",
      "chapter": "index",
      "ref_num": 18,
      "bibtex_key": "Goodfellow-et-al-2016",
      "cite_label": "Goodfellow et al., 2016",
      "claim_context": "While a complete sociological analysis is beyond the technical scope of this thesis, examining current AI development reveals a striking pattern: researchers consistently draw inspiration from a specific conception of biological cognition ([Goodfellow et al., 2016]; [Russell et al., 1995]; [LeCun et al., 2015]), cognition that provides no attribution-based control over its predictions. For example, humans do not ask permission from their parents, friends, co-workers, or evolutionary ancestors every time they seek to use a concept taught to them by these external parties (i.e. every time the...",
      "section_id": "natural-problem",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Deep Learning' is the canonical AI/deep learning textbook, relevant to the claim about AI researchers drawing inspiration from biological cognition, but no abstract available (book source).",
        "checked_at": "2026-02-27T22:56:14.160643+00:00"
      }
    },
    {
      "id": "index:ref-69:0",
      "chapter": "index",
      "ref_num": 69,
      "bibtex_key": "russell1995modern",
      "cite_label": "Russell et al., 1995",
      "claim_context": "While a complete sociological analysis is beyond the technical scope of this thesis, examining current AI development reveals a striking pattern: researchers consistently draw inspiration from a specific conception of biological cognition ([Goodfellow et al., 2016]; [Russell et al., 1995]; [LeCun et al., 2015]), cognition that provides no attribution-based control over its predictions. For example, humans do not ask permission from their parents, friends, co-workers, or evolutionary ancestors every time they seek to use a concept taught to them by these external parties (i.e. every time the...",
      "section_id": "natural-problem",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'A modern approach' (Russell and Norvig's 'Artificial Intelligence') is the canonical AI textbook, directly relevant to the claim about AI researchers drawing from biological cognition models, but no abstract available.",
        "checked_at": "2026-02-27T22:56:14.160645+00:00"
      }
    },
    {
      "id": "index:ref-74:0",
      "chapter": "index",
      "ref_num": 74,
      "bibtex_key": "lecun2015deep",
      "cite_label": "LeCun et al., 2015",
      "claim_context": "While a complete sociological analysis is beyond the technical scope of this thesis, examining current AI development reveals a striking pattern: researchers consistently draw inspiration from a specific conception of biological cognition ([Goodfellow et al., 2016]; [Russell et al., 1995]; [LeCun et al., 2015]), cognition that provides no attribution-based control over its predictions. For example, humans do not ask permission from their parents, friends, co-workers, or evolutionary ancestors every time they seek to use a concept taught to them by these external parties (i.e. every time the...",
      "section_id": "natural-problem",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes deep learning methods using multiple processing layers inspired by computational models, directly supporting the claim about AI researchers drawing inspiration from biological cognition.",
        "checked_at": "2026-02-27T22:56:14.160646+00:00"
      }
    },
    {
      "id": "chapter2:ref-1:0",
      "chapter": "chapter2",
      "ref_num": 1,
      "bibtex_key": "scaling_laws_2020",
      "cite_label": "Kaplan et al., 2020",
      "claim_context": "Estimates below suggest that models are trained using less than 1/1,000,000th of the world’s data and AI compute productivity. Consequently, following AI’s scaling laws ([Kaplan et al., 2020]; [Hoffmann et al., 2022]), AI models possess capabilities which are insignificant compared to what existing data, compute, and algorithms could create.",
      "section_id": "chapter-summary",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms scaling laws for language model performance as power-law with model size, dataset size, and compute, directly supporting the claim about AI scaling laws.",
        "checked_at": "2026-02-27T22:56:14.226795+00:00"
      }
    },
    {
      "id": "chapter2:ref-2:0",
      "chapter": "chapter2",
      "ref_num": 2,
      "bibtex_key": "hoffmann2022trainingcomputeoptimallargelanguage",
      "cite_label": "Hoffmann et al.,\n                            2022",
      "claim_context": "Estimates below suggest that models are trained using less than 1/1,000,000th of the world’s data and AI compute productivity. Consequently, following AI’s scaling laws ([Kaplan et al., 2020]; [Hoffmann et al., 2022]), AI models possess capabilities which are insignificant compared to what existing data, compute, and algorithms could create.",
      "section_id": "chapter-summary",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms investigation of optimal model size and training tokens under compute budgets, directly supporting the claim about AI scaling laws governing model capabilities.",
        "checked_at": "2026-02-27T22:56:14.226987+00:00"
      }
    },
    {
      "id": "chapter2:ref-3:0",
      "chapter": "chapter2",
      "ref_num": 3,
      "bibtex_key": "robison2024openai",
      "cite_label": "Robison 2024",
      "claim_context": "As of NeurIPS 2024, leading AI researchers have reported that available compute and data reserves are approaching saturation, creating constraints on both computational resources and pre-training scale ([Robison 2024]; [Strati et al., 2024]). However, this assessment overlooks approximately six orders of magnitude of underutilized compute productivity and siloed data.",
      "section_id": "symptom",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract quotes Sutskever saying 'We've achieved peak data and there'll be no more,' directly supporting the claim that leading AI researchers reported data reserves are approaching saturation.",
        "checked_at": "2026-02-27T22:56:14.226990+00:00"
      }
    },
    {
      "id": "chapter2:ref-20:0",
      "chapter": "chapter2",
      "ref_num": 20,
      "bibtex_key": "gpu_shortage",
      "cite_label": "Strati et al., 2024",
      "claim_context": "As of NeurIPS 2024, leading AI researchers have reported that available compute and data reserves are approaching saturation, creating constraints on both computational resources and pre-training scale ([Robison 2024]; [Strati et al., 2024]). However, this assessment overlooks approximately six orders of magnitude of underutilized compute productivity and siloed data.",
      "section_id": "symptom",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'ML Training with Cloud GPU Shortages' is relevant to compute resource constraints, but source inaccessible (HTTP 403) and no abstract available to confirm specific claim.",
        "checked_at": "2026-02-27T22:56:14.226992+00:00"
      }
    },
    {
      "id": "chapter2:ref-21:0",
      "chapter": "chapter2",
      "ref_num": 21,
      "bibtex_key": "Kaye2025NvidiaFirstCompany",
      "cite_label": "Kaye\n                            2025",
      "claim_context": "The AI industry’s computational requirements have driven significant economic and geopolitical consequences, including NVIDIA’s rise to become the world’s most valuable company, U.S. export restrictions on AI chips to China, and intense competition for latest-generation hardware among startups, enterprises, and major technology firms ([Kaye 2025]; [Kachwala and Bajwa 2025]; [Howley 2023]). However, recent evidence suggests that current AI training and inference processes utilize less than 0.0002",
      "section_id": "underutilized-compute",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract and page title confirm Nvidia became world's first $5 trillion company, directly supporting the claim about NVIDIA's rise due to AI computational requirements.",
        "checked_at": "2026-02-27T22:56:14.226993+00:00"
      }
    },
    {
      "id": "chapter2:ref-22:0",
      "chapter": "chapter2",
      "ref_num": 22,
      "bibtex_key": "kachwala2024nvidia",
      "cite_label": "Kachwala and Bajwa 2025",
      "claim_context": "The AI industry’s computational requirements have driven significant economic and geopolitical consequences, including NVIDIA’s rise to become the world’s most valuable company, U.S. export restrictions on AI chips to China, and intense competition for latest-generation hardware among startups, enterprises, and major technology firms ([Kaye 2025]; [Kachwala and Bajwa 2025]; [Howley 2023]). However, recent evidence suggests that current AI training and inference processes utilize less than 0.0002% of available compute productivity, indicating that perceived compute scarcity may reflect ineff...",
      "section_id": "underutilized-compute",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Nvidia faces revenue threat from new U.S. AI chip export curbs' directly relates to the claim about U.S. export restrictions on AI chips, but source inaccessible (HTTP 401) with no abstract.",
        "checked_at": "2026-02-27T22:56:14.226994+00:00"
      }
    },
    {
      "id": "chapter2:ref-23:0",
      "chapter": "chapter2",
      "ref_num": 23,
      "bibtex_key": "howley2023nvidia",
      "cite_label": "Howley 2023",
      "claim_context": "The AI industry’s computational requirements have driven significant economic and geopolitical consequences, including NVIDIA’s rise to become the world’s most valuable company, U.S. export restrictions on AI chips to China, and intense competition for latest-generation hardware among startups, enterprises, and major technology firms ([Kaye 2025]; [Kachwala and Bajwa 2025]; [Howley 2023]). However, recent evidence suggests that current AI training and inference processes utilize less than 0.0002% of available compute productivity, indicating that perceived compute scarcity may reflect ineff...",
      "section_id": "underutilized-compute",
      "verification": {
        "status": "supported",
        "reasoning": "Title and abstract about Nvidia riding the AI wave and being called 'the only arms dealer' in the AI competition directly supports the claim about intense competition for AI hardware.",
        "checked_at": "2026-02-27T22:56:14.226996+00:00"
      }
    },
    {
      "id": "chapter2:ref-24:0",
      "chapter": "chapter2",
      "ref_num": 24,
      "bibtex_key": "epoch2025openaicomputespend",
      "cite_label": "You 2025",
      "claim_context": "Due to its role in commercial deployments, analysts estimate that AI firms spend billions of dollars annually on inference ([You 2025]). However, while these costs might appear to reflect fundamental requirements for achieving high performance, a growing body of empirical work indicates substantial inefficiency in current inference practices.",
      "section_id": "underutilized-compute",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract states OpenAI spent ~$7B on cloud compute with most going to R&D, directly supporting the claim that AI firms spend billions annually on compute including inference.",
        "checked_at": "2026-02-27T22:56:14.227170+00:00"
      }
    },
    {
      "id": "chapter2:ref-5:0",
      "chapter": "chapter2",
      "ref_num": 5,
      "bibtex_key": "izacard2023atlas",
      "cite_label": "Izacard et al., 2023",
      "claim_context": "DeepMind’s RETRO achieves comparable performance to GPT-3 while using 1/25th of the parameters through retrieval from a large-scale vector database (Borgeaud et al., 2022). Similarly, Meta’s ATLAS demonstrates that models can be reduced to 1/50th their original size while maintaining or exceeding baseline performance through database-augmented inference ([Izacard et al., 2023]).",
      "section_id": "underutilized-compute",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms Atlas outperforms a 540B parameter model with 50x fewer parameters through retrieval augmentation, directly supporting the claim about 1/50th model size with maintained performance.",
        "checked_at": "2026-02-27T22:56:14.227174+00:00"
      }
    },
    {
      "id": "chapter2:ref-55:0",
      "chapter": "chapter2",
      "ref_num": 55,
      "bibtex_key": "lederer2024statistical",
      "cite_label": "Lederer 2024",
      "claim_context": "We adopt RETRO/ATLAS-style parameter efficiency as a conservative lower bound on current compute waste, noting that these approaches have not been widely adopted in either the sparsity literature ([Lederer 2024]) or frontier AI deployments, nor have comparable efficiency gains been demonstrated through alternative methods (cf. the persistent redundancy problem in Mixture of Experts models ([Dai et al., 2024])). These results suggest that at least 96-98% of parameters activated during dense inference are unnecessary for individual queries.",
      "section_id": "underutilized-compute",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Statistical guarantees for sparse deep learning' is relevant to sparsity literature claim, but source inaccessible (timeout) with no abstract available.",
        "checked_at": "2026-02-27T22:56:14.227175+00:00"
      }
    },
    {
      "id": "chapter2:ref-25:0",
      "chapter": "chapter2",
      "ref_num": 25,
      "bibtex_key": "dai2024deepseekmoe",
      "cite_label": "Dai et al., 2024",
      "claim_context": "We adopt RETRO/ATLAS-style parameter efficiency as a conservative lower bound on current compute waste, noting that these approaches have not been widely adopted in either the sparsity literature ([Lederer 2024]) or frontier AI deployments, nor have comparable efficiency gains been demonstrated through alternative methods (cf. the persistent redundancy problem in Mixture of Experts models ([Dai et al., 2024])). These results suggest that at least 96-98% of parameters activated during dense inference are unnecessary for individual queries.",
      "section_id": "underutilized-compute",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses redundancy in MoE routed experts and proposes strategies to mitigate it, directly supporting the claim about persistent redundancy problems in Mixture of Experts models.",
        "checked_at": "2026-02-27T22:56:14.227177+00:00"
      }
    },
    {
      "id": "chapter2:ref-6:0",
      "chapter": "chapter2",
      "ref_num": 6,
      "bibtex_key": "guo2023towards",
      "cite_label": "Guo et al.,\n                            2023",
      "claim_context": "Recent work demonstrates that current architectures contain redundant and underutilized parameters. Guo et al. achieve 5-10x reduction in parameter count through lossless compression while maintaining accuracy: ”Notably, we distill CIFAR-10 and CIFAR-100 to 1/5 and Tiny ImageNet to 1/10 of their original sizes without any performance loss on ConvNet, offering the first lossless method of dataset distillation” ([Guo et al., 2023]). This compression has been demonstrated across multiple standard a",
      "section_id": "underutilized-compute",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms achieving 'lossless dataset distillation for the very first time' by compressing datasets to 1/5 and 1/10 of original sizes, supporting the claim about 5-10x reduction while maintaining accuracy.",
        "checked_at": "2026-02-27T22:56:14.227178+00:00"
      }
    },
    {
      "id": "chapter2:ref-26:0",
      "chapter": "chapter2",
      "ref_num": 26,
      "bibtex_key": "meta2024llama",
      "cite_label": "Meta AI 2024",
      "claim_context": "AI firms famously spend immense amounts of money training their AI models, a point which features heavily in their marketing ([Meta AI 2024]; [Brown et al., 2020]; [Wiggers 2024]). However, despite widespread hype around AI training spend, a theoretical inefficiency is backed by an increasingly large body of empirical observations, suggesting that the compute requirements for training AI models are largely misunderstood.",
      "section_id": "underutilized-compute",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Introducing Llama 3.1' is relevant to the claim about AI firms marketing their training spend, but source inaccessible (HTTP 400) with no abstract available.",
        "checked_at": "2026-02-27T22:56:14.227179+00:00"
      }
    },
    {
      "id": "chapter2:ref-27:0",
      "chapter": "chapter2",
      "ref_num": 27,
      "bibtex_key": "brown2020language",
      "cite_label": "Brown et al., 2020",
      "claim_context": "AI firms famously spend immense amounts of money training their AI models, a point which features heavily in their marketing ([Meta AI 2024]; [Brown et al., 2020]; [Wiggers 2024]). However, despite widespread hype around AI training spend, a theoretical inefficiency is backed by an increasingly large body of empirical observations, suggesting that the compute requirements for training AI models are largely misunderstood.",
      "section_id": "underutilized-compute",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes training GPT-3 with 175 billion parameters as a major achievement, supporting the claim that AI firms feature training costs heavily in their marketing and publications.",
        "checked_at": "2026-02-27T22:56:14.227181+00:00"
      }
    },
    {
      "id": "chapter2:ref-28:0",
      "chapter": "chapter2",
      "ref_num": 28,
      "bibtex_key": "wiggers2024openai",
      "cite_label": "Wiggers\n                            2024",
      "claim_context": "AI firms famously spend immense amounts of money training their AI models, a point which features heavily in their marketing ([Meta AI 2024]; [Brown et al., 2020]; [Wiggers 2024]). However, despite widespread hype around AI training spend, a theoretical inefficiency is backed by an increasingly large body of empirical observations, suggesting that the compute requirements for training AI models are largely misunderstood. To introduce the theory, consider an analogy.",
      "section_id": "underutilized-compute",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms Altman admitted lack of compute capacity prevents shipping products, supporting the claim about AI firms spending immense money on training and emphasizing compute needs.",
        "checked_at": "2026-02-27T22:56:14.227182+00:00"
      }
    },
    {
      "id": "chapter2:ref-29:0",
      "chapter": "chapter2",
      "ref_num": 29,
      "bibtex_key": "EpochAIModels2025",
      "cite_label": "Epoch AI 2025",
      "claim_context": "AI models store information within their weights. To acquire this information, models are trained on large corpora at substantial computational cost (e.g., significant portions of the public internet) ([Epoch AI 2025]). However, when models require substantial updates (either incorporating new information or removing outdated content) current practice involves retraining from scratch ([Goodfellow et al., 2016]).",
      "section_id": "underutilized-compute",
      "verification": {
        "status": "supported",
        "reasoning": "Database tracking 3200+ ML models from 1950 to today directly supports the claim about AI models being trained on large corpora at substantial computational cost, as a data source for these statistics.",
        "checked_at": "2026-02-27T22:56:14.227183+00:00"
      }
    },
    {
      "id": "chapter2:ref-7:0",
      "chapter": "chapter2",
      "ref_num": 7,
      "bibtex_key": "Goodfellow-et-al-2016",
      "cite_label": "Goodfellow et al., 2016",
      "claim_context": "To acquire this information, models are trained on large corpora at substantial computational cost (e.g., significant portions of the public internet) ([Epoch AI 2025]). However, when models require substantial updates (either incorporating new information or removing outdated content) current practice involves retraining from scratch ([Goodfellow et al., 2016]). This approach discards all previously computed parameters and repeats the entire training process, even when the majority of learned representations remain valid.",
      "section_id": "underutilized-compute",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Deep Learning' by Goodfellow et al. is the standard textbook that would cover retraining practices, but no abstract available to confirm the specific claim about retraining from scratch.",
        "checked_at": "2026-02-27T22:56:14.227184+00:00"
      }
    },
    {
      "id": "chapter2:ref-30:0",
      "chapter": "chapter2",
      "ref_num": 30,
      "bibtex_key": "mehta2024zuckerberg",
      "cite_label": "Mehta 2024",
      "claim_context": "Analysis of the largest AI firms reveals that pre-training their most capable models consumes less than 1% of quarterly compute budgets (see Table 2.2; methodology detailed in Appendices I and II). Yet these same firms continue expanding computational infrastructure to support larger models ([Mehta 2024]; [OpenAI 2024]; [Sevilla and Roldan 2024]), suggesting that remaining compute capacity is allocated to other training activities rather than final model production.",
      "section_id": "underutilized-compute",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract quotes Zuckerberg saying Llama 4 training needs 10x more compute than Llama 3, directly supporting the claim that firms continue expanding computational infrastructure for larger models.",
        "checked_at": "2026-02-27T22:56:14.227186+00:00"
      }
    },
    {
      "id": "chapter2:ref-31:0",
      "chapter": "chapter2",
      "ref_num": 31,
      "bibtex_key": "openai2024learning",
      "cite_label": "OpenAI 2024",
      "claim_context": "Analysis of the largest AI firms reveals that pre-training their most capable models consumes less than 1% of quarterly compute budgets (see Table 2.2; methodology detailed in Appendices I and II). Yet these same firms continue expanding computational infrastructure to support larger models ([Mehta 2024]; [OpenAI 2024]; [Sevilla and Roldan 2024]), suggesting that remaining compute capacity is allocated to other training activities rather than final model production.",
      "section_id": "underutilized-compute",
      "verification": {
        "status": "plausible",
        "reasoning": "Abstract describes OpenAI o1 as a new model trained with reinforcement learning for complex reasoning, relevant to expanding infrastructure but does not directly confirm infrastructure expansion claims.",
        "checked_at": "2026-02-27T22:56:14.227187+00:00"
      }
    },
    {
      "id": "chapter2:ref-32:0",
      "chapter": "chapter2",
      "ref_num": 32,
      "bibtex_key": "sevilla2024training",
      "cite_label": "Sevilla and Roldan 2024",
      "claim_context": "Analysis of the largest AI firms reveals that pre-training their most capable models consumes less than 1% of quarterly compute budgets (see Table 2.2; methodology detailed in Appendices I and II). Yet these same firms continue expanding computational infrastructure to support larger models ([Mehta 2024]; [OpenAI 2024]; [Sevilla and Roldan 2024]), suggesting that remaining compute capacity is allocated to other training activities rather than final model production.",
      "section_id": "underutilized-compute",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms training compute grew 4-5x per year from 2010 to 2024 for frontier models, directly supporting the claim about firms continuing to expand computational infrastructure.",
        "checked_at": "2026-02-27T22:56:14.227188+00:00"
      }
    },
    {
      "id": "chapter2:ref-33:0",
      "chapter": "chapter2",
      "ref_num": 33,
      "bibtex_key": "weights2025sweeps",
      "cite_label": "Weights & Biases\n                            2025",
      "claim_context": "The contrast between frontier models consuming a small fraction of quarterly compute budgets and ongoing infrastructure expansion suggests that leading AI firms train numerous experimental models beyond their final deployments. This interpretation aligns with widely documented practices in frontier AI laboratories. Major labs employ hundreds to thousands of researchers who routinely train models during development. Standard optimization procedures, such as hyperparameter sweeps, involve training",
      "section_id": "underutilized-compute",
      "verification": {
        "status": "plausible",
        "reasoning": "The page redirected to W&B Integrations overview rather than the Sweeps tutorial; the title is relevant to hyperparameter sweeps in AI training but the specific content about sweep practices is not confirmed from available abstract.",
        "checked_at": "2026-02-27T22:56:14.227189+00:00"
      }
    },
    {
      "id": "chapter2:ref-34:0",
      "chapter": "chapter2",
      "ref_num": 34,
      "bibtex_key": "veldanda2024llm",
      "cite_label": "Veldanda et al., 2024",
      "claim_context": "This contrasts with modular systems where components can be incrementally optimized without discarding the entire structure. Recent work on targeted model modification (e.g., LLM surgery ([Veldanda et al., 2024])) suggests alternatives to full retraining, but such techniques have not been widely adopted in frontier model development, necessitating substantial computational expenditure during optimization.",
      "section_id": "underutilized-compute",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes LLM Surgery as a framework to modify LLMs without retraining from scratch, directly supporting the claim about targeted model modification as an alternative to full retraining.",
        "checked_at": "2026-02-27T22:56:14.227190+00:00"
      }
    },
    {
      "id": "chapter2:ref-35:0",
      "chapter": "chapter2",
      "ref_num": 35,
      "bibtex_key": "bratt2025inference",
      "cite_label": "Bratt 2025",
      "claim_context": "If frontier labs aim to produce general-purpose models, then computational resources allocated to intermediate experimental models represent overhead that does not directly contribute to final model capability. Based on Table 2.2, approximately 98.53% of annual training budgets are allocated to models other than the final deployment (calculated as 100% − (0.22%/15%), where 15% represents the estimated fraction of total compute dedicated to training ([Bratt 2025])). Under the objective of producing a generalpurpose model, this implies that the vast majority of training compute is allocated t...",
      "section_id": "underutilized-compute",
      "verification": {
        "status": "plausible",
        "reasoning": "Abstract mentions AI inference leading to efficient outcomes, topically relevant to the claim about training vs inference compute split, but too brief to confirm the specific 15% training fraction claim.",
        "checked_at": "2026-02-27T22:56:14.227191+00:00"
      }
    },
    {
      "id": "chapter2:ref-4:0",
      "chapter": "chapter2",
      "ref_num": 4,
      "bibtex_key": "borgeaud2022improving",
      "cite_label": "Borgeaud et al., 2022",
      "claim_context": "This annual estimate may understate total inefficiency, as it assumes frontier labs must train at least one model from scratch annually. If model parameters could be efficiently reused across generations (as RETRO/ATLAS demonstrate through their retrieval databases, where up to 98% of knowledge can be transferred between model versions ([Borgeaud et al., 2022]; [Izacard et al., 2023])) the efficiency gap would be larger. However, retraining overhead represents only one source of computational waste.",
      "section_id": "underutilized-compute",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms RETRO achieves comparable performance to GPT-3 using 25x fewer parameters through retrieval databases, directly supporting the claim about knowledge transfer between model versions via retrieval.",
        "checked_at": "2026-02-27T22:56:14.227192+00:00"
      }
    },
    {
      "id": "chapter2:ref-5:1",
      "chapter": "chapter2",
      "ref_num": 5,
      "bibtex_key": "izacard2023atlas",
      "cite_label": "Izacard\n                            et al., 2023",
      "claim_context": "This annual estimate may understate total inefficiency, as it assumes frontier labs must train at least one model from scratch annually. If model parameters could be efficiently reused across generations (as RETRO/ATLAS demonstrate through their retrieval databases, where up to 98% of knowledge can be transferred between model versions ([Borgeaud et al., 2022]; [Izacard et al., 2023])) the efficiency gap would be larger. However, retraining overhead represents only one source of computational wa",
      "section_id": "underutilized-compute",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms Atlas achieves comparable performance with 50x fewer parameters through retrieval, supporting the claim about efficient knowledge transfer through retrieval databases.",
        "checked_at": "2026-02-27T22:56:14.227196+00:00"
      }
    },
    {
      "id": "chapter2:ref-36:0",
      "chapter": "chapter2",
      "ref_num": 36,
      "bibtex_key": "doi:10.1073/pnas.1611835114",
      "cite_label": "Kirkpatrick et al., 2017",
      "claim_context": "To add the first training example into an untrained model like GPT-3, AI users forward propagate through the entire model and all of its knowledge and store that information in random locations throughout the model. Then, as more training examples pile into the model, the model experiences catastrophic forgetting ([Kirkpatrick et al., 2017]) as collisions occur. And to train an entire model like GPT-3 on modern training corpuses (i.e. trillions of tokens), it repeats this process trillions of times.",
      "section_id": "underutilized-compute",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Overcoming catastrophic forgetting in neural networks' directly relates to the catastrophic forgetting claim, but source inaccessible (HTTP 403) with no abstract available.",
        "checked_at": "2026-02-27T22:56:14.227197+00:00"
      }
    },
    {
      "id": "chapter2:ref-8:0",
      "chapter": "chapter2",
      "ref_num": 8,
      "bibtex_key": "kemker2018measuring",
      "cite_label": "Kemker et al., 2018",
      "claim_context": "RETRO and ATLAS provide partial evidence, as does work on curriculum learning and knowledge distillation. Kemker et al. observe that avoiding catastrophic forgetting requires approximately 40x larger model capacity ([Kemker et al., 2018]), though this estimate is not especially recent. Multiple sources of training inefficiency compound: full model retraining during updates, dense forward propagation through all parameters, parameter redundancy from insufficient compression, and capacity overhead to prevent catastrophic forgetting during sequential training.",
      "section_id": "underutilized-compute",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms that neural networks are prone to catastrophically forgetting previous tasks and that the problem has yet to be solved, supporting the claim about catastrophic forgetting requiring larger model capacity.",
        "checked_at": "2026-02-27T22:56:14.227198+00:00"
      }
    },
    {
      "id": "chapter2:ref-3:1",
      "chapter": "chapter2",
      "ref_num": 3,
      "bibtex_key": "robison2024openai",
      "cite_label": "Robison 2024",
      "claim_context": "Following growing rumors across the AI research community that data is becoming a major bottleneck, OpenAI’s former chief scientist, Ilya Sutskever, announced during his test of time award speech at NeurIPS 2024 that data for training AI has peaked, ”We’ve achieved peak data and there’ll be no more” ([Robison 2024]). However, while this may be true for the AI industry, and Ilya (being recent Chief Scientist at OpenAI) is perhaps one of the best people in the world to know, Ilya’s statement does not reflect the reality of what data exists in the world.",
      "section_id": "siloed-data",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract quotes 'We've achieved peak data and there'll be no more,' directly supporting the claim that Sutskever announced at NeurIPS 2024 that data for training AI has peaked.",
        "checked_at": "2026-02-27T22:56:14.227199+00:00"
      }
    },
    {
      "id": "chapter2:ref-37:0",
      "chapter": "chapter2",
      "ref_num": 37,
      "bibtex_key": "epoch2024hardware",
      "cite_label": "Epoch AI 2024",
      "claim_context": "This mirrors the current state of AI training. When frontier models like GPT-4 (trained on 6.5 trillion tokens), and Qwen2.5-72B (18 trillion tokens), LLama 4 (30 trillion tokens), ([Epoch AI 2024]) report hitting data limits, they’re really hitting access limits. They’re not running out of data, they’re running out of data they can freely collect.",
      "section_id": "siloed-data",
      "verification": {
        "status": "plausible",
        "reasoning": "The reference is about ML hardware data (GPUs/TPUs), not directly about model training token counts; the claim cites it for frontier model training data sizes which would more likely come from the AI Models database, not the hardware database.",
        "checked_at": "2026-02-27T22:56:14.227200+00:00"
      }
    },
    {
      "id": "chapter2:ref-29:1",
      "chapter": "chapter2",
      "ref_num": 29,
      "bibtex_key": "EpochAIModels2025",
      "cite_label": "Epoch AI 2025",
      "claim_context": "GPT-4 was trained on approximately 6.5 trillion tokens, while Alibaba’s Qwen2.5-72B used 18 trillion tokens. The largest reported text dataset, used for Meta’s Llama 4, contains 30 trillion tokens ([Epoch AI 2025]). Using RedPajama as a reference ([Together 2023]), each trillion tokens requires less than 6TB of storage, implying that the largest known training dataset (Llama 4) occupies less than 180TB.",
      "section_id": "siloed-data",
      "verification": {
        "status": "plausible",
        "reasoning": "Database tracking 3200+ ML models is the right type of source for training data statistics like token counts, but the abstract does not specifically confirm the GPT-4/Qwen/Llama token count figures cited.",
        "checked_at": "2026-02-27T22:56:14.227202+00:00"
      }
    },
    {
      "id": "chapter2:ref-38:0",
      "chapter": "chapter2",
      "ref_num": 38,
      "bibtex_key": "together2023redpajama",
      "cite_label": "Together 2023",
      "claim_context": "The largest reported text dataset, used for Meta’s Llama 4, contains 30 trillion tokens ([Epoch AI 2025]). Using RedPajama as a reference ([Together 2023]), each trillion tokens requires less than 6TB of storage, implying that the largest known training dataset (Llama 4) occupies less than 180TB.",
      "section_id": "siloed-data",
      "verification": {
        "status": "supported",
        "reasoning": "Page title 'RedPajama-Data-v2: An open dataset with 30 trillion tokens' directly supports using RedPajama as a reference for tokens-to-storage conversion calculations.",
        "checked_at": "2026-02-27T22:56:14.227203+00:00"
      }
    },
    {
      "id": "chapter2:ref-9:0",
      "chapter": "chapter2",
      "ref_num": 9,
      "bibtex_key": "educating_silicon_2024",
      "cite_label": "Cummins 2024",
      "claim_context": "The scale of untapped data is staggering. As shown in Tables 2.5 and 2.6, stored email and instant messages alone contain over 1,850 trillion tokens, approximately 60 times the largest known training dataset ([Cummins 2024]). Daily human communication generates approximately 150 trillion tokens, accumulating to roughly 55 quadrillion tokens annually (approximately 1,750 times the scale of frontier training sets).",
      "section_id": "siloed-data",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'How much LLM training data is there, in the limit?' is directly relevant to the claim about untapped data scale and token estimates, but no abstract available to confirm specific figures.",
        "checked_at": "2026-02-27T22:56:14.227204+00:00"
      }
    },
    {
      "id": "chapter2:ref-40:0",
      "chapter": "chapter2",
      "ref_num": 40,
      "bibtex_key": "wikipedia_commoncrawl_2024",
      "cite_label": "Wikipedia contributors 2024",
      "claim_context": "Yet even this vast sea of text represents merely a drop in the ocean of total digital data. While frontier AI models train on curated web scrapes such as Common Crawl (454 TB as of December 2023) ([Wikipedia contributors 2024]), the Internet Archive’s Wayback Machine alone stores approximately 100 petabytes ([Kahle 2024]). Meanwhile, global digital data is projected to reach 180 zettabytes by 2025 ([Mider 2024]; [Taylor 2024]), six orders of magnitude larger than The Internet Archive and nine orders of magnitude larger than the largest known training datasets.",
      "section_id": "siloed-data",
      "verification": {
        "status": "plausible",
        "reasoning": "Wikipedia page on Common Crawl is the right source for Common Crawl data size claims, but no abstract content available to confirm the specific 454 TB figure.",
        "checked_at": "2026-02-27T22:56:14.227205+00:00"
      }
    },
    {
      "id": "chapter2:ref-41:0",
      "chapter": "chapter2",
      "ref_num": 41,
      "bibtex_key": "internet_archive_donation",
      "cite_label": "Kahle 2024",
      "claim_context": "Yet even this vast sea of text represents merely a drop in the ocean of total digital data. While frontier AI models train on curated web scrapes such as Common Crawl (454 TB as of December 2023) ([Wikipedia contributors 2024]), the Internet Archive’s Wayback Machine alone stores approximately 100 petabytes ([Kahle 2024]). Meanwhile, global digital data is projected to reach 180 zettabytes by 2025 ([Mider 2024]; [Taylor 2024]), six orders of magnitude larger than The Internet Archive and nine orders of magnitude larger than the largest known training datasets.",
      "section_id": "siloed-data",
      "verification": {
        "status": "plausible",
        "reasoning": "Page title confirms Internet Archive site, relevant to the claim about Wayback Machine storing 100 petabytes, but no abstract content available to confirm the specific storage figure.",
        "checked_at": "2026-02-27T22:56:14.227206+00:00"
      }
    },
    {
      "id": "chapter2:ref-42:0",
      "chapter": "chapter2",
      "ref_num": 42,
      "bibtex_key": "mider_osint_2024",
      "cite_label": "Mider 2024",
      "claim_context": "While frontier AI models train on curated web scrapes such as Common Crawl (454 TB as of December 2023) ([Wikipedia contributors 2024]), the Internet Archive’s Wayback Machine alone stores approximately 100 petabytes ([Kahle 2024]). Meanwhile, global digital data is projected to reach 180 zettabytes by 2025 ([Mider 2024]; [Taylor 2024]), six orders of magnitude larger than The Internet Archive and nine orders of magnitude larger than the largest known training datasets.",
      "section_id": "siloed-data",
      "verification": {
        "status": "plausible",
        "reasoning": "Title about open source intelligence on the internet is tangentially relevant to global digital data projections, but the abstract does not confirm the specific 180 zettabytes claim.",
        "checked_at": "2026-02-27T22:56:14.227208+00:00"
      }
    },
    {
      "id": "chapter2:ref-43:0",
      "chapter": "chapter2",
      "ref_num": 43,
      "bibtex_key": "taylor2024data",
      "cite_label": "Taylor 2024",
      "claim_context": "While frontier AI models train on curated web scrapes such as Common Crawl (454 TB as of December 2023) ([Wikipedia contributors 2024]), the Internet Archive’s Wayback Machine alone stores approximately 100 petabytes ([Kahle 2024]). Meanwhile, global digital data is projected to reach 180 zettabytes by 2025 ([Mider 2024]; [Taylor 2024]), six orders of magnitude larger than The Internet Archive and nine orders of magnitude larger than the largest known training datasets.",
      "section_id": "siloed-data",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract states 'The total amount of data created, captured, copied, and consumed globally is forecast to increase rapidly,' and the Statista page title references data growth projections, supporting the 180 zettabytes global data claim.",
        "checked_at": "2026-02-27T22:56:14.227209+00:00"
      }
    },
    {
      "id": "chapter2:ref-4:1",
      "chapter": "chapter2",
      "ref_num": 4,
      "bibtex_key": "borgeaud2022improving",
      "cite_label": "Borgeaud\n                            et al., 2022",
      "claim_context": "RETRO and ATLAS demonstrate the minimum scale of such a breakthrough. By maintaining source-based partitions through their database architecture, they achieve equal performance while activating only 2-4% of the parameters of similarly performant dense models ([Borgeaud et al., 2022]; [Izacard et al., 2023]).",
      "section_id": "first-why",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms RETRO achieves comparable performance to GPT-3 using 25x fewer parameters through retrieval, directly supporting the claim about source-based partitions enabling 2-4% parameter activation.",
        "checked_at": "2026-02-27T22:56:14.227211+00:00"
      }
    },
    {
      "id": "chapter2:ref-5:2",
      "chapter": "chapter2",
      "ref_num": 5,
      "bibtex_key": "izacard2023atlas",
      "cite_label": "Izacard et al., 2023",
      "claim_context": "RETRO and ATLAS demonstrate the minimum scale of such a breakthrough. By maintaining source-based partitions through their database architecture, they achieve equal performance while activating only 2-4% of the parameters of similarly performant dense models ([Borgeaud et al., 2022]; [Izacard et al., 2023]).",
      "section_id": "first-why",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms Atlas outperforms a 540B model with 50x fewer parameters, directly supporting the claim about achieving equal performance while activating only 2-4% of parameters.",
        "checked_at": "2026-02-27T22:56:14.227219+00:00"
      }
    },
    {
      "id": "chapter2:ref-8:1",
      "chapter": "chapter2",
      "ref_num": 8,
      "bibtex_key": "kemker2018measuring",
      "cite_label": "Kemker et\n                            al., 2018",
      "claim_context": "Recall that current AI models must retrain entirely from scratch when updating their knowledge, because of problems such as catastrophic forgetting ([Kemker et al., 2018]). While various approaches to incremental training exist, successful ABC would necessitate one particular solution: source-partitioned retraining. Consequently, if an AI model can train source-separated subsections of its weights, it can re-train them as well, and it is very likely to also greatly reduce the computational compl",
      "section_id": "first-why",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms neural networks are prone to catastrophically forgetting previous tasks and must retrain from scratch, directly supporting the claim about models needing full retraining due to catastrophic forgetting.",
        "checked_at": "2026-02-27T22:56:14.227221+00:00"
      }
    },
    {
      "id": "chapter2:ref-44:0",
      "chapter": "chapter2",
      "ref_num": 44,
      "bibtex_key": "bogwasi2025business",
      "cite_label": "Bogwasi 2025",
      "claim_context": "Recall that current AI models can only train on data they can access, which is dominated by data available to the public over the internet. Consequently, AI models almost certainly train on less than 1/1,000,000th of the digitized information in the world because they cannot access the other 99.9999%, which remains hidden amongst the world’s 360 million companies, 8+ billion citizens, etc. ([Bogwasi 2025]).",
      "section_id": "first-why",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Business Facts: Essential Business Statistics' is relevant to the claim about 360 million companies worldwide, but abstract is too generic to confirm specific figures.",
        "checked_at": "2026-02-27T22:56:14.227223+00:00"
      }
    },
    {
      "id": "chapter2:ref-7:1",
      "chapter": "chapter2",
      "ref_num": 7,
      "bibtex_key": "Goodfellow-et-al-2016",
      "cite_label": "Goodfellow et al.,\n                            2016",
      "claim_context": "The previous section revealed how solving attribution-based control would necessarily enable massive data and compute gains in AI systems. Yet this raises a deeper question: why do current AI systems fail to maintain attribution in the first place? The answer lies in deep learning’s foundational premise: algorithms should learn everything from scratch through layers of (largely) unrestricted feature mixing on raw data ([Goodfellow et al., 2016]).",
      "section_id": "second-why",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Deep Learning' by Goodfellow et al. is the foundational textbook covering deep learning's premise of learning from scratch through feature mixing, but no abstract available to confirm the specific claim.",
        "checked_at": "2026-02-27T22:56:14.227224+00:00"
      }
    },
    {
      "id": "chapter2:ref-10:0",
      "chapter": "chapter2",
      "ref_num": 10,
      "bibtex_key": "le2013building",
      "cite_label": "Le\n                            2013",
      "claim_context": "Consider what happens when a neural network learns to recognize cats. Rather than storing clear, interpretable features like ”pointy ears” or ”whiskers”, the network distributes this knowledge across its weights in complex, entangled patterns ([Le 2013]). This unrestricted mixing of features makes post-hoc source-based partitioning impossible (at the present moment):",
      "section_id": "second-why",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes training a network that learns to detect faces/cats through distributed unsupervised feature learning, directly supporting the claim about networks distributing knowledge in complex entangled patterns rather than interpretable features.",
        "checked_at": "2026-02-27T22:56:14.227225+00:00"
      }
    },
    {
      "id": "chapter2:ref-19:0",
      "chapter": "chapter2",
      "ref_num": 19,
      "bibtex_key": "nguyen2024surveymachineunlearning",
      "cite_label": "Nguyen et al., 2024",
      "claim_context": "The research community has made extensive efforts to address these limitations. Recent work has attempted to trace predictions back to training data through influence functions, remove specific datapoints’ influence through machine unlearning, and develop various attribution methods to reverse engineer the source-prediction relationship ([Nguyen et al., 2024]). Yet despite these attempts, both influence functions and unlearning remain unsolved challenges in the literature.",
      "section_id": "second-why",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms machine unlearning as an unsolved challenge, discussing influence functions and attempts to remove data from ML models that have not been able to completely solve the problem, directly supporting the claim.",
        "checked_at": "2026-02-27T22:56:14.227226+00:00"
      }
    },
    {
      "id": "chapter2:ref-45:0",
      "chapter": "chapter2",
      "ref_num": 45,
      "bibtex_key": "hochreiter1998vanishing",
      "cite_label": "Hochreiter 1998",
      "claim_context": "First, in models like GPT-3, through forward and backward propagation, each example’s information eventually touches every weight in the network. Even tiny weight changes alter how future examples flow through the network, creating cascading effects that can amplify initially small influences (related: vanishing and exploding gradients ([Hochreiter 1998]; [Hanin 2018])).",
      "section_id": "third-why",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'The vanishing gradient problem during learning recurrent neural nets' directly relates to the claim about vanishing/exploding gradients, but source inaccessible (HTTP 403) with no abstract available.",
        "checked_at": "2026-02-27T22:56:14.227227+00:00"
      }
    },
    {
      "id": "chapter2:ref-46:0",
      "chapter": "chapter2",
      "ref_num": 46,
      "bibtex_key": "hanin2018neural",
      "cite_label": "Hanin 2018",
      "claim_context": "First, in models like GPT-3, through forward and backward propagation, each example’s information eventually touches every weight in the network. Even tiny weight changes alter how future examples flow through the network, creating cascading effects that can amplify initially small influences (related: vanishing and exploding gradients ([Hochreiter 1998]; [Hanin 2018])).",
      "section_id": "third-why",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract analyzes gradient behavior showing empirical variance is exponential in architecture-dependent constants and gradients can 'vary wildly,' directly supporting the claim about cascading effects and exploding gradients.",
        "checked_at": "2026-02-27T22:56:14.227229+00:00"
      }
    },
    {
      "id": "chapter2:ref-11:0",
      "chapter": "chapter2",
      "ref_num": 11,
      "bibtex_key": "krizhevsky2012imagenet",
      "cite_label": "Krizhevsky et al., 2012",
      "claim_context": "Deep learning’s central hypothesis would suggest we can’t, that features need to densely mix (using addition) in order to learn the powerful correlations and representations that give deep learning its predictive capability. However, presumably deep learning maps multiple distinct concepts into a shared feature when those two concepts are related ([Krizhevsky et al., 2012]). For example, a deep learning model which classifies images might have features which detect ears, fur, and eyes — features which would be useful for modeling many different animals which possess these related concepts (...",
      "section_id": "third-hypothesis",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'ImageNet Classification with Deep Convolutional Neural Networks' is relevant to deep learning's feature mixing hypothesis, but no abstract available to confirm the specific claim about dense feature mixing.",
        "checked_at": "2026-02-27T22:56:14.227230+00:00"
      }
    },
    {
      "id": "chapter2:ref-12:0",
      "chapter": "chapter2",
      "ref_num": 12,
      "bibtex_key": "zeiler2014visualizing",
      "cite_label": "Zeiler and Fergus 2014",
      "claim_context": "However, presumably deep learning maps multiple distinct concepts into a shared feature when those two concepts are related ([Krizhevsky et al., 2012]). For example, a deep learning model which classifies images might have features which detect ears, fur, and eyes — features which would be useful for modeling many different animals which possess these related concepts ([Zeiler and Fergus 2014]). However, as these features are not laid out in advance, deep learning needs to densely mix its features in order to discover these related patterns across training datapoints.",
      "section_id": "third-hypothesis",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes visualization of intermediate feature layers showing how deep networks learn shared features across concepts, supporting the claim that deep learning maps distinct concepts into shared features.",
        "checked_at": "2026-02-27T22:56:14.227231+00:00"
      }
    },
    {
      "id": "chapter2:ref-13:0",
      "chapter": "chapter2",
      "ref_num": 13,
      "bibtex_key": "chomsky2014aspects",
      "cite_label": "Chomsky\n                            2014",
      "claim_context": "In contrast, perhaps most information is encyclopedic and appears sparsely: specific facts about the world, domain expertise in particular fields, claims made by individual sources, etc. The capital of France, the rules of chess, statistics about pizza... each appears in distinct contexts with limited overlap. As Chomsky noted in linguistics ([Chomsky 2014]), while we use common patterns to express all knowledge, the knowledge itself often remains naturally partitioned by topic, and when documen",
      "section_id": "third-hypothesis",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Aspects of the Theory of Syntax' by Chomsky is the right source for linguistic theory about common patterns expressing knowledge, but source inaccessible (HTTP 403) and it is a book with no abstract.",
        "checked_at": "2026-02-27T22:56:14.227232+00:00"
      }
    },
    {
      "id": "chapter2:ref-14:0",
      "chapter": "chapter2",
      "ref_num": 14,
      "bibtex_key": "dwork2006calibrating",
      "cite_label": "Dwork et al., 2006",
      "claim_context": "A key insight of this chapter is that techniques from privacy-preserving machine learning, particularly differential privacy (DP) ([Dwork et al., 2006]), provide a principled way to measure and control which features benefit from dense mixing versus sparse representation. Differential privacy quantifies how much a model’s outputs depend on any individual training example:",
      "section_id": "third-hypothesis",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms research on privacy-preserving databases with noise calibration to sensitivity, directly supporting the claim about differential privacy as a technique for measuring output dependence on individual training examples.",
        "checked_at": "2026-02-27T22:56:14.227233+00:00"
      }
    },
    {
      "id": "chapter2:ref-54:0",
      "chapter": "chapter2",
      "ref_num": 54,
      "bibtex_key": "Abadi_2016",
      "cite_label": "Abadi\n                            et al., 2016",
      "claim_context": "Given differential privacy’s use in deep learning (e.g., ([Abadi et al., 2016])), this framework suggests an architectural insight: a neural network serving diverse stakeholders requires the capability to enforce different ϵ regimes for different information. Information requiring privacy (small ϵ) can pass through privacy-constrained layers with dense mixing via addition. Information requiring attribution (large ϵ) must route through pathways preserving source identity via sparse concatenation.",
      "section_id": "third-hypothesis",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Deep Learning with Differential Privacy' directly relates to the claim about differential privacy's use in deep learning, but source inaccessible (HTTP 403) with no abstract available.",
        "checked_at": "2026-02-27T22:56:14.227235+00:00"
      }
    },
    {
      "id": "chapter2:ref-47:0",
      "chapter": "chapter2",
      "ref_num": 47,
      "bibtex_key": "idp",
      "cite_label": "Soria-Comas et al., 2016",
      "claim_context": "Worst-case differential privacy must constrain the entire mechanism based on that one outlier, reducing utility for everyone—even though the mechanism only ever operates on the actual dataset, not all possible datasets. Individual differential privacy ([Soria-Comas et al., 2016]) provides a more nuanced approach by focusing privacy guarantees on the actual dataset rather than all possible datasets:",
      "section_id": "second-hypothesis",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract proposes individual differential privacy as an alternative that focuses guarantees on the actual dataset rather than all possible datasets, directly supporting the claim about a more nuanced approach to privacy.",
        "checked_at": "2026-02-27T22:56:14.227236+00:00"
      }
    },
    {
      "id": "chapter2:ref-48:0",
      "chapter": "chapter2",
      "ref_num": 48,
      "bibtex_key": "dwork2014algorithmic",
      "cite_label": "Dwork et al., 2014",
      "claim_context": "ABC needs to measure and control influence at the hospital level, not just the individual patient level. The differential privacy literature addresses this through group differential privacy ([Dwork et al., 2014]), which extends privacy guarantees from individuals to groups of records:",
      "section_id": "second-hypothesis",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'The algorithmic foundations of differential privacy' is the standard reference for group differential privacy concepts, but source inaccessible (HTTP 403) with no abstract available.",
        "checked_at": "2026-02-27T22:56:14.227237+00:00"
      }
    },
    {
      "id": "chapter2:ref-15:0",
      "chapter": "chapter2",
      "ref_num": 15,
      "bibtex_key": "feldman2020individual",
      "cite_label": "Feldman and Zrnic 2020",
      "claim_context": "Feldman and Zrnic’s individual differential privacy framework ([Feldman and Zrnic 2020]) provides a concrete example of intelligence budgets implemented through adaptive filtering rather than architectural routing.",
      "section_id": "second-hypothesis",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes individual privacy accounting via personalized privacy loss estimates with adaptive filtering using a Renyi filter, directly supporting the claim about intelligence budgets through adaptive filtering.",
        "checked_at": "2026-02-27T22:56:14.227238+00:00"
      }
    },
    {
      "id": "chapter2:ref-17:0",
      "chapter": "chapter2",
      "ref_num": 17,
      "bibtex_key": "zhao2018federated",
      "cite_label": "Zhao\n                            et al., 2018",
      "claim_context": "across clients, MNIST accuracy drops from 98.69% to 96.29% ([Zhao et al., 2018])—a 2.4% degradation relative to joint training. This reflects a tension between attribution boundaries and cross-source pattern learning.",
      "section_id": "empirical-evidence",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms federated learning accuracy reduces significantly with non-IID data (up to 55% for highly skewed data), supporting the claim about accuracy degradation when maintaining attribution boundaries across clients.",
        "checked_at": "2026-02-27T22:56:14.227239+00:00"
      }
    },
    {
      "id": "chapter2:ref-51:0",
      "chapter": "chapter2",
      "ref_num": 51,
      "bibtex_key": "grover2018mnist",
      "cite_label": "Grover and Toghi 2018",
      "claim_context": "Pure memory-based approaches (λ = 0,γ = 0) such as k-NN provide perfect attribution by directly linking predictions to source examples. These systems achieve 97.2% accuracy on MNIST ([Grover and Toghi 2018]) but cannot generalize beyond patterns explicitly present in their memory banks.",
      "section_id": "empirical-evidence",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes k-NN classification on MNIST achieving improved accuracy through a modified distance metric, directly supporting the claim about k-NN achieving high accuracy on MNIST as a memory-based approach.",
        "checked_at": "2026-02-27T22:56:14.227241+00:00"
      }
    },
    {
      "id": "chapter2:ref-5:3",
      "chapter": "chapter2",
      "ref_num": 5,
      "bibtex_key": "izacard2023atlas",
      "cite_label": "Izacard et al., 2023",
      "claim_context": "ATLAS demonstrates similar gains: 25-50x parameter efficiency improvements while maintaining or exceeding baseline performance ([Izacard et al., 2023]). Both systems achieve these results through a fundamental architectural shift: rather than compressing all knowledge into dense parameters, they maintain explicit connections to source documents through retrieval.",
      "section_id": "empirical-evidence",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms Atlas outperforms a 540B model with 50x fewer parameters through retrieval, directly supporting the claim about 25-50x parameter efficiency improvements.",
        "checked_at": "2026-02-27T22:56:14.227242+00:00"
      }
    },
    {
      "id": "chapter2:ref-16:0",
      "chapter": "chapter2",
      "ref_num": 16,
      "bibtex_key": "papernot2018scalableprivatelearningpate",
      "cite_label": "Papernot et al., 2018",
      "claim_context": "Traditional privacy-preserving approaches incur 10-20% performance degradation. PATE reduces this gap to 0.7% on MNIST (98.5% accuracy versus 99.2% non-private baseline) while maintaining differential privacy guarantees through source-separated teacher ensembles ([Papernot et al., 2018]).",
      "section_id": "empirical-evidence",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes PATE achieving high utility with strong privacy (epsilon < 1.0) through source-separated teacher ensembles with noisy aggregation, directly supporting the claim about PATE's performance on MNIST with differential privacy.",
        "checked_at": "2026-02-27T22:56:14.227243+00:00"
      }
    },
    {
      "id": "chapter2:ref-52:0",
      "chapter": "chapter2",
      "ref_num": 52,
      "bibtex_key": "10.36227/techrxiv.171837853.31531482/v1",
      "cite_label": "Hou\n                            and Wang 2024",
      "claim_context": "Federated RAG systems demonstrate concurrent improvements in attribution and performance. Recent work shows that federated RAG improves both attribution clarity and model accuracy simultaneously (Table 2.8) ([Hou and Wang 2024]).",
      "section_id": "empirical-evidence",
      "verification": {
        "status": "mismatch",
        "reasoning": "The URL resolves to 'The Black Box Society' by Frank Pasquale (Harvard University Press), a book about corporate data practices, not a paper about federated retrieval-augmented generation as cited.",
        "checked_at": "2026-02-27T22:56:14.227244+00:00"
      }
    },
    {
      "id": "chapter2:ref-18:0",
      "chapter": "chapter2",
      "ref_num": 18,
      "bibtex_key": "ainsworth2022git",
      "cite_label": "Ainsworth\n                            et al., 2022",
      "claim_context": "Git Re-Basin demonstrates that independently trained models can be merged with minimal performance loss through weight permutation alignment ([Ainsworth et al., 2022]). This extends previous model merging results ([Zhao et al., 2018]) by enabling merging across models trained on separate dataset partitions from similar distributions. The technique identifies and corrects for arbitrary permutations of hidden layer neurons that occur during independent training, effectively aligning equivalent fea",
      "section_id": "empirical-evidence",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes algorithms to merge independently trained models through weight permutation alignment with minimal performance loss, directly supporting the claim about Git Re-Basin merging models via permutation alignment.",
        "checked_at": "2026-02-27T22:56:14.227245+00:00"
      }
    },
    {
      "id": "chapter2:ref-17:1",
      "chapter": "chapter2",
      "ref_num": 17,
      "bibtex_key": "zhao2018federated",
      "cite_label": "Zhao et al., 2018",
      "claim_context": "Git Re-Basin demonstrates that independently trained models can be merged with minimal performance loss through weight permutation alignment ([Ainsworth et al., 2022]). This extends previous model merging results ([Zhao et al., 2018]) by enabling merging across models trained on separate dataset partitions from similar distributions. The technique identifies and corrects for arbitrary permutations of hidden layer neurons that occur during independent training, effectively aligning equivalent features across models before merging.",
      "section_id": "empirical-evidence",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes federated learning with model aggregation across clients, supporting the claim about extending model merging results across models trained on separate dataset partitions.",
        "checked_at": "2026-02-27T22:56:14.227247+00:00"
      }
    },
    {
      "id": "chapter2:ref-53:0",
      "chapter": "chapter2",
      "ref_num": 53,
      "bibtex_key": "privacy_blocks_medical_sharing",
      "cite_label": "Youssef et al., 2023",
      "claim_context": "Global digital data reaches 180 zettabytes (6-9 orders of magnitude larger). Vast institutional repositories remain inaccessible primarily due to attribution and control concerns ([Youssef et al., 2023]). Deep voting’s source-partitioned architecture provides the attribution mechanism these institutions require, establishing a viable technical path to unlock this siloed data",
      "section_id": "empirical-evidence",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Organizational Factors in Clinical Data Sharing for AI in Health Care' is directly relevant to institutional repositories being inaccessible due to control concerns, but source inaccessible (HTTP 403) with no abstract available.",
        "checked_at": "2026-02-27T22:56:14.227248+00:00"
      }
    },
    {
      "id": "chapter3:ref-1:0",
      "chapter": "chapter3",
      "ref_num": 1,
      "bibtex_key": "st",
      "cite_label": "Trask et al., 2020",
      "claim_context": "This suggests a direction for a solution: to reduce the copying of information within AI systems, averting unilateral control. Building upon this diagnosis, the chapter calls upon the framework of structured transparency ([Trask et al., 2020]) to avert the copy problem, revealing a new alternative to open-source and closed-source AI: network-source AI — and a viable path towards true attribution-based control in AI systems.",
      "section_id": "chapter-summary",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract explicitly discusses 'the copy problem' and proposes the 'structured transparency' framework, directly matching the claim about using structured transparency to avert the copy problem.",
        "checked_at": "2026-02-27T22:56:14.278940+00:00"
      }
    },
    {
      "id": "chapter3:ref-2:0",
      "chapter": "chapter3",
      "ref_num": 2,
      "bibtex_key": "privacy_blocks_medical_sharing",
      "cite_label": "Youssef et al., 2023",
      "claim_context": "This creates a trust barrier that blocks deep voting’s potential. Medical institutions, for example, cite exactly these control and attribution concerns as primary barriers to sharing their vast repositories of valuable data ([Youssef et al., 2023]). Without a way to technically enforce how their information will be used and attributed, they cannot safely contribute to AI training.",
      "section_id": "problem",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Organizational Factors in Clinical Data Sharing for AI in Health Care' is directly relevant to medical institutions' barriers to data sharing, but no abstract available to confirm specific control/attribution concerns (HTTP 403).",
        "checked_at": "2026-02-27T22:56:14.279111+00:00"
      }
    },
    {
      "id": "chapter3:ref-18:0",
      "chapter": "chapter3",
      "ref_num": 18,
      "bibtex_key": "fecher_2015_what",
      "cite_label": "Fecher et al., 2015",
      "claim_context": "Without a way to technically enforce how their information will be used and attributed, they cannot safely contribute to AI training. This pattern repeats across domains, from scientific research ([Fecher et al., 2015]; [Ascoli 2015]) to financial data ([Sienkiewicz 2025]) to government records ([Scott and Gong 2021]); vast stores of valuable information remain siloed because we lack mechanisms to ensure attribution-based control, and the corresponding incentives for collaboration that ABC would bring to AI systems",
      "section_id": "problem",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms that despite support from policy makers, academic researchers rarely share data, and identifies barriers to data sharing in scientific research, directly supporting the claim about siloed research data.",
        "checked_at": "2026-02-27T22:56:14.279113+00:00"
      }
    },
    {
      "id": "chapter3:ref-19:0",
      "chapter": "chapter3",
      "ref_num": 19,
      "bibtex_key": "ascoli2015sharing",
      "cite_label": "Ascoli 2015",
      "claim_context": "Without a way to technically enforce how their information will be used and attributed, they cannot safely contribute to AI training. This pattern repeats across domains, from scientific research ([Fecher et al., 2015]; [Ascoli 2015]) to financial data ([Sienkiewicz 2025]) to government records ([Scott and Gong 2021]); vast stores of valuable information remain siloed because we lack mechanisms to ensure attribution-based control, and the corresponding incentives for collaboration that ABC would bring to AI systems",
      "section_id": "problem",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms neuroscience data is 'still rarely shared' and discusses barriers to sharing, supporting the claim that scientific research data remains siloed.",
        "checked_at": "2026-02-27T22:56:14.279115+00:00"
      }
    },
    {
      "id": "chapter3:ref-20:0",
      "chapter": "chapter3",
      "ref_num": 20,
      "bibtex_key": "sienkiewicz2025data",
      "cite_label": "Sienkiewicz 2025",
      "claim_context": "Without a way to technically enforce how their information will be used and attributed, they cannot safely contribute to AI training. This pattern repeats across domains, from scientific research ([Fecher et al., 2015]; [Ascoli 2015]) to financial data ([Sienkiewicz 2025]) to government records ([Scott and Gong 2021]); vast stores of valuable information remain siloed because we lack mechanisms to ensure attribution-based control, and the corresponding incentives for collaboration that ABC would bring to AI systems",
      "section_id": "problem",
      "verification": {
        "status": "mismatch",
        "reasoning": "Abstract describes overstock issues in a pet retail Purchase-to-Pay process, not financial data silos or data architecture; the retrieved abstract appears to be from a different paper than the cited title about data mesh in financial data architecture.",
        "checked_at": "2026-02-27T22:56:14.279116+00:00"
      }
    },
    {
      "id": "chapter3:ref-21:0",
      "chapter": "chapter3",
      "ref_num": 21,
      "bibtex_key": "scott2021coordinating",
      "cite_label": "Scott and Gong 2021",
      "claim_context": "Without a way to technically enforce how their information will be used and attributed, they cannot safely contribute to AI training. This pattern repeats across domains, from scientific research ([Fecher et al., 2015]; [Ascoli 2015]) to financial data ([Sienkiewicz 2025]) to government records ([Scott and Gong 2021]); vast stores of valuable information remain siloed because we lack mechanisms to ensure attribution-based control, and the corresponding incentives for collaboration that ABC would bring to AI systems",
      "section_id": "problem",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses silos in government and challenges of horizontal coordination between them, directly supporting the claim about government records remaining siloed.",
        "checked_at": "2026-02-27T22:56:14.279117+00:00"
      }
    },
    {
      "id": "chapter3:ref-22:0",
      "chapter": "chapter3",
      "ref_num": 22,
      "bibtex_key": "vidal2024compelling",
      "cite_label": "Vidal\n                            2024",
      "claim_context": "The debate between open and closed source AI crystallizes how society grapples with fundamental questions of control over artificial intelligence ([Vidal 2024]). This debate has become a proxy for broader concerns about privacy, disinformation, copyright, safety, bias, and alignment ([National Telecommunications and Information Administration 2024]). Yet when examined through the lens of attribution-based control (ABC), both approaches fundamentally fail to address these concerns, though they fa",
      "section_id": "second-why",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms OSI compiled responses to NTIA's AI Open Model Weights RFC, supporting the claim about the debate over open vs closed source AI and fundamental questions of control.",
        "checked_at": "2026-02-27T22:56:14.279118+00:00"
      }
    },
    {
      "id": "chapter3:ref-23:0",
      "chapter": "chapter3",
      "ref_num": 23,
      "bibtex_key": "ntia2024aiweights",
      "cite_label": "National Telecommunications and Information Administration\n                            2024",
      "claim_context": "The debate between open and closed source AI crystallizes how society grapples with fundamental questions of control over artificial intelligence ([Vidal 2024]). This debate has become a proxy for broader concerns about privacy, disinformation, copyright, safety, bias, and alignment ([National Telecommunications and Information Administration 2024]). Yet when examined through the lens of attribution-based control (ABC), both approaches fundamentally fail to address these concerns, though they fa",
      "section_id": "second-why",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'NTIA AI Open Model Weights RFC' is directly relevant to the debate about open/closed source AI and concerns about privacy, disinformation, copyright, safety, bias, and alignment, but no abstract available to confirm specific claims.",
        "checked_at": "2026-02-27T22:56:14.279120+00:00"
      }
    },
    {
      "id": "chapter3:ref-24:0",
      "chapter": "chapter3",
      "ref_num": 24,
      "bibtex_key": "associatedpress2025anthropic",
      "cite_label": "Associated Press 2025",
      "claim_context": "Copyright and Intellectual Property: The closed source approach could better respect IP rights through licensing and usage restrictions negotiated between formal AI companies and data sources ([Associated Press 2025]). Open source suggests that unrestricted sharing better serves creators by enabling innovation and creativity ([Open Source Initiative 2024]).",
      "section_id": "second-why",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract mentions settlement after authors claimed company took pirated copies of their work to train chatbots, supporting the claim about IP rights and licensing restrictions between AI companies and data sources.",
        "checked_at": "2026-02-27T22:56:14.279121+00:00"
      }
    },
    {
      "id": "chapter3:ref-25:0",
      "chapter": "chapter3",
      "ref_num": 25,
      "bibtex_key": "osi_osaid_2024",
      "cite_label": "Open\n                            Source Initiative 2024",
      "claim_context": "Copyright and Intellectual Property: The closed source approach could better respect IP rights through licensing and usage restrictions negotiated between formal AI companies and data sources ([Associated Press 2025]). Open source suggests that unrestricted sharing better serves creators by enabling innovation and creativity ([Open Source Initiative 2024]). Yet through ABC’s lens, neither addresses creators’ fundamental need to control how their work informs AI outputs. Closed source consolidate",
      "section_id": "second-why",
      "verification": {
        "status": "plausible",
        "reasoning": "Abstract discusses removing barriers to learning, using, sharing in open source AI, topically relevant to the claim that open source suggests unrestricted sharing enables innovation, but does not specifically address enabling creators through innovation and creativity.",
        "checked_at": "2026-02-27T22:56:14.279122+00:00"
      }
    },
    {
      "id": "chapter3:ref-26:0",
      "chapter": "chapter3",
      "ref_num": 26,
      "bibtex_key": "owenjackson2024opensource",
      "cite_label": "Owen-Jackson 2024",
      "claim_context": "Safety and Misuse Prevention: Closed source proponents claim centralized oversight prevents harmful applications ([Owen-Jackson 2024]). Open source advocates argue that collective scrutiny better identifies risks ([Grow 2025]).",
      "section_id": "second-why",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract states generative AI could be 'the new cyber crime frontier' without regulation, directly supporting the claim that closed source proponents argue centralized oversight prevents harmful applications.",
        "checked_at": "2026-02-27T22:56:14.279123+00:00"
      }
    },
    {
      "id": "chapter3:ref-27:0",
      "chapter": "chapter3",
      "ref_num": 27,
      "bibtex_key": "grow2025zuckerberg",
      "cite_label": "Grow 2025",
      "claim_context": "Safety and Misuse Prevention: Closed source proponents claim centralized oversight prevents harmful applications ([Owen-Jackson 2024]). Open source advocates argue that collective scrutiny better identifies risks ([Grow 2025]). Yet ABC reveals that both approaches fail to provide",
      "section_id": "second-why",
      "verification": {
        "status": "plausible",
        "reasoning": "Abstract about Meta's CEO and Chief AI Scientist telling different stories about superintelligence is topically related to the open-source AI debate, but does not specifically discuss collective scrutiny identifying risks.",
        "checked_at": "2026-02-27T22:56:14.279124+00:00"
      }
    },
    {
      "id": "chapter3:ref-28:0",
      "chapter": "chapter3",
      "ref_num": 28,
      "bibtex_key": "gabriel2024ethicsadvancedaiassistants",
      "cite_label": "Gabriel et al., 2024",
      "claim_context": "Bias and Representation: Closed source teams promise careful curation to prevent bias ([Gabriel et al., 2024]). Open source suggests community oversight ensures fair representation ([Wealand 2025]).",
      "section_id": "second-why",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract covers AI value alignment, safety, bias, misinformation, equity, and recommendations for developers and policymakers, supporting the claim about careful curation to prevent bias in AI assistants.",
        "checked_at": "2026-02-27T22:56:14.279126+00:00"
      }
    },
    {
      "id": "chapter3:ref-29:0",
      "chapter": "chapter3",
      "ref_num": 29,
      "bibtex_key": "wealand2025reducing",
      "cite_label": "Wealand 2025",
      "claim_context": "Bias and Representation: Closed source teams promise careful curation to prevent bias ([Gabriel et al., 2024]). Open source suggests community oversight ensures fair representation ([Wealand 2025]). Yet ABC shows how both approaches fail to give communities ongoing control over how their perspectives inform AI decisions.",
      "section_id": "second-why",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract explicitly discusses Western-biased AI models and how open source AI can help achieve diverse, equitable AI, directly supporting the claim about community oversight ensuring fair representation.",
        "checked_at": "2026-02-27T22:56:14.279127+00:00"
      }
    },
    {
      "id": "chapter3:ref-30:0",
      "chapter": "chapter3",
      "ref_num": 30,
      "bibtex_key": "pasquale_2015",
      "cite_label": "Pasquale 2015",
      "claim_context": "Yet ABC shows how both approaches fail to give communities ongoing control over how their perspectives inform AI decisions. Closed source centralizes these choices under corporate teams ([Pasquale 2015]), while open source allows anyone to modify how perspectives are weighted.",
      "section_id": "second-why",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes corporations connecting dots about personal behavior with hidden algorithms, and how decisions at major tech firms are shrouded in secrecy, directly supporting the claim about closed source centralizing choices under corporate teams.",
        "checked_at": "2026-02-27T22:56:14.279128+00:00"
      }
    },
    {
      "id": "chapter3:ref-3:0",
      "chapter": "chapter3",
      "ref_num": 3,
      "bibtex_key": "kaissis_2020",
      "cite_label": "Kaissis et al., 2020",
      "claim_context": "Yet this merely transforms ABC into corporate benevolence (exactly the kind of centralized control that has already failed to unlock the world’s data and compute resources). Medical institutions withhold valuable research data ([Kaissis et al., 2020]; Gould 2015), publishers restrict access to their work ([Grynbaum and Mac 2023]) precisely because they reject this model of centralized corporate control. The very centralization that supposedly enables control actually undermines it, replacing genuine ABC with a hope that power will be used wisely.",
      "section_id": "second-why",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms medical imaging data is subject to privacy and IP restrictions and discusses federated learning as a bridge, supporting the claim that medical institutions withhold research data due to control concerns.",
        "checked_at": "2026-02-27T22:56:14.279129+00:00"
      }
    },
    {
      "id": "chapter3:ref-32:0",
      "chapter": "chapter3",
      "ref_num": 32,
      "bibtex_key": "grynbaum2023times",
      "cite_label": "Grynbaum and Mac 2023",
      "claim_context": "Yet this merely transforms ABC into corporate benevolence (exactly the kind of centralized control that has already failed to unlock the world’s data and compute resources). Medical institutions withhold valuable research data ([Kaissis et al., 2020]; Gould 2015), publishers restrict access to their work ([Grynbaum and Mac 2023]) precisely because they reject this model of centralized corporate control. The very centralization that supposedly enables control actually undermines it, replacing genuine ABC with a hope that power will be used wisely.",
      "section_id": "second-why",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'The Times Sues OpenAI and Microsoft Over A.I. Use of Copyrighted Work' is directly relevant to publishers restricting access to their work, but source inaccessible (HTTP 403) so no abstract to confirm.",
        "checked_at": "2026-02-27T22:56:14.279131+00:00"
      }
    },
    {
      "id": "chapter3:ref-14:0",
      "chapter": "chapter3",
      "ref_num": 14,
      "bibtex_key": "cummings2017democracy",
      "cite_label": "Cummings\n                            2017",
      "claim_context": "This limitation extends beyond AI. The music industry’s struggles with digital piracy ([Cummings 2017]), society’s challenges with viral misinformation ([Shu et al., 2020]), and government efforts to control classified information ([Elsea 2006]) share the same fundamental issue: information, once copied, escapes control ([Schneier 2015]; [Veliz 2020]).",
      "section_id": "third-why",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Democracy of sound: Music piracy and the remaking of American copyright' is directly relevant to the music industry's struggles with digital piracy, but no abstract available (book).",
        "checked_at": "2026-02-27T22:56:14.279132+00:00"
      }
    },
    {
      "id": "chapter3:ref-15:0",
      "chapter": "chapter3",
      "ref_num": 15,
      "bibtex_key": "shu2020combating",
      "cite_label": "Shu et al., 2020",
      "claim_context": "This limitation extends beyond AI. The music industry’s struggles with digital piracy ([Cummings 2017]), society’s challenges with viral misinformation ([Shu et al., 2020]), and government efforts to control classified information ([Elsea 2006]) share the same fundamental issue: information, once copied, escapes control ([Schneier 2015]; [Veliz 2020]).",
      "section_id": "third-why",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Combating disinformation in a social media age' is directly relevant to society's challenges with viral misinformation, but source inaccessible (HTTP 403) so no abstract to confirm.",
        "checked_at": "2026-02-27T22:56:14.279133+00:00"
      }
    },
    {
      "id": "chapter3:ref-33:0",
      "chapter": "chapter3",
      "ref_num": 33,
      "bibtex_key": "elsea2006protection",
      "cite_label": "Elsea 2006",
      "claim_context": "This limitation extends beyond AI. The music industry’s struggles with digital piracy ([Cummings 2017]), society’s challenges with viral misinformation ([Shu et al., 2020]), and government efforts to control classified information ([Elsea 2006]) share the same fundamental issue: information, once copied, escapes control ([Schneier 2015]; [Veliz 2020]).",
      "section_id": "third-why",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'The protection of classified information: The legal framework' is directly relevant to government efforts to control classified information, but no abstract available.",
        "checked_at": "2026-02-27T22:56:14.279134+00:00"
      }
    },
    {
      "id": "chapter3:ref-16:0",
      "chapter": "chapter3",
      "ref_num": 16,
      "bibtex_key": "schneier_2015_data",
      "cite_label": "Schneier 2015",
      "claim_context": "This limitation extends beyond AI. The music industry’s struggles with digital piracy ([Cummings 2017]), society’s challenges with viral misinformation ([Shu et al., 2020]), and government efforts to control classified information ([Elsea 2006]) share the same fundamental issue: information, once copied, escapes control ([Schneier 2015]; [Veliz 2020]).",
      "section_id": "third-why",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes surveillance, data collection, and how personal information escapes individual control once collected, supporting the claim that information once copied escapes control.",
        "checked_at": "2026-02-27T22:56:14.279135+00:00"
      }
    },
    {
      "id": "chapter3:ref-17:0",
      "chapter": "chapter3",
      "ref_num": 17,
      "bibtex_key": "veliz_2020_privacy",
      "cite_label": "Veliz 2020",
      "claim_context": "This limitation extends beyond AI. The music industry’s struggles with digital piracy ([Cummings 2017]), society’s challenges with viral misinformation ([Shu et al., 2020]), and government efforts to control classified information ([Elsea 2006]) share the same fundamental issue: information, once copied, escapes control ([Schneier 2015]; [Veliz 2020]).",
      "section_id": "third-why",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses how tech companies harvest personal information without permission and how digital technology steals personal data and control, supporting the claim that copied information escapes control.",
        "checked_at": "2026-02-27T22:56:14.279137+00:00"
      }
    },
    {
      "id": "chapter3:ref-34:0",
      "chapter": "chapter3",
      "ref_num": 34,
      "bibtex_key": "mcmahan2017communication",
      "cite_label": "McMahan et al., 2017",
      "claim_context": "To begin, we call upon the concept of semi-input privacy, which enables multiple parties to jointly compute a function together where at least some of the parties don’t have to reveal their data to each other. Perhaps the most famous semi-input privacy technique is on-device/federated learning ([McMahan et al., 2017]), wherein a computation moves to the data instead of the data being centralized for computation.",
      "section_id": "semi-input-privacy",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes learning models on mobile devices with data remaining on-device, which is the definition of federated/on-device learning, directly supporting the claim about computation moving to data instead of centralizing data.",
        "checked_at": "2026-02-27T22:56:14.279138+00:00"
      }
    },
    {
      "id": "chapter3:ref-3:1",
      "chapter": "chapter3",
      "ref_num": 3,
      "bibtex_key": "kaissis_2020",
      "cite_label": "Kaissis et al.,\n                            2020",
      "claim_context": "Deep voting could leverage federated computation ([Kaissis et al., 2020]) to allow data sources to train their respective section of an AI model without sharing raw data. More specifically, recall that a deep voting architecture partitions an AI model’s weights into source-specific sections, which are merged through either semantic (e.g., RAG) or semantic (e.g., model merging) means. In theory, instead of each of these partitions being created and/or stored by the AI user or singular model owner",
      "section_id": "semi-input-privacy",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses federated learning for medical imaging as a bridge between data protection and data utilization, supporting the claim about leveraging federated computation for training without sharing raw data.",
        "checked_at": "2026-02-27T22:56:14.279139+00:00"
      }
    },
    {
      "id": "chapter3:ref-4:0",
      "chapter": "chapter3",
      "ref_num": 4,
      "bibtex_key": "gentry2009fully",
      "cite_label": "Gentry and Boneh 2009",
      "claim_context": "A variety of technologies can provide input privacy: secure enclaves, homomorphic encryption, and various other secure multi-party computation algorithms ([Gentry and Boneh 2009]; [Costan and Devadas 2016]; [Yao 1982a]; [Goldreich 1987]; [Bogdanov et al., 2014]; [Craddocket al., 2018]). For ease of exposition, consider a combined use of homomorphic encryption and secure enclaves.",
      "section_id": "full-input-privacy",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'A fully homomorphic encryption scheme' is directly relevant to the claim about homomorphic encryption providing input privacy, but source inaccessible (HTTP 403) so no abstract to confirm.",
        "checked_at": "2026-02-27T22:56:14.279140+00:00"
      }
    },
    {
      "id": "chapter3:ref-5:0",
      "chapter": "chapter3",
      "ref_num": 5,
      "bibtex_key": "costan2016intel",
      "cite_label": "Costan and Devadas 2016",
      "claim_context": "A variety of technologies can provide input privacy: secure enclaves, homomorphic encryption, and various other secure multi-party computation algorithms ([Gentry and Boneh 2009]; [Costan and Devadas 2016]; [Yao 1982a]; [Goldreich 1987]; [Bogdanov et al., 2014]; [Craddocket al., 2018]). For ease of exposition, consider a combined use of homomorphic encryption and secure enclaves.",
      "section_id": "full-input-privacy",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes Intel SGX as providing integrity and privacy guarantees for security-sensitive computation, directly supporting the claim about secure enclaves providing input privacy.",
        "checked_at": "2026-02-27T22:56:14.279142+00:00"
      }
    },
    {
      "id": "chapter3:ref-6:0",
      "chapter": "chapter3",
      "ref_num": 6,
      "bibtex_key": "Yao1982ProtocolsFS",
      "cite_label": "Yao\n                            1982a",
      "claim_context": "A variety of technologies can provide input privacy: secure enclaves, homomorphic encryption, and various other secure multi-party computation algorithms ([Gentry and Boneh 2009]; [Costan and Devadas 2016]; [Yao 1982a]; [Goldreich 1987]; [Bogdanov et al., 2014]; [Craddocket al., 2018]). For ease of exposition, consider a combined use of homomorphic encryption and secure enclaves.",
      "section_id": "full-input-privacy",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Protocols for secure computations' is directly relevant to secure multi-party computation algorithms providing input privacy, but no abstract available.",
        "checked_at": "2026-02-27T22:56:14.279143+00:00"
      }
    },
    {
      "id": "chapter3:ref-7:0",
      "chapter": "chapter3",
      "ref_num": 7,
      "bibtex_key": "goldreich1987towards",
      "cite_label": "Goldreich 1987",
      "claim_context": "A variety of technologies can provide input privacy: secure enclaves, homomorphic encryption, and various other secure multi-party computation algorithms ([Gentry and Boneh 2009]; [Costan and Devadas 2016]; [Yao 1982a]; [Goldreich 1987]; [Bogdanov et al., 2014]; [Craddocket al., 2018]). For ease of exposition, consider a combined use of homomorphic encryption and secure enclaves.",
      "section_id": "full-input-privacy",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Towards a theory of software protection and simulation by oblivious RAMs' is relevant to secure computation techniques, but source inaccessible (HTTP 403) so no abstract to confirm.",
        "checked_at": "2026-02-27T22:56:14.279144+00:00"
      }
    },
    {
      "id": "chapter3:ref-35:0",
      "chapter": "chapter3",
      "ref_num": 35,
      "bibtex_key": "bogdanov2014input",
      "cite_label": "Bogdanov et\n                            al., 2014",
      "claim_context": "A variety of technologies can provide input privacy: secure enclaves, homomorphic encryption, and various other secure multi-party computation algorithms ([Gentry and Boneh 2009]; [Costan and Devadas 2016]; [Yao 1982a]; [Goldreich 1987]; [Bogdanov et al., 2014]; [Craddocket al., 2018]). For ease of exposition, consider a combined use of homomorphic encryption and secure enclaves.",
      "section_id": "full-input-privacy",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses secure multi-party computation primitives, input privacy notions, and composable security, directly supporting the claim about secure multi-party computation algorithms providing input privacy.",
        "checked_at": "2026-02-27T22:56:14.279145+00:00"
      }
    },
    {
      "id": "chapter3:ref-36:0",
      "chapter": "chapter3",
      "ref_num": 36,
      "bibtex_key": "a2019_un",
      "cite_label": "Craddocket al., 2018",
      "claim_context": "A variety of technologies can provide input privacy: secure enclaves, homomorphic encryption, and various other secure multi-party computation algorithms ([Gentry and Boneh 2009]; [Costan and Devadas 2016]; [Yao 1982a]; [Goldreich 1987]; [Bogdanov et al., 2014]; [Craddocket al., 2018]). For ease of exposition, consider a combined use of homomorphic encryption and secure enclaves.",
      "section_id": "full-input-privacy",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes privacy-preserving computation techniques for statistical analysis of sensitive data including methods that protect privacy during processing, directly supporting the claim about input privacy technologies.",
        "checked_at": "2026-02-27T22:56:14.279146+00:00"
      }
    },
    {
      "id": "chapter3:ref-4:1",
      "chapter": "chapter3",
      "ref_num": 4,
      "bibtex_key": "gentry2009fully",
      "cite_label": "Gentry and\n                            Boneh 2009",
      "claim_context": "In the context of deep voting, two privacy preserving operations are necessary. First, the AI user needs to privately send their query to millions of sources, who then must reply with relevant semantic information (e.g. RAG results), and relevant syntactic information (e.g. model partitions). For this, an AI user might leverage a homomorphic encryption ([Gentry and Boneh 2009]; [Boneh et al., 2011]) key-value database, enabling them to query vast collections of databases without precisely reveal",
      "section_id": "full-input-privacy",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'A fully homomorphic encryption scheme' is directly relevant to the claim about leveraging homomorphic encryption for private database queries, but source inaccessible (HTTP 403) so no abstract to confirm.",
        "checked_at": "2026-02-27T22:56:14.279148+00:00"
      }
    },
    {
      "id": "chapter3:ref-37:0",
      "chapter": "chapter3",
      "ref_num": 37,
      "bibtex_key": "boneh2011functional",
      "cite_label": "Boneh et al., 2011",
      "claim_context": "RAG results), and relevant syntactic information (e.g. model partitions). For this, an AI user might leverage a homomorphic encryption ([Gentry and Boneh 2009]; [Boneh et al., 2011]) key-value database, enabling them to query vast collections of databases without precisely revealing their query.",
      "section_id": "full-input-privacy",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses functional encryption where restricted secret keys enable learning specific functions of encrypted data, supporting the claim about homomorphic encryption enabling private database queries.",
        "checked_at": "2026-02-27T22:56:14.279149+00:00"
      }
    },
    {
      "id": "chapter3:ref-5:1",
      "chapter": "chapter3",
      "ref_num": 5,
      "bibtex_key": "costan2016intel",
      "cite_label": "Costan and Devadas\n                            2016",
      "claim_context": "This brings us to the second privacy preserving operation, the transformation from semantic and syntactic responses into an output prediction. For this, the group might co-leverage a collection of GPU enclaves. GPU enclaves ([Costan and Devadas 2016]) offer full input privacy by only decrypting information when that information is actively being computed over within its chip, writing only encrypted information to RAM and hard disks throughout its process. Indeed, a GPU enclave enables a collecti",
      "section_id": "full-input-privacy",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes SGX providing integrity and privacy guarantees for computation where privileged software is potentially malicious, directly supporting the claim about GPU enclaves offering full input privacy by only decrypting during computation.",
        "checked_at": "2026-02-27T22:56:14.279150+00:00"
      }
    },
    {
      "id": "chapter3:ref-36:1",
      "chapter": "chapter3",
      "ref_num": 36,
      "bibtex_key": "a2019_un",
      "cite_label": "Craddock et al., 2018",
      "claim_context": "Taken together, while there are a variety of full input privacy technologies, some combination of technologies fit for querying and then computing is likely appropriate for maximizing various performance tradeoffs ([Craddock et al., 2018]). And when used properly, these technologies could enable deep voting wherein each data source retained sole control over the only copy of their information (semantic and syntactic), and the AI user didn’t need to fully reveal their query to the many data sources they elect to leverage.",
      "section_id": "full-input-privacy",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes various privacy-preserving computation techniques and their tradeoffs for statistical analysis, supporting the claim about combining different input privacy technologies for performance tradeoffs.",
        "checked_at": "2026-02-27T22:56:14.279152+00:00"
      }
    },
    {
      "id": "chapter3:ref-38:0",
      "chapter": "chapter3",
      "ref_num": 38,
      "bibtex_key": "dwork2006calibrating",
      "cite_label": "Dwork et al., 2006",
      "claim_context": "However, deep voting’s intelligence budgets (via differential attribution mechanisms) already provide the necessary output privacy guarantees. This is because differential attribution and differential privacy ([Dwork et al., 2006]; [Dwork et al., 2014]) are two sides of the same coin:",
      "section_id": "output-privacy",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses privacy-preserving statistical databases and query functions, directly supporting the claim about differential privacy as related to output privacy guarantees.",
        "checked_at": "2026-02-27T22:56:14.279153+00:00"
      }
    },
    {
      "id": "chapter3:ref-8:0",
      "chapter": "chapter3",
      "ref_num": 8,
      "bibtex_key": "dwork2014algorithmic",
      "cite_label": "Dwork et al., 2014",
      "claim_context": "However, deep voting’s intelligence budgets (via differential attribution mechanisms) already provide the necessary output privacy guarantees. This is because differential attribution and differential privacy ([Dwork et al., 2006]; [Dwork et al., 2014]) are two sides of the same coin:",
      "section_id": "output-privacy",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'The algorithmic foundations of differential privacy' is directly relevant to the claim about differential privacy being related to differential attribution, but source inaccessible (HTTP 403) so no abstract to confirm.",
        "checked_at": "2026-02-27T22:56:14.279154+00:00"
      }
    },
    {
      "id": "chapter3:ref-8:1",
      "chapter": "chapter3",
      "ref_num": 8,
      "bibtex_key": "dwork2014algorithmic",
      "cite_label": "Dwork et al., 2014",
      "claim_context": "Both are ways of measuring and providing guarantees over the same fundamental constraint: the degree to which an input source contributes to an output prediction ([Dwork et al., 2014]). Thus, by enforcing intelligence budgets and attribution bounds, deep voting naturally limits how much information about any source can be leaked through outputs.",
      "section_id": "output-privacy",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'The algorithmic foundations of differential privacy' is directly relevant to the claim about measuring input source contribution to output prediction, but source inaccessible (HTTP 403) so no abstract to confirm.",
        "checked_at": "2026-02-27T22:56:14.279159+00:00"
      }
    },
    {
      "id": "chapter3:ref-9:0",
      "chapter": "chapter3",
      "ref_num": 9,
      "bibtex_key": "goldwasser1989knowledge",
      "cite_label": "Goldwasser et al., 1989",
      "claim_context": "Input verification addresses this problem through cryptographic techniques like zero-knowledge proofs and attestation chains ([Goldwasser et al., 1989]; [Feige et al., 1988]). These allow data sources to prove properties about their data without revealing the data itself.",
      "section_id": "input-verification",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'The knowledge complexity of interactive proof systems' is directly relevant to zero-knowledge proofs for input verification, but source inaccessible (HTTP 403) so no abstract to confirm.",
        "checked_at": "2026-02-27T22:56:14.279160+00:00"
      }
    },
    {
      "id": "chapter3:ref-10:0",
      "chapter": "chapter3",
      "ref_num": 10,
      "bibtex_key": "feige1988zero",
      "cite_label": "Feige et al., 1988",
      "claim_context": "Input verification addresses this problem through cryptographic techniques like zero-knowledge proofs and attestation chains ([Goldwasser et al., 1989]; [Feige et al., 1988]). These allow data sources to prove properties about their data without revealing the data itself.",
      "section_id": "input-verification",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes zero-knowledge proofs of identity where provers demonstrate knowledge without revealing computational information, directly supporting the claim about proving data properties without revealing data.",
        "checked_at": "2026-02-27T22:56:14.279161+00:00"
      }
    },
    {
      "id": "chapter3:ref-39:0",
      "chapter": "chapter3",
      "ref_num": 39,
      "bibtex_key": "sovrin",
      "cite_label": "Sovrin",
      "claim_context": "An expert could prove their credentials without revealing their identity ([Sovrin]; [Wang and De Filippi 2020]), because the issuer of those credentials has cryptographically signed a statement.",
      "section_id": "input-verification",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses Sovrin Foundation's work on zero-knowledge proofs and their application in the Sovrin Network, supporting the claim about proving credentials without revealing identity.",
        "checked_at": "2026-02-27T22:56:14.279163+00:00"
      }
    },
    {
      "id": "chapter3:ref-40:0",
      "chapter": "chapter3",
      "ref_num": 40,
      "bibtex_key": "wang_2020_selfsovereign",
      "cite_label": "Wang\n                                and De Filippi 2020",
      "claim_context": "An expert could prove their credentials without revealing their identity ([Sovrin]; [Wang and De Filippi 2020]), because the issuer of those credentials has cryptographically signed a statement.",
      "section_id": "input-verification",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses self-sovereign identity based on attestations, claims, credentials and permissions that is not dependent on any government for legitimacy, supporting the claim about proving credentials without revealing identity.",
        "checked_at": "2026-02-27T22:56:14.279164+00:00"
      }
    },
    {
      "id": "chapter3:ref-11:0",
      "chapter": "chapter3",
      "ref_num": 11,
      "bibtex_key": "laurie2014certificate",
      "cite_label": "Laurie 2014",
      "claim_context": "Input verification techniques can be used by combining basic public-key cryptography (e.g. signed hashes of data) ([Laurie 2014]; [Chase et al., 2020]) with verified computation techniques like zero-knowledge proofs, active security, or secure (GPU or CPU) enclaves ([Goldwasser et al., 1989]; [Loftus and Smart 2011]; [Costan and Devadas 2016]).",
      "section_id": "input-verification",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Certificate transparency' is relevant to public-key cryptography and signed hashes of data, but no abstract available to confirm the specific claim about combining cryptography with verification techniques.",
        "checked_at": "2026-02-27T22:56:14.279165+00:00"
      }
    },
    {
      "id": "chapter3:ref-12:0",
      "chapter": "chapter3",
      "ref_num": 12,
      "bibtex_key": "chase2020signal",
      "cite_label": "Chase et al., 2020",
      "claim_context": "Input verification techniques can be used by combining basic public-key cryptography (e.g. signed hashes of data) ([Laurie 2014]; [Chase et al., 2020]) with verified computation techniques like zero-knowledge proofs, active security, or secure (GPU or CPU) enclaves ([Goldwasser et al., 1989]; [Loftus and Smart 2011]; [Costan and Devadas 2016]).",
      "section_id": "input-verification",
      "verification": {
        "status": "plausible",
        "reasoning": "Abstract mentions Signal's private group system but only provides a brief intro about group messaging; insufficient detail to confirm the specific claim about public-key cryptography and signed hashes.",
        "checked_at": "2026-02-27T22:56:14.279166+00:00"
      }
    },
    {
      "id": "chapter3:ref-9:1",
      "chapter": "chapter3",
      "ref_num": 9,
      "bibtex_key": "goldwasser1989knowledge",
      "cite_label": "Goldwasser\n                            et al., 1989",
      "claim_context": "Input verification techniques can be used by combining basic public-key cryptography (e.g. signed hashes of data) ([Laurie 2014]; [Chase et al., 2020]) with verified computation techniques like zero-knowledge proofs, active security, or secure (GPU or CPU) enclaves ([Goldwasser et al., 1989]; [Loftus and Smart 2011]; [Costan and Devadas 2016]).",
      "section_id": "input-verification",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'The knowledge complexity of interactive proof systems' is directly relevant to zero-knowledge proofs as a verified computation technique, but source inaccessible (HTTP 403) so no abstract to confirm.",
        "checked_at": "2026-02-27T22:56:14.279167+00:00"
      }
    },
    {
      "id": "chapter3:ref-41:0",
      "chapter": "chapter3",
      "ref_num": 41,
      "bibtex_key": "loftus2011secure",
      "cite_label": "Loftus and Smart 2011",
      "claim_context": "Input verification techniques can be used by combining basic public-key cryptography (e.g. signed hashes of data) ([Laurie 2014]; [Chase et al., 2020]) with verified computation techniques like zero-knowledge proofs, active security, or secure (GPU or CPU) enclaves ([Goldwasser et al., 1989]; [Loftus and Smart 2011]; [Costan and Devadas 2016]).",
      "section_id": "input-verification",
      "verification": {
        "status": "plausible",
        "reasoning": "Abstract mentions multi-party computation and specific application scenarios with different security guarantees, topically relevant to verified computation techniques, but too truncated to confirm the specific claim.",
        "checked_at": "2026-02-27T22:56:14.279168+00:00"
      }
    },
    {
      "id": "chapter3:ref-5:2",
      "chapter": "chapter3",
      "ref_num": 5,
      "bibtex_key": "costan2016intel",
      "cite_label": "Costan and\n                            Devadas 2016",
      "claim_context": "Input verification techniques can be used by combining basic public-key cryptography (e.g. signed hashes of data) ([Laurie 2014]; [Chase et al., 2020]) with verified computation techniques like zero-knowledge proofs, active security, or secure (GPU or CPU) enclaves ([Goldwasser et al., 1989]; [Loftus and Smart 2011]; [Costan and Devadas 2016]).",
      "section_id": "input-verification",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes SGX providing integrity and privacy guarantees for security-sensitive computation, supporting the claim about secure enclaves as a verified computation technique.",
        "checked_at": "2026-02-27T22:56:14.279170+00:00"
      }
    },
    {
      "id": "chapter3:ref-41:1",
      "chapter": "chapter3",
      "ref_num": 41,
      "bibtex_key": "loftus2011secure",
      "cite_label": "Loftus and Smart\n                            2011",
      "claim_context": "For internal consistency, verified computation ([Loftus and Smart 2011]) enables one to send a function to inspect data and check whether it has properties it should. For example, an AI user who is leveraging MRI scans might send in a classifier to check whether the MRI scans actually contain ”pictures of a human head”, receiving back summary statistics validating that the data they cannot see is, in fact, the right type of data.",
      "section_id": "input-verification",
      "verification": {
        "status": "plausible",
        "reasoning": "Abstract mentions multi-party computation with different security guarantees and application scenarios, topically relevant to verified computation for inspecting data properties, but too truncated to confirm the specific use case described.",
        "checked_at": "2026-02-27T22:56:14.279171+00:00"
      }
    },
    {
      "id": "chapter3:ref-11:1",
      "chapter": "chapter3",
      "ref_num": 11,
      "bibtex_key": "laurie2014certificate",
      "cite_label": "Laurie 2014",
      "claim_context": "Meanwhile, input verification techniques can also enable the external validation form of verification. As a first step, parties who believe something about a piece of data (i.e. ”According to me... this statement is true”), can use public-key cryptography to hash and sign the underlying data with their cryptographic signature ([Laurie 2014]; [Chase et al., 2020]). For example, a journalist might sign their article as being true.",
      "section_id": "input-verification",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Certificate transparency' is relevant to public-key cryptography for signing data, but no abstract available to confirm the specific claim about using cryptographic signatures for external validation.",
        "checked_at": "2026-02-27T22:56:14.279172+00:00"
      }
    },
    {
      "id": "chapter3:ref-12:1",
      "chapter": "chapter3",
      "ref_num": 12,
      "bibtex_key": "chase2020signal",
      "cite_label": "Chase et al., 2020",
      "claim_context": "Meanwhile, input verification techniques can also enable the external validation form of verification. As a first step, parties who believe something about a piece of data (i.e. ”According to me... this statement is true”), can use public-key cryptography to hash and sign the underlying data with their cryptographic signature ([Laurie 2014]; [Chase et al., 2020]). For example, a journalist might sign their article as being true.",
      "section_id": "input-verification",
      "verification": {
        "status": "plausible",
        "reasoning": "Abstract mentions Signal's private group system but only provides a brief intro; insufficient detail to confirm the specific claim about using public-key cryptography to sign data for external validation.",
        "checked_at": "2026-02-27T22:56:14.279173+00:00"
      }
    },
    {
      "id": "chapter3:ref-11:2",
      "chapter": "chapter3",
      "ref_num": 11,
      "bibtex_key": "laurie2014certificate",
      "cite_label": "Laurie 2014",
      "claim_context": "Note that while it might sound far-fetched for everyone to be cryptographically signing all their information, it is noteworthy that every website loadable by HTTPS gets signed by the web server hosting it ([Laurie 2014]). Thus, there is actually a rather robustly deployed chain of signatures already deployed in the world.",
      "section_id": "input-verification",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Certificate transparency' is directly relevant to the claim about HTTPS websites being cryptographically signed, but no abstract available to confirm the specific claim.",
        "checked_at": "2026-02-27T22:56:14.279174+00:00"
      }
    },
    {
      "id": "chapter3:ref-40:1",
      "chapter": "chapter3",
      "ref_num": 40,
      "bibtex_key": "wang_2020_selfsovereign",
      "cite_label": "Wang and De\n                            Filippi 2020",
      "claim_context": "Note that while it might sound far-fetched for everyone to be cryptographically signing all their information, it is noteworthy that every website loadable by HTTPS gets signed by the web server hosting it ([Laurie 2014]). Thus, there is actually a rather robustly deployed chain of signatures already deployed in the world. For example, if I needed to prove to you that I have a certain amount of money in my bank account, I could load a webpage of my bank, download the page with the signed hash fr",
      "section_id": "input-verification",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses self-sovereign identity based on attestations, claims, credentials, and a signed digital history of actions, supporting the claim about using web signatures and credentials for identity verification.",
        "checked_at": "2026-02-27T22:56:14.279176+00:00"
      }
    },
    {
      "id": "chapter3:ref-42:0",
      "chapter": "chapter3",
      "ref_num": 42,
      "bibtex_key": "ssi",
      "cite_label": "Chaum 1985",
      "claim_context": "And the fact that all HTTPS webpages are cryptographically signed by my bank, and the fact that the page would contain my name, address, and bank balance, would be enough for me to prove to you that I possessed a certain amount of money. The generality of this technology being deployed at web scale is one source of optimism around the DID:WEB movement ([Wang and De Filippi 2020]; [Chaum 1985]; [Adler et al., 2024]).",
      "section_id": "input-verification",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Security without Identification: Transaction Systems to Make Big Brother Obsolete' is directly relevant to the DID:WEB movement and credential-based identity, but source inaccessible (HTTP 403) so no abstract to confirm.",
        "checked_at": "2026-02-27T22:56:14.279177+00:00"
      }
    },
    {
      "id": "chapter3:ref-43:0",
      "chapter": "chapter3",
      "ref_num": 43,
      "bibtex_key": "adler2024personhood",
      "cite_label": "Adler et al., 2024",
      "claim_context": "And the fact that all HTTPS webpages are cryptographically signed by my bank, and the fact that the page would contain my name, address, and bank balance, would be enough for me to prove to you that I possessed a certain amount of money. The generality of this technology being deployed at web scale is one source of optimism around the DID:WEB movement ([Wang and De Filippi 2020]; [Chaum 1985]; [Adler et al., 2024]).",
      "section_id": "input-verification",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses personhood credentials that let users demonstrate they are real people without disclosing personal information, using anonymous credentials and proof-of-personhood systems, directly supporting the claim about the DID:WEB movement.",
        "checked_at": "2026-02-27T22:56:14.279178+00:00"
      }
    },
    {
      "id": "chapter3:ref-5:3",
      "chapter": "chapter3",
      "ref_num": 5,
      "bibtex_key": "costan2016intel",
      "cite_label": "Costan and Devadas 2016",
      "claim_context": "However, this raises another critical question: even if we can verify the inputs, how can we trust that the secure enclaves and homomorphic encryption systems are actually computing what they claim to be computing? Given that no-one can see what happens within these systems ([Costan and Devadas 2016]), who is to say that they are actually running the program which has been requested by the AI user and data sources? This leads us to our next guarantee: output verification.",
      "section_id": "input-verification",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes SGX providing privacy guarantees where computations are shielded even from privileged software, directly supporting the claim about the opacity of secure enclaves and the question of trusting what they compute.",
        "checked_at": "2026-02-27T22:56:14.279180+00:00"
      }
    },
    {
      "id": "chapter3:ref-9:2",
      "chapter": "chapter3",
      "ref_num": 9,
      "bibtex_key": "goldwasser1989knowledge",
      "cite_label": "Goldwasser et al., 1989",
      "claim_context": "Output verification addresses this through two complementary mechanisms: verifiable computation and attestation chains ([Goldwasser et al., 1989]; [Costan and Devadas 2016]). Verifiable computation ([Goldwasser et al., 1989]; [Loftus and Smart 2011]) enables parties to prove that specific computations were performed correctly without revealing the private inputs.",
      "section_id": "output-verification",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'The knowledge complexity of interactive proof systems' is directly relevant to verifiable computation and attestation chains, but source inaccessible (HTTP 403) so no abstract to confirm.",
        "checked_at": "2026-02-27T22:56:14.279182+00:00"
      }
    },
    {
      "id": "chapter3:ref-5:4",
      "chapter": "chapter3",
      "ref_num": 5,
      "bibtex_key": "costan2016intel",
      "cite_label": "Costan and Devadas 2016",
      "claim_context": "Output verification addresses this through two complementary mechanisms: verifiable computation and attestation chains ([Goldwasser et al., 1989]; [Costan and Devadas 2016]). Verifiable computation ([Goldwasser et al., 1989]; [Loftus and Smart 2011]) enables parties to prove that specific computations were performed correctly without revealing the private inputs.",
      "section_id": "output-verification",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes SGX providing integrity guarantees for computation, supporting the claim about attestation chains as a mechanism for output verification.",
        "checked_at": "2026-02-27T22:56:14.279183+00:00"
      }
    },
    {
      "id": "chapter3:ref-9:3",
      "chapter": "chapter3",
      "ref_num": 9,
      "bibtex_key": "goldwasser1989knowledge",
      "cite_label": "Goldwasser et al., 1989",
      "claim_context": "Output verification addresses this through two complementary mechanisms: verifiable computation and attestation chains ([Goldwasser et al., 1989]; [Costan and Devadas 2016]). Verifiable computation ([Goldwasser et al., 1989]; [Loftus and Smart 2011]) enables parties to prove that specific computations were performed correctly without revealing the private inputs.",
      "section_id": "output-verification",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'The knowledge complexity of interactive proof systems' is directly relevant to verifiable computation proving correct computation without revealing inputs, but source inaccessible (HTTP 403) so no abstract to confirm.",
        "checked_at": "2026-02-27T22:56:14.279184+00:00"
      }
    },
    {
      "id": "chapter3:ref-41:2",
      "chapter": "chapter3",
      "ref_num": 41,
      "bibtex_key": "loftus2011secure",
      "cite_label": "Loftus and Smart 2011",
      "claim_context": "Output verification addresses this through two complementary mechanisms: verifiable computation and attestation chains ([Goldwasser et al., 1989]; [Costan and Devadas 2016]). Verifiable computation ([Goldwasser et al., 1989]; [Loftus and Smart 2011]) enables parties to prove that specific computations were performed correctly without revealing the private inputs. For deep voting, includes the critical sub-parts of the overal computation:",
      "section_id": "output-verification",
      "verification": {
        "status": "plausible",
        "reasoning": "Abstract mentions multi-party computation with different security guarantees, topically relevant to verifiable computation, but too truncated to confirm the specific claim about proving correct computation without revealing private inputs.",
        "checked_at": "2026-02-27T22:56:14.279185+00:00"
      }
    },
    {
      "id": "chapter3:ref-13:0",
      "chapter": "chapter3",
      "ref_num": 13,
      "bibtex_key": "shamir1979share",
      "cite_label": "Shamir 1979",
      "claim_context": "Through techniques like additive secret sharing, SMPC enables numbers (and thus any digital computation) to be split into cryptographic ”shares” distributed among participants. Each share-holder gains mathematical veto power over how their share is used in subsequent computations ([Shamir 1979]).",
      "section_id": "flow-governance",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'How to share a secret' is the foundational paper on secret sharing schemes directly relevant to additive secret sharing and SMPC, but source inaccessible (HTTP 403) so no abstract to confirm the specific claim about cryptographic shares and veto power.",
        "checked_at": "2026-02-27T22:56:14.279187+00:00"
      }
    },
    {
      "id": "chapter4:ref-1:0",
      "chapter": "chapter4",
      "ref_num": 1,
      "bibtex_key": "granovetter1973strength",
      "cite_label": "Granovetter 1973",
      "claim_context": "Sociologists call these high-trust relationships \"strong ties\": relationships characterized by sustained interaction, emotional investment, and mutual obligation ([Granovetter 1973]). The last element is crucial.",
      "section_id": "first-why",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'The strength of weak ties' directly relates to the claim about strong ties in sociology, but source inaccessible (HTTP 403) so no abstract to confirm the specific definition cited.",
        "checked_at": "2026-02-27T22:56:14.332557+00:00"
      }
    },
    {
      "id": "chapter4:ref-2:0",
      "chapter": "chapter4",
      "ref_num": 2,
      "bibtex_key": "dunbar1993coevolution",
      "cite_label": "Dunbar 1993",
      "claim_context": "The problem is that building strong-tie relationships requires attention, and attention is finite. Dunbar's research establishes the constraint as universal across humans: approximately 150 stable relationships... a limit traced to neocortex size itself ([Dunbar 1993]).",
      "section_id": "first-why",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms group size covaries with neocortical volume and predicts human group sizes similar to hunter-gatherer societies (~150), directly supporting the claim about Dunbar's number and its link to neocortex size.",
        "checked_at": "2026-02-27T22:56:14.332718+00:00"
      }
    },
    {
      "id": "chapter4:ref-3:0",
      "chapter": "chapter4",
      "ref_num": 3,
      "bibtex_key": "grybauskas2023twitter",
      "cite_label": "Grybauskas 2023",
      "claim_context": "Consider data owners. As Chapter 2 documented, 6+ orders of magnitude of data remains siloed because data owners do not trust institutional intermediaries with their information ([Grybauskas 2023]; [O'Brien 2025]; [Grynbaum and Mac 2023]; [Youssef et al., 2023]). This resistance is rational.",
      "section_id": "institutional-path",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses Twitter rate limits as a response to data scraping, directly supporting the claim about data owners resisting institutional intermediaries scraping their information.",
        "checked_at": "2026-02-27T22:56:14.332720+00:00"
      }
    },
    {
      "id": "chapter4:ref-4:0",
      "chapter": "chapter4",
      "ref_num": 4,
      "bibtex_key": "obrien2025reddit",
      "cite_label": "O'Brien\n                            2025",
      "claim_context": "Consider data owners. As Chapter 2 documented, 6+ orders of magnitude of data remains siloed because data owners do not trust institutional intermediaries with their information ([Grybauskas 2023]; [O'Brien 2025]; [Grynbaum and Mac 2023]; [Youssef et al., 2023]). This resistance is rational. Data owners are each one of millions; the institution has no particular stake in protecting any single owner's interests. Without mutual stakes, data owners have no mechanism to hold institutions accountable",
      "section_id": "institutional-path",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms Reddit sued Perplexity AI for scraping user comments, directly supporting the claim about data owners not trusting intermediaries with their information.",
        "checked_at": "2026-02-27T22:56:14.332722+00:00"
      }
    },
    {
      "id": "chapter4:ref-5:0",
      "chapter": "chapter4",
      "ref_num": 5,
      "bibtex_key": "grynbaum2023times",
      "cite_label": "Grynbaum and Mac 2023",
      "claim_context": "Consider data owners. As Chapter 2 documented, 6+ orders of magnitude of data remains siloed because data owners do not trust institutional intermediaries with their information ([Grybauskas 2023]; [O'Brien 2025]; [Grynbaum and Mac 2023]; [Youssef et al., 2023]). This resistance is rational.",
      "section_id": "institutional-path",
      "verification": {
        "status": "unverifiable",
        "reasoning": "Source inaccessible (HTTP 403); title 'The Times Sues OpenAI and Microsoft Over A.I. Use of Copyrighted Work' is relevant to data owners resisting scraping, but no content available to confirm.",
        "checked_at": "2026-02-27T22:56:14.332723+00:00"
      }
    },
    {
      "id": "chapter4:ref-6:0",
      "chapter": "chapter4",
      "ref_num": 6,
      "bibtex_key": "privacy_blocks_medical_sharing",
      "cite_label": "Youssef et al., 2023",
      "claim_context": "Consider data owners. As Chapter 2 documented, 6+ orders of magnitude of data remains siloed because data owners do not trust institutional intermediaries with their information ([Grybauskas 2023]; [O'Brien 2025]; [Grynbaum and Mac 2023]; [Youssef et al., 2023]). This resistance is rational.",
      "section_id": "institutional-path",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Organizational Factors in Clinical Data Sharing for Artificial Intelligence in Health Care' is directly relevant to the claim about data owners not trusting intermediaries, but source inaccessible (HTTP 403) and no abstract available.",
        "checked_at": "2026-02-27T22:56:14.332724+00:00"
      }
    },
    {
      "id": "chapter4:ref-7:0",
      "chapter": "chapter4",
      "ref_num": 7,
      "bibtex_key": "rao2015klout",
      "cite_label": "Rao et al., 2015",
      "claim_context": "Consider governments. The company Klout processed forty-five billion social interactions daily to score 750 million users ([Rao et al., 2015]), attracted $200 million in acquisition value, and served over two thousand business partners ([Fortune 2014]). Western society shut it down anyway... the day GDPR took effect ([Oremus 2018]).",
      "section_id": "institutional-path",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms the Klout Score system assigned scores to 750 million users across 9 social networks daily by processing over 3600 features, directly supporting the claim about Klout scoring 750 million users.",
        "checked_at": "2026-02-27T22:56:14.332726+00:00"
      }
    },
    {
      "id": "chapter4:ref-8:0",
      "chapter": "chapter4",
      "ref_num": 8,
      "bibtex_key": "fortune2014klout",
      "cite_label": "Fortune\n                            2014",
      "claim_context": "Consider governments. The company Klout processed forty-five billion social interactions daily to score 750 million users ([Rao et al., 2015]), attracted $200 million in acquisition value, and served over two thousand business partners ([Fortune 2014]). Western society shut it down anyway... the day GDPR took effect ([Oremus 2018]). Privacy regulation made Klout's level of trust evaluation illegal by making its level of surveillance illegal. Yet, GDPR is not a local legislation targeted a specif",
      "section_id": "institutional-path",
      "verification": {
        "status": "plausible",
        "reasoning": "Abstract mentions Klout as a 'social scoring startup' that was 'scooped up,' consistent with the $200 million acquisition claim, but the abstract is too brief to confirm the exact acquisition value.",
        "checked_at": "2026-02-27T22:56:14.332727+00:00"
      }
    },
    {
      "id": "chapter4:ref-9:0",
      "chapter": "chapter4",
      "ref_num": 9,
      "bibtex_key": "slate2018klout",
      "cite_label": "Oremus 2018",
      "claim_context": "The company Klout processed forty-five billion social interactions daily to score 750 million users ([Rao et al., 2015]), attracted $200 million in acquisition value, and served over two thousand business partners ([Fortune 2014]). Western society shut it down anyway... the day GDPR took effect ([Oremus 2018]). Privacy regulation made Klout's level of trust evaluation illegal by making its level of surveillance illegal.",
      "section_id": "institutional-path",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Klout Is Dead, Just in Time for Europe's GDPR Privacy Law. That's Not a Coincidence' directly supports the claim that Klout shut down when GDPR took effect, but the abstract snippet is too brief to confirm specific details.",
        "checked_at": "2026-02-27T22:56:14.332728+00:00"
      }
    },
    {
      "id": "chapter4:ref-10:0",
      "chapter": "chapter4",
      "ref_num": 10,
      "bibtex_key": "simmons2022gdpr",
      "cite_label": "Simmons 2022",
      "claim_context": "Privacy regulation made Klout's level of trust evaluation illegal by making its level of surveillance illegal. Yet, GDPR is not a local legislation targeted a specific companies like Klout, it is a European wide paradigm shift in privacy expectations, inspiring a wave of similar legislation around the world ([Simmons 2022]).",
      "section_id": "institutional-path",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms GDPR inspired similar legislation worldwide ('GDPR wasn't the first data privacy law of its kind and it won't be the last... 17 countries with GDPR-like laws'), directly supporting the claim about a wave of similar legislation.",
        "checked_at": "2026-02-27T22:56:14.332729+00:00"
      }
    },
    {
      "id": "chapter4:ref-11:0",
      "chapter": "chapter4",
      "ref_num": 11,
      "bibtex_key": "hootsuite2024",
      "cite_label": "Hootsuite 2024",
      "claim_context": "The result is visible in current online infrastructure. A handful of social media platforms observe and manage approximately 8.5% of all waking human experience on earth... four trillion hours annually ([Hootsuite 2024]).2 This is surveillance at civilizational scale, yet the online trust verification problem remains woefully unsolved: half of web traffic is bots ([Imperva 2024]), up to 30% of online reviews remain fraudulent ([D'Onfro 2013]; [Cross 2022]), and state actors operate troll factories at scale ([Foreign, Commonwealth & Development Office 2022]). Western platforms cannot even re...",
      "section_id": "institutional-path",
      "verification": {
        "status": "plausible",
        "reasoning": "Title and abstract reference a global digital overview report from Hootsuite and We Are Social, which would contain statistics about social media usage and waking hours spent online, but the abstract does not specifically confirm the 8.5% or four trillion hours figures.",
        "checked_at": "2026-02-27T22:56:14.332730+00:00"
      }
    },
    {
      "id": "chapter4:ref-12:0",
      "chapter": "chapter4",
      "ref_num": 12,
      "bibtex_key": "imperva2024bots",
      "cite_label": "Imperva 2024",
      "claim_context": "The result is visible in current online infrastructure. A handful of social media platforms observe and manage approximately 8.5% of all waking human experience on earth... four trillion hours annually ([Hootsuite 2024]).2 This is surveillance at civilizational scale, yet the online trust verification problem remains woefully unsolved: half of web traffic is bots ([Imperva 2024]), up to 30% of online reviews remain fraudulent ([D'Onfro 2013]; [Cross 2022]), and state actors operate troll factories at scale ([Foreign, Commonwealth & Development Office 2022]). Western platforms cannot even re...",
      "section_id": "institutional-path",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses bad bots mimicking human behavior and driving online fraud, directly supporting the claim that half of web traffic is bots.",
        "checked_at": "2026-02-27T22:56:14.332731+00:00"
      }
    },
    {
      "id": "chapter4:ref-13:0",
      "chapter": "chapter4",
      "ref_num": 13,
      "bibtex_key": "donfro2013yelp",
      "cite_label": "D'Onfro\n                            2013",
      "claim_context": "The result is visible in current online infrastructure. A handful of social media platforms observe and manage approximately 8.5% of all waking human experience on earth... four trillion hours annually ([Hootsuite 2024]).2 This is surveillance at civilizational scale, yet the online trust verification problem remains woefully unsolved: half of web traffic is bots ([Imperva 2024]), up to 30% of online reviews remain fraudulent ([D'Onfro 2013]; [Cross 2022]), and state actors operate troll factori",
      "section_id": "institutional-path",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract/title states 'A Whopping 20 Percent Of Yelp Reviews Are Fake,' directly supporting the claim about fake online reviews in the context of unsolved trust verification.",
        "checked_at": "2026-02-27T22:56:14.332733+00:00"
      }
    },
    {
      "id": "chapter4:ref-14:0",
      "chapter": "chapter4",
      "ref_num": 14,
      "bibtex_key": "cross2022fake",
      "cite_label": "Cross 2022",
      "claim_context": "The result is visible in current online infrastructure. A handful of social media platforms observe and manage approximately 8.5% of all waking human experience on earth... four trillion hours annually ([Hootsuite 2024]).2 This is surveillance at civilizational scale, yet the online trust verification problem remains woefully unsolved: half of web traffic is bots ([Imperva 2024]), up to 30% of online reviews remain fraudulent ([D'Onfro 2013]; [Cross 2022]), and state actors operate troll factories at scale ([Foreign, Commonwealth & Development Office 2022]). Western platforms cannot even re...",
      "section_id": "institutional-path",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses fake online reviews and FTC crackdowns, and the title explicitly states 'Up to 30% of online reviews are fake,' directly supporting the claim about 30% fake reviews.",
        "checked_at": "2026-02-27T22:56:14.332734+00:00"
      }
    },
    {
      "id": "chapter4:ref-15:0",
      "chapter": "chapter4",
      "ref_num": 15,
      "bibtex_key": "fcdo2022trollfarm",
      "cite_label": "Foreign, Commonwealth &\n                            Development Office 2022",
      "claim_context": "The result is visible in current online infrastructure. A handful of social media platforms observe and manage approximately 8.5% of all waking human experience on earth... four trillion hours annually ([Hootsuite 2024]).2 This is surveillance at civilizational scale, yet the online trust verification problem remains woefully unsolved: half of web traffic is bots ([Imperva 2024]), up to 30% of online reviews remain fraudulent ([D'Onfro 2013]; [Cross 2022]), and state actors operate troll factori",
      "section_id": "institutional-path",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms the Kremlin uses a troll factory to spread propaganda on social media, directly supporting the claim about state-sponsored troll activity as evidence of unsolved online trust verification.",
        "checked_at": "2026-02-27T22:56:14.332735+00:00"
      }
    },
    {
      "id": "chapter4:ref-16:0",
      "chapter": "chapter4",
      "ref_num": 16,
      "bibtex_key": "lawrence1999digital",
      "cite_label": "Lawrence et\n                            al., 1999",
      "claim_context": "This is mathematical possibility, not a claim about actual social networks, but the pattern has precedent. Information propagation phenomena like word of mouth have operated this way for thousands of years; as they advanced prior to mass broadcasting technologies, the spread of early fire-making, agriculture, and religious ideas all propagated through peer networks of this kind. Academic citation networks, PageRank, and Wikipedia exhibit similar structure: researchers cite dozens of sources, pag",
      "section_id": "third-hypothesis",
      "verification": {
        "status": "plausible",
        "reasoning": "Abstract discusses how the web improved accessibility to scientific information and how citation indexing helps evaluate contributions, which is topically relevant to academic citation networks propagating information, but does not specifically confirm the peer-network propagation pattern claimed.",
        "checked_at": "2026-02-27T22:56:14.332736+00:00"
      }
    },
    {
      "id": "chapter4:ref-17:0",
      "chapter": "chapter4",
      "ref_num": 17,
      "bibtex_key": "page1999pagerank",
      "cite_label": "Page et al., 1999",
      "claim_context": "Information propagation phenomena like word of mouth have operated this way for thousands of years; as they advanced prior to mass broadcasting technologies, the spread of early fire-making, agriculture, and religious ideas all propagated through peer networks of this kind. Academic citation networks, PageRank, and Wikipedia exhibit similar structure: researchers cite dozens of sources, pages/papers link to dozens of others, people/editors assess within their capacity at each hop in the graph, and no central node determines outcomes ([Lawrence et al., 1999]; [Page et al., 1999]; [Reagle 201...",
      "section_id": "third-hypothesis",
      "verification": {
        "status": "unverifiable",
        "reasoning": "Source inaccessible (timeout after 15 seconds); title 'The PageRank citation ranking: Bringing order to the web' is directly relevant to the claim about PageRank exhibiting citation network structure, but no content available.",
        "checked_at": "2026-02-27T22:56:14.332737+00:00"
      }
    },
    {
      "id": "chapter4:ref-18:0",
      "chapter": "chapter4",
      "ref_num": 18,
      "bibtex_key": "reagle2010good",
      "cite_label": "Reagle 2010",
      "claim_context": "Information propagation phenomena like word of mouth have operated this way for thousands of years; as they advanced prior to mass broadcasting technologies, the spread of early fire-making, agriculture, and religious ideas all propagated through peer networks of this kind. Academic citation networks, PageRank, and Wikipedia exhibit similar structure: researchers cite dozens of sources, pages/papers link to dozens of others, people/editors assess within their capacity at each hop in the graph, and no central node determines outcomes ([Lawrence et al., 1999]; [Page et al., 1999]; [Reagle 201...",
      "section_id": "third-hypothesis",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Good faith collaboration: The culture of Wikipedia' is directly relevant to the claim about Wikipedia exhibiting similar peer-network structure, but source inaccessible (HTTP 403) so no abstract to confirm specifics.",
        "checked_at": "2026-02-27T22:56:14.332739+00:00"
      }
    },
    {
      "id": "chapter4:ref-19:0",
      "chapter": "chapter4",
      "ref_num": 19,
      "bibtex_key": "goldhaber1997attention",
      "cite_label": "Goldhaber 1997",
      "claim_context": "Yet communication infrastructure has upgraded these mechanisms unevenly. The printing press, radio, television, and digital networks each scaled broadcasting to larger audiences while leaving the capacity to filter, synthesize, and verify information at human speed ([Goldhaber 1997]). The branching problem is a consequence of this asymmetry: when broadcasting scales but trust does not, bottlenecks form, and power concentrates.",
      "section_id": "third-hypothesis",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'The attention economy and the net' is directly relevant to the claim about broadcasting scaling while trust-filtering capacity remained at human speed, but no abstract available to confirm the specific claim.",
        "checked_at": "2026-02-27T22:56:14.332740+00:00"
      }
    },
    {
      "id": "chapter4:ref-20:0",
      "chapter": "chapter4",
      "ref_num": 20,
      "bibtex_key": "backstrom2012four",
      "cite_label": "Backstrom et al. 2012",
      "claim_context": "Regarding whether social graphs are connected enough, a long-standing meme has suggested that each person in the world is separated by six degrees of separation, a claim a team of Facebook researchers later reduced to four degrees of separation ([Backstrom et al. 2012]). It would seem that the world's social graph is connected enough for four hops to facilitate information propagating globally.",
      "section_id": "the-requirement-recursion",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract discusses the 'six degrees of separation' concept and Facebook researchers' work on social distance, directly supporting the claim that the study reduced the separation to four degrees.",
        "checked_at": "2026-02-27T22:56:14.332741+00:00"
      }
    },
    {
      "id": "chapter4:ref-17:1",
      "chapter": "chapter4",
      "ref_num": 17,
      "bibtex_key": "page1999pagerank",
      "cite_label": "Page et al., 1999",
      "claim_context": "This chapter proposes a trust propagation algorithm called BaconRank, named after the \"six degrees of Kevin Bacon\" game that illustrates the small-world property of social networks. BaconRank is inspired by Google's PageRank algorithm ([Page et al., 1999]), which demonstrated that recursive propagation through a link graph could produce meaningful authority scores at web scale. Both algorithms share a core insight: the importance of a node can be inferred from the structure of paths leading to it, without any centralized authority assigning scores.",
      "section_id": "baconrank",
      "verification": {
        "status": "unverifiable",
        "reasoning": "Source inaccessible (timeout after 15 seconds); title 'The PageRank citation ranking' is relevant to the claim about BaconRank being inspired by PageRank, but no content available to verify.",
        "checked_at": "2026-02-27T22:56:14.332742+00:00"
      }
    },
    {
      "id": "chapter4:ref-21:0",
      "chapter": "chapter4",
      "ref_num": 21,
      "bibtex_key": "haveliwala2002topicsensitive",
      "cite_label": "Haveliwala 2002",
      "claim_context": "Two users with different social graphs will assign different trust scores to the same source. This is analogous to Personalized PageRank ([Haveliwala 2002]), but grounded in social trust rather than topic similarity.",
      "section_id": "baconrank",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'Topic-sensitive PageRank' is directly relevant to the claim about Personalized PageRank as an analogy for BaconRank's per-user trust scores, but source inaccessible (HTTP 403) so no abstract to confirm.",
        "checked_at": "2026-02-27T22:56:14.332744+00:00"
      }
    },
    {
      "id": "chapter5:ref-3:0",
      "chapter": "chapter5",
      "ref_num": 3,
      "bibtex_key": "borgeaud2022improving",
      "cite_label": "Borgeaud et al., 2022",
      "claim_context": "What changes is the possibility of source inspection. When each token of output can be traced to the training examples that most influenced its generation ([Borgeaud et al., 2022]), users gain the architectural prerequisite for verification. They can examine whether claimed facts derive from sources they consider reliable, whether confident assertions rest on thin evidential bases, whether patterns reflect genuine regularities or artifacts of biased sampling.",
      "section_id": "individual-level",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes RETRO, which conditions language models on retrieved document chunks based on token similarity, directly supporting the claim about tracing output tokens to training examples that influenced generation.",
        "checked_at": "2026-02-27T22:56:14.332745+00:00"
      }
    },
    {
      "id": "chapter5:ref-2:0",
      "chapter": "chapter5",
      "ref_num": 2,
      "bibtex_key": "scaling_laws_2020",
      "cite_label": "Kaplan et al.,\n                            2020",
      "claim_context": "At the institutional level, the introduction observed that data owners have rational incentives to withhold contributions from AI systems. Contributing data to centralized training pipelines means surrendering control over how that data will be used, who will benefit from insights derived from it, and whether contributors will receive any attribution or compensation. The introduction estimated that this dynamic has locked away six or more orders of magnitude of potentially valuable training data",
      "section_id": "institutional-level",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms empirical scaling laws showing performance scales as a power-law with model size, dataset size, and compute, supporting the thesis context about data quantity driving AI capability and incentive structures.",
        "checked_at": "2026-02-27T22:56:14.332746+00:00"
      }
    },
    {
      "id": "chapter5:ref-1:0",
      "chapter": "chapter5",
      "ref_num": 1,
      "bibtex_key": "st",
      "cite_label": "Trask et al.,\n                            2020",
      "claim_context": "At the institutional level, the introduction observed that data owners have rational incentives to withhold contributions from AI systems. Contributing data to centralized training pipelines means surrendering control over how that data will be used, who will benefit from insights derived from it, and whether contributors will receive any attribution or compensation. The introduction estimated that this dynamic has locked away six or more orders of magnitude of potentially valuable training data",
      "section_id": "institutional-level",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes the 'copy problem' where sharing information means losing control over its use, directly supporting the claim that data owners have rational incentives to withhold contributions.",
        "checked_at": "2026-02-27T22:56:14.332747+00:00"
      }
    },
    {
      "id": "chapter5:ref-7:0",
      "chapter": "chapter5",
      "ref_num": 7,
      "bibtex_key": "dwork2014algorithmic",
      "cite_label": "Dwork & Roth,\n                            2014",
      "claim_context": "At the institutional level, the introduction observed that data owners have rational incentives to withhold contributions from AI systems. Contributing data to centralized training pipelines means surrendering control over how that data will be used, who will benefit from insights derived from it, and whether contributors will receive any attribution or compensation. The introduction estimated that this dynamic has locked away six or more orders of magnitude of potentially valuable training data",
      "section_id": "institutional-level",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'The algorithmic foundations of differential privacy' is directly relevant to the claim about cryptographic mechanisms enforcing access policies, but source inaccessible (HTTP 403) so no abstract to confirm.",
        "checked_at": "2026-02-27T22:56:14.332749+00:00"
      }
    },
    {
      "id": "chapter5:ref-6:0",
      "chapter": "chapter5",
      "ref_num": 6,
      "bibtex_key": "fl",
      "cite_label": "McMahan et al., 2017",
      "claim_context": "Structured transparency ([Trask et al., 2020]), as surveyed in Chapter 3, addresses the technical barrier that makes this withholding rational. When cryptographic mechanisms can enforce access policies ([Dwork & Roth, 2014]; [McMahan et al., 2017]), when contributors can specify conditions under which their data may be used, when audit trails can verify compliance with those conditions, the calculus changes. Data owners might contribute to systems where they retain meaningful control even as their contributions enable collective intelligence.",
      "section_id": "institutional-level",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract describes federated learning as an approach that leaves training data on mobile devices rather than centralizing it, directly supporting the claim about cryptographic mechanisms enabling contributors to retain control.",
        "checked_at": "2026-02-27T22:56:14.332750+00:00"
      }
    },
    {
      "id": "chapter5:ref-8:0",
      "chapter": "chapter5",
      "ref_num": 8,
      "bibtex_key": "gabriel2020artificial",
      "cite_label": "Gabriel, 2020",
      "claim_context": "At the societal level, the introduction raised questions of governance and alignment that currently dominate discourse in the field. How do we ensure that AI systems behave in accordance with human values ([Gabriel, 2020])? How do we maintain meaningful human control over systems that may eventually exceed human capabilities in many domains?",
      "section_id": "societal-level",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract addresses philosophical questions of AI alignment including differences between AI aligned with instructions, intentions, preferences, interests, and values, directly supporting the claim about ensuring AI behaves in accordance with human values.",
        "checked_at": "2026-02-27T22:56:14.332751+00:00"
      }
    },
    {
      "id": "chapter5:ref-4:0",
      "chapter": "chapter5",
      "ref_num": 4,
      "bibtex_key": "dunbar1993coevolution",
      "cite_label": "Dunbar, 1993",
      "claim_context": "Current approaches typically involve centralized actors tuning systems on samples of human feedback, hoping that the tuning generalizes appropriately. The recursive structures surveyed in Chapter 4 ([Dunbar, 1993]; [Granovetter, 1973]) suggest an alternative architecture. If AI systems require ongoing contributions from distributed sources, and if contributors retain the ability to withdraw those contributions, then alignment becomes structurally enforced rather than centrally imposed.",
      "section_id": "societal-level",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract confirms the relationship between neocortical size and group size in primates, supporting the citation in Chapter 5's context of recursive social structures surveyed in Chapter 4.",
        "checked_at": "2026-02-27T22:56:14.332752+00:00"
      }
    },
    {
      "id": "chapter5:ref-5:0",
      "chapter": "chapter5",
      "ref_num": 5,
      "bibtex_key": "granovetter1973strength",
      "cite_label": "Granovetter,\n                            1973",
      "claim_context": "At the societal level, the introduction raised questions of governance and alignment that currently dominate discourse in the field. How do we ensure that AI systems behave in accordance with human values ([Gabriel, 2020])? How do we maintain meaningful human control over systems that may eventually exceed human capabilities in many domains? Current approaches typically involve centralized actors tuning systems on samples of human feedback, hoping that the tuning generalizes appropriately. The r",
      "section_id": "societal-level",
      "verification": {
        "status": "plausible",
        "reasoning": "Title 'The strength of weak ties' is directly relevant to the claim about recursive social structures from Chapter 4, but source inaccessible (HTTP 403) so no abstract to confirm specifics.",
        "checked_at": "2026-02-27T22:56:14.332754+00:00"
      }
    },
    {
      "id": "appendix1:ref-1:0",
      "chapter": "appendix1",
      "ref_num": 1,
      "bibtex_key": "mann2024ai",
      "cite_label": "Mann 2024",
      "claim_context": "Reported quarterly growth patterns ([Mann 2024])",
      "section_id": "methodology",
      "verification": {
        "status": "plausible",
        "reasoning": "Title and abstract discuss AI chip market competition and infrastructure growth, which is topically relevant to reported quarterly growth patterns, but the abstract is too brief to confirm specific quarterly figures.",
        "checked_at": "2026-02-27T22:56:14.332755+00:00"
      }
    },
    {
      "id": "appendix1:ref-2:0",
      "chapter": "appendix1",
      "ref_num": 2,
      "bibtex_key": "epoch2024hardware",
      "cite_label": "Epoch AI 2024",
      "claim_context": "Historical hardware retirement rates ([Epoch AI 2024])",
      "section_id": "methodology",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract presents data on over 170 AI accelerators (GPUs, TPUs) used in deep learning, directly supporting the citation about historical hardware retirement rates.",
        "checked_at": "2026-02-27T22:56:14.332758+00:00"
      }
    },
    {
      "id": "appendix1:ref-1:1",
      "chapter": "appendix1",
      "ref_num": 1,
      "bibtex_key": "mann2024ai",
      "cite_label": "Mann 2024",
      "claim_context": "Reported infrastructure retention patterns ([Mann 2024])",
      "section_id": "methodology",
      "verification": {
        "status": "plausible",
        "reasoning": "Title and abstract about AI chip market infrastructure are topically relevant to infrastructure retention patterns, but the abstract is too brief to confirm specific retention data.",
        "checked_at": "2026-02-27T22:56:14.332759+00:00"
      }
    },
    {
      "id": "appendix1:ref-3:0",
      "chapter": "appendix1",
      "ref_num": 3,
      "bibtex_key": "ming2024nvidia",
      "cite_label": "Ming 2024",
      "claim_context": "275M average quarterly smartphone sales ([Ming 2024])",
      "section_id": "methodology",
      "verification": {
        "status": "plausible",
        "reasoning": "Abstract discusses NVIDIA GPU shipments projected at 4 million in 2024 with 98% market share, which is about GPU shipments not smartphone sales; the claim context cites this for '275M average quarterly smartphone sales' which appears to be a different statistic the article may also contain.",
        "checked_at": "2026-02-27T22:56:14.332761+00:00"
      }
    },
    {
      "id": "appendix1:ref-1:2",
      "chapter": "appendix1",
      "ref_num": 1,
      "bibtex_key": "mann2024ai",
      "cite_label": "Mann 2024",
      "claim_context": "[Mann 2024]",
      "section_id": "hardware-specs",
      "verification": {
        "status": "plausible",
        "reasoning": "Title relevant to AI chip infrastructure analysis but claim context is just the bare citation '[Mann 2024]' with no specific claim to verify against.",
        "checked_at": "2026-02-27T22:56:14.332762+00:00"
      }
    },
    {
      "id": "appendix1:ref-2:1",
      "chapter": "appendix1",
      "ref_num": 2,
      "bibtex_key": "epoch2024hardware",
      "cite_label": "Epoch AI 2024",
      "claim_context": "[Epoch AI 2024]",
      "section_id": "hardware-specs",
      "verification": {
        "status": "plausible",
        "reasoning": "Title relevant to ML hardware data but claim context is just the bare citation '[Epoch AI 2024]' with no specific claim to verify against.",
        "checked_at": "2026-02-27T22:56:14.332763+00:00"
      }
    },
    {
      "id": "appendix1:ref-2:2",
      "chapter": "appendix1",
      "ref_num": 2,
      "bibtex_key": "epoch2024hardware",
      "cite_label": "Epoch AI 2024",
      "claim_context": "[Epoch AI 2024]",
      "section_id": "hardware-specs",
      "verification": {
        "status": "plausible",
        "reasoning": "Title relevant to ML hardware data but claim context is just the bare citation '[Epoch AI 2024]' with no specific claim to verify against.",
        "checked_at": "2026-02-27T22:56:14.332764+00:00"
      }
    },
    {
      "id": "appendix1:ref-1:3",
      "chapter": "appendix1",
      "ref_num": 1,
      "bibtex_key": "mann2024ai",
      "cite_label": "Mann 2024",
      "claim_context": "[Mann 2024]",
      "section_id": "hardware-specs",
      "verification": {
        "status": "plausible",
        "reasoning": "Title relevant to AI chip infrastructure analysis but claim context is just the bare citation '[Mann 2024]' with no specific claim to verify against.",
        "checked_at": "2026-02-27T22:56:14.332765+00:00"
      }
    },
    {
      "id": "appendix1:ref-1:4",
      "chapter": "appendix1",
      "ref_num": 1,
      "bibtex_key": "mann2024ai",
      "cite_label": "Mann 2024",
      "claim_context": "[Mann 2024]",
      "section_id": "hardware-specs",
      "verification": {
        "status": "plausible",
        "reasoning": "Title relevant to AI chip infrastructure analysis but claim context is just the bare citation '[Mann 2024]' with no specific claim to verify against.",
        "checked_at": "2026-02-27T22:56:14.332767+00:00"
      }
    },
    {
      "id": "appendix1:ref-1:5",
      "chapter": "appendix1",
      "ref_num": 1,
      "bibtex_key": "mann2024ai",
      "cite_label": "Mann 2024",
      "claim_context": "[Mann 2024]",
      "section_id": "hardware-specs",
      "verification": {
        "status": "plausible",
        "reasoning": "Title relevant to AI chip infrastructure analysis but claim context is just the bare citation '[Mann 2024]' with no specific claim to verify against.",
        "checked_at": "2026-02-27T22:56:14.332768+00:00"
      }
    },
    {
      "id": "appendix1:ref-1:6",
      "chapter": "appendix1",
      "ref_num": 1,
      "bibtex_key": "mann2024ai",
      "cite_label": "Mann 2024",
      "claim_context": "[Mann 2024]",
      "section_id": "hardware-specs",
      "verification": {
        "status": "plausible",
        "reasoning": "Title relevant to AI chip infrastructure analysis but claim context is just the bare citation '[Mann 2024]' with no specific claim to verify against.",
        "checked_at": "2026-02-27T22:56:14.332769+00:00"
      }
    },
    {
      "id": "appendix1:ref-3:1",
      "chapter": "appendix1",
      "ref_num": 3,
      "bibtex_key": "ming2024nvidia",
      "cite_label": "Ming 2024",
      "claim_context": "[Ming 2024]",
      "section_id": "hardware-specs",
      "verification": {
        "status": "plausible",
        "reasoning": "Title relevant to NVIDIA GPU shipment data but claim context is just the bare citation '[Ming 2024]' with no specific claim to verify against.",
        "checked_at": "2026-02-27T22:56:14.332770+00:00"
      }
    },
    {
      "id": "appendix1:ref-3:2",
      "chapter": "appendix1",
      "ref_num": 3,
      "bibtex_key": "ming2024nvidia",
      "cite_label": "Ming 2024",
      "claim_context": "[Ming 2024]",
      "section_id": "hardware-specs",
      "verification": {
        "status": "plausible",
        "reasoning": "Title relevant to NVIDIA GPU shipment data but claim context is just the bare citation '[Ming 2024]' with no specific claim to verify against.",
        "checked_at": "2026-02-27T22:56:14.332771+00:00"
      }
    },
    {
      "id": "appendix1:ref-2:3",
      "chapter": "appendix1",
      "ref_num": 2,
      "bibtex_key": "epoch2024hardware",
      "cite_label": "Epoch AI 2024",
      "claim_context": "[Epoch AI 2024]",
      "section_id": "hardware-specs",
      "verification": {
        "status": "plausible",
        "reasoning": "Title relevant to ML hardware data but claim context is just the bare citation '[Epoch AI 2024]' with no specific claim to verify against.",
        "checked_at": "2026-02-27T22:56:14.332772+00:00"
      }
    },
    {
      "id": "appendix1:ref-2:4",
      "chapter": "appendix1",
      "ref_num": 2,
      "bibtex_key": "epoch2024hardware",
      "cite_label": "Epoch AI 2024",
      "claim_context": "[Epoch AI 2024]",
      "section_id": "hardware-specs",
      "verification": {
        "status": "plausible",
        "reasoning": "Title relevant to ML hardware data but claim context is just the bare citation '[Epoch AI 2024]' with no specific claim to verify against.",
        "checked_at": "2026-02-27T22:56:14.332774+00:00"
      }
    },
    {
      "id": "appendix1:ref-2:5",
      "chapter": "appendix1",
      "ref_num": 2,
      "bibtex_key": "epoch2024hardware",
      "cite_label": "Epoch AI 2024",
      "claim_context": "[Epoch AI 2024]",
      "section_id": "hardware-specs",
      "verification": {
        "status": "plausible",
        "reasoning": "Title relevant to ML hardware data but claim context is just the bare citation '[Epoch AI 2024]' with no specific claim to verify against.",
        "checked_at": "2026-02-27T22:56:14.332775+00:00"
      }
    },
    {
      "id": "appendix1:ref-2:6",
      "chapter": "appendix1",
      "ref_num": 2,
      "bibtex_key": "epoch2024hardware",
      "cite_label": "Epoch AI 2024",
      "claim_context": "[Epoch AI 2024]",
      "section_id": "analysis",
      "verification": {
        "status": "plausible",
        "reasoning": "Title relevant to ML hardware data but claim context is just the bare citation '[Epoch AI 2024]' with no specific claim to verify against.",
        "checked_at": "2026-02-27T22:56:14.332776+00:00"
      }
    },
    {
      "id": "appendix1:ref-1:7",
      "chapter": "appendix1",
      "ref_num": 1,
      "bibtex_key": "mann2024ai",
      "cite_label": "Mann 2024",
      "claim_context": "[Mann 2024]",
      "section_id": "analysis",
      "verification": {
        "status": "plausible",
        "reasoning": "Title relevant to AI chip infrastructure analysis but claim context is just the bare citation '[Mann 2024]' with no specific claim to verify against.",
        "checked_at": "2026-02-27T22:56:14.332777+00:00"
      }
    },
    {
      "id": "appendix1:ref-1:8",
      "chapter": "appendix1",
      "ref_num": 1,
      "bibtex_key": "mann2024ai",
      "cite_label": "Mann 2024",
      "claim_context": "[Mann 2024]",
      "section_id": "analysis",
      "verification": {
        "status": "plausible",
        "reasoning": "Title relevant to AI chip infrastructure analysis but claim context is just the bare citation '[Mann 2024]' with no specific claim to verify against.",
        "checked_at": "2026-02-27T22:56:14.332778+00:00"
      }
    },
    {
      "id": "appendix1:ref-2:7",
      "chapter": "appendix1",
      "ref_num": 2,
      "bibtex_key": "epoch2024hardware",
      "cite_label": "Epoch AI 2024",
      "claim_context": "[Epoch AI 2024]",
      "section_id": "analysis",
      "verification": {
        "status": "plausible",
        "reasoning": "Title relevant to ML hardware data but claim context is just the bare citation '[Epoch AI 2024]' with no specific claim to verify against.",
        "checked_at": "2026-02-27T22:56:14.332780+00:00"
      }
    },
    {
      "id": "appendix1:ref-2:8",
      "chapter": "appendix1",
      "ref_num": 2,
      "bibtex_key": "epoch2024hardware",
      "cite_label": "Epoch AI 2024",
      "claim_context": "[Epoch AI 2024]",
      "section_id": "analysis",
      "verification": {
        "status": "plausible",
        "reasoning": "Title relevant to ML hardware data but claim context is just the bare citation '[Epoch AI 2024]' with no specific claim to verify against.",
        "checked_at": "2026-02-27T22:56:14.332781+00:00"
      }
    },
    {
      "id": "appendix1:ref-2:9",
      "chapter": "appendix1",
      "ref_num": 2,
      "bibtex_key": "epoch2024hardware",
      "cite_label": "Epoch AI 2024",
      "claim_context": "[Epoch AI 2024]",
      "section_id": "analysis",
      "verification": {
        "status": "plausible",
        "reasoning": "Title relevant to ML hardware data but claim context is just the bare citation '[Epoch AI 2024]' with no specific claim to verify against.",
        "checked_at": "2026-02-27T22:56:14.332782+00:00"
      }
    },
    {
      "id": "appendix1:ref-1:9",
      "chapter": "appendix1",
      "ref_num": 1,
      "bibtex_key": "mann2024ai",
      "cite_label": "Mann 2024",
      "claim_context": "[Mann 2024]",
      "section_id": "analysis",
      "verification": {
        "status": "plausible",
        "reasoning": "Title relevant to AI chip infrastructure analysis but claim context is just the bare citation '[Mann 2024]' with no specific claim to verify against.",
        "checked_at": "2026-02-27T22:56:14.332783+00:00"
      }
    },
    {
      "id": "appendix1:ref-2:10",
      "chapter": "appendix1",
      "ref_num": 2,
      "bibtex_key": "epoch2024hardware",
      "cite_label": "Epoch AI 2024",
      "claim_context": "[Epoch AI 2024]",
      "section_id": "analysis",
      "verification": {
        "status": "plausible",
        "reasoning": "Title relevant to ML hardware data but claim context is just the bare citation '[Epoch AI 2024]' with no specific claim to verify against.",
        "checked_at": "2026-02-27T22:56:14.332784+00:00"
      }
    },
    {
      "id": "appendix1:ref-3:3",
      "chapter": "appendix1",
      "ref_num": 3,
      "bibtex_key": "ming2024nvidia",
      "cite_label": "Ming 2024",
      "claim_context": "[Ming 2024]",
      "section_id": "analysis",
      "verification": {
        "status": "plausible",
        "reasoning": "Title relevant to NVIDIA GPU shipment data but claim context is just the bare citation '[Ming 2024]' with no specific claim to verify against.",
        "checked_at": "2026-02-27T22:56:14.332785+00:00"
      }
    },
    {
      "id": "appendix1:ref-1:10",
      "chapter": "appendix1",
      "ref_num": 1,
      "bibtex_key": "mann2024ai",
      "cite_label": "Mann 2024",
      "claim_context": "[Mann 2024]",
      "section_id": "analysis",
      "verification": {
        "status": "plausible",
        "reasoning": "Title relevant to AI chip infrastructure analysis but claim context is just the bare citation '[Mann 2024]' with no specific claim to verify against.",
        "checked_at": "2026-02-27T22:56:14.332787+00:00"
      }
    },
    {
      "id": "appendix1:ref-2:11",
      "chapter": "appendix1",
      "ref_num": 2,
      "bibtex_key": "epoch2024hardware",
      "cite_label": "Epoch AI 2024",
      "claim_context": "[Epoch AI 2024]",
      "section_id": "analysis",
      "verification": {
        "status": "plausible",
        "reasoning": "Title relevant to ML hardware data but claim context is just the bare citation '[Epoch AI 2024]' with no specific claim to verify against.",
        "checked_at": "2026-02-27T22:56:14.332788+00:00"
      }
    },
    {
      "id": "appendix1:ref-2:12",
      "chapter": "appendix1",
      "ref_num": 2,
      "bibtex_key": "epoch2024hardware",
      "cite_label": "Epoch AI 2024",
      "claim_context": "[Epoch AI 2024]",
      "section_id": "analysis",
      "verification": {
        "status": "plausible",
        "reasoning": "Title relevant to ML hardware data but claim context is just the bare citation '[Epoch AI 2024]' with no specific claim to verify against.",
        "checked_at": "2026-02-27T22:56:14.332789+00:00"
      }
    },
    {
      "id": "appendix1:ref-2:13",
      "chapter": "appendix1",
      "ref_num": 2,
      "bibtex_key": "epoch2024hardware",
      "cite_label": "Epoch AI 2024",
      "claim_context": "[Epoch AI 2024]",
      "section_id": "analysis",
      "verification": {
        "status": "plausible",
        "reasoning": "Title relevant to ML hardware data but claim context is just the bare citation '[Epoch AI 2024]' with no specific claim to verify against.",
        "checked_at": "2026-02-27T22:56:14.332790+00:00"
      }
    },
    {
      "id": "appendix1:ref-4:0",
      "chapter": "appendix1",
      "ref_num": 4,
      "bibtex_key": "macrotrends2024nvidia",
      "cite_label": "Macrotrends LLC 2024",
      "claim_context": "Temporal Distribution: The 80% factor used for 2023-2024 portion of 2022-2024 deployments is a rough estimate based on the degree to which NVIDIA sales and chip FLOP capacity have greatly increased in the past two years ([Macrotrends LLC 2024]).",
      "section_id": "analysis",
      "verification": {
        "status": "supported",
        "reasoning": "Abstract provides NVIDIA quarterly revenue history and growth rate data from 2012-2025, directly supporting the claim about estimating the 80% factor for 2023-2024 deployments based on NVIDIA sales increases.",
        "checked_at": "2026-02-27T22:56:14.332791+00:00"
      }
    },
    {
      "id": "appendix2:ref-1:0",
      "chapter": "appendix2",
      "ref_num": 1,
      "bibtex_key": "epoch2024hardware",
      "cite_label": "Epo",
      "claim_context": "This appendix presents a comprehensive ranking of 182 notable AI models, combining data from Epoch AI’s “Notable AI Models” database ([Epo] ) with organizational compute capacity estimates from Appendix I. For each model, we track:",
      "section_id": "overview",
      "verification": {
        "status": "plausible",
        "reasoning": "Abstract presents data on 170+ AI accelerators, which is topically relevant to the claim about using Epoch AI's database for ranking 182 notable AI models, though the abstract describes hardware data rather than the 'Notable AI Models' database specifically.",
        "checked_at": "2026-02-27T22:56:14.332793+00:00"
      }
    }
  ],
  "url_checks": {
    "fl": {
      "url": "http://arxiv.org/abs/1602.05629",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:05.026573+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/1602.05629",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.",
      "error": null
    },
    "idp": {
      "url": "http://arxiv.org/abs/1612.02298",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:06.228227+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/1612.02298",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "Differential privacy is a popular privacy model within the research community because of the strong privacy guarantee it offers, namely that the presence or absence of any individual in a data set does not significantly influence the results of analyses on the data set. However, enforcing this strict guarantee in practice significantly distorts data and/or limits data uses, thus diminishing the analytical utility of the differentially private results. In an attempt to address this shortcoming, several relaxations of differential privacy have been proposed that trade off privacy guarantees for improved data utility. In this work, we argue that the standard formalization of differential privacy is stricter than required by the intuitive privacy guarantee it seeks. In particular, the standard formalization requires indistinguishability of results between any pair of neighbor data sets, while indistinguishability between the actual data set and its neighbor data sets should be enough. This limits the data controller&#39;s ability to adjust the level of protection to the actual data, hence resulting in significant accuracy loss. In this respect, we propose individual differential privacy, an alternative differential privacy notion that offers em the same privacy guarantees as standard differential privacy to individuals (even though not to groups of individuals). This new notion allows the data controller to adjust the distortion to the actual data set, which results in less distortion and more analytical accuracy. We propose several mechanisms to attain individual differential privacy and we compare the new notion against standard differential privacy in terms of the accuracy of the analytical results.",
      "error": null
    },
    "backstrom2012four": {
      "url": "https://arxiv.org/abs/1111.4570",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:07.411721+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/1111.4570",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "Frigyes Karinthy, in his 1929 short story &#34;Láancszemek&#34; (&#34;Chains&#34;) suggested that any two persons are distanced by at most six friendship links. (The exact wording of the story is slightly ambiguous: &#34;He bet us that, using no more than five individuals, one of whom is a personal acquaintance, he could contact the selected individual [...]&#34;. It is not completely clear whether the selected individual is part of the five, so this could actually allude to distance five or six in the language of graph theory, but the &#34;six degrees of separation&#34; phrase stuck after John Guare&#39;s 1990 eponymous play. Following Milgram&#39;s definition and Guare&#39;s interpretation, we will assume that &#34;degrees of separation&#34; is the same as &#34;distance minus one&#34;, where &#34;distance&#34; is the usual path length-the number of arcs in the path.) Stanley Milgram in his famous experiment challenged people to route postcards to a fixed recipient by passing them only through direct acquaintances. The average number of intermediaries on the path of the postcards lay between 4.4 and 5.7, depending on the sample of people chosen. We report the results of the first world-scale social-network graph-distance computations, using the entire Facebook network of active users (\\approx721 million users, \\approx69 billion friendship links). The average distance we observe is 4.74, corresponding to 3.74 intermediaries or &#34;degrees of separation&#34;, showing that the world is even smaller than we expected, and prompting the title of this paper. More generally, we study the distance distribution of Facebook and of some interesting geographic subgraphs, looking also at their evolution over time. The networks we are able to explore are almost two orders of magnitude larger than those analysed in the previous literature. We report detailed statistical metadata showing that our measurements (which rely on probabilistic algorithms) are very accurate.",
      "error": null
    },
    "le2013building": {
      "url": "https://arxiv.org/abs/1112.6209",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:08.546504+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/1112.6209",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting with these learned features, we trained our network to obtain 15.8% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative improvement over the previous state-of-the-art.",
      "error": null
    },
    "zeiler2014visualizing": {
      "url": "https://arxiv.org/abs/1311.2901",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:09.711675+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/1311.2901",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \\etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.",
      "error": null
    },
    "rao2015klout": {
      "url": "https://arxiv.org/abs/1510.08487",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:10.840311+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/1510.08487",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "In this work, we present the Klout Score, an influence scoring system that assigns scores to 750 million users across 9 different social networks on a daily basis. We propose a hierarchical framework for generating an influence score for each user, by incorporating information for the user from multiple networks and communities. Over 3600 features that capture signals of influential interactions are aggregated across multiple dimensions for each user. The features are scalably generated by processing over 45 billion interactions from social networks every day, as well as by incorporating factors that indicate real world influence. Supervised models trained from labeled data determine the weights for features, and the final Klout Score is obtained by hierarchically combining communities and networks. We validate the correctness of the score by showing that users with higher scores are able to spread information more effectively in a network. Finally, we use several comparisons to other ranking systems to show that highly influential and recognizable users across different domains have high Klout scores.",
      "error": null
    },
    "amodei2016concreteproblemsaisafety": {
      "url": "https://arxiv.org/abs/1606.06565",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:11.961393+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/1606.06565",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (&#34;avoiding side effects&#34; and &#34;avoiding reward hacking&#34;), an objective function that is too expensive to evaluate frequently (&#34;scalable supervision&#34;), or undesirable behavior during the learning process (&#34;safe exploration&#34; and &#34;distributional shift&#34;). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.",
      "error": null
    },
    "kemker2018measuring": {
      "url": "https://arxiv.org/abs/1708.02072",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:13.092213+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/1708.02072",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "Deep neural networks are used in many state-of-the-art systems for machine perception. Once a network is trained to do a specific task, e.g., bird classification, it cannot easily be trained to do new tasks, e.g., incrementally learning to recognize additional bird species or learning an entirely different task such as flower recognition. When new tasks are added, typical deep neural networks are prone to catastrophically forgetting previous tasks. Networks that are capable of assimilating new information incrementally, much like how humans form new memories over time, will be more efficient than re-training the model from scratch each time a new task needs to be learned. There have been multiple attempts to develop schemes that mitigate catastrophic forgetting, but these methods have not been directly compared, the tests used to evaluate them vary considerably, and these methods have only been evaluated on small-scale problems (e.g., MNIST). In this paper, we introduce new metrics and benchmarks for directly comparing five different mechanisms designed to mitigate catastrophic forgetting in neural networks: regularization, ensembling, rehearsal, dual-memory, and sparse-coding. Our experiments on real-world images and sounds show that the mechanism(s) that are critical for optimal performance vary based on the incremental training paradigm and type of data being used, but they all demonstrate that the catastrophic forgetting problem has yet to be solved.",
      "error": null
    },
    "hanin2018neural": {
      "url": "https://arxiv.org/abs/1801.03744",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:14.225665+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/1801.03744",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "We give a rigorous analysis of the statistical behavior of gradients in a randomly initialized fully connected network N with ReLU activations. Our results show that the empirical variance of the squares of the entries in the input-output Jacobian of N is exponential in a simple architecture-dependent constant beta, given by the sum of the reciprocals of the hidden layer widths. When beta is large, the gradients computed by N at initialization vary wildly. Our approach complements the mean field theory analysis of random networks. From this point of view, we rigorously compute finite width corrections to the statistics of gradients at the edge of chaos.",
      "error": null
    },
    "papernot2018scalableprivatelearningpate": {
      "url": "https://arxiv.org/abs/1802.08908",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:15.347575+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/1802.08908",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "The rapid adoption of machine learning has increased concerns about the privacy implications of machine learning models trained on sensitive data, such as medical records or other personal information. To address those concerns, one promising approach is Private Aggregation of Teacher Ensembles, or PATE, which transfers to a &#34;student&#34; model the knowledge of an ensemble of &#34;teacher&#34; models, with intuitive privacy provided by training teachers on disjoint data and strong privacy guaranteed by noisy aggregation of teachers&#39; answers. However, PATE has so far been evaluated only on simple classification tasks like MNIST, leaving unclear its utility when applied to larger-scale learning tasks and real-world datasets. In this work, we show how PATE can scale to learning tasks with large numbers of output classes and uncurated, imbalanced training data with errors. For this, we introduce new noisy aggregation mechanisms for teacher ensembles that are more selective and add less noise, and prove their tighter differential-privacy guarantees. Our new mechanisms build on two insights: the chance of teacher consensus is increased by using more concentrated noise and, lacking consensus, no answer need be given to a student. The consensus answers used are more likely to be correct, offer better intuitive privacy, and incur lower-differential privacy cost. Our evaluation shows our mechanisms improve on the original PATE on all measures, and scale to larger tasks with both high utility and very strong privacy ($\\varepsilon$ &lt; 1.0).",
      "error": null
    },
    "zhao2018federated": {
      "url": "https://arxiv.org/abs/1806.00582",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:16.473184+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/1806.00582",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "Federated learning enables resource-constrained edge compute devices, such as mobile phones and IoT devices, to learn a shared model for prediction, while keeping the training data local. This decentralized approach to train models provides privacy, security, regulatory and economic benefits. In this work, we focus on the statistical challenge of federated learning when local data is non-IID. We first show that the accuracy of federated learning reduces significantly, by up to 55% for neural networks trained for highly skewed non-IID data, where each client device trains only on a single class of data. We further show that this accuracy reduction can be explained by the weight divergence, which can be quantified by the earth mover&#39;s distance (EMD) between the distribution over classes on each device and the population distribution. As a solution, we propose a strategy to improve training on non-IID data by creating a small subset of data which is globally shared between all the edge devices. Experiments show that accuracy can be increased by 30% for the CIFAR-10 dataset with only 5% globally shared data.",
      "error": null
    },
    "grover2018mnist": {
      "url": "https://arxiv.org/abs/1809.06846",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:17.595825+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/1809.06846",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "The MNIST dataset of the handwritten digits is known as one of the commonly used datasets for machine learning and computer vision research. We aim to study a widely applicable classification problem and apply a simple yet efficient K-nearest neighbor classifier with an enhanced heuristic. We evaluate the performance of the K-nearest neighbor classification algorithm on the MNIST dataset where the $L2$ Euclidean distance metric is compared to a modified distance metric which utilizes the sliding window technique in order to avoid performance degradation due to slight spatial misalignments. The accuracy metric and confusion matrices are used as the performance indicators to compare the performance of the baseline algorithm versus the enhanced sliding window method and results show significant improvement using this proposed method.",
      "error": null
    },
    "scaling_laws_2020": {
      "url": "https://arxiv.org/abs/2001.08361",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:18.714292+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2001.08361",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.",
      "error": null
    },
    "brown2020language": {
      "url": "https://arxiv.org/abs/2005.14165",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:19.849919+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2005.14165",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3&#39;s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
      "error": null
    },
    "feldman2020individual": {
      "url": "https://arxiv.org/abs/2008.11193",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:20.973398+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2008.11193",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "We consider a sequential setting in which a single dataset of individuals is used to perform adaptively-chosen analyses, while ensuring that the differential privacy loss of each participant does not exceed a pre-specified privacy budget. The standard approach to this problem relies on bounding a worst-case estimate of the privacy loss over all individuals and all possible values of their data, for every single analysis. Yet, in many scenarios this approach is overly conservative, especially for &#34;typical&#34; data points which incur little privacy loss by participation in most of the analyses. In this work, we give a method for tighter privacy loss accounting based on the value of a personalized privacy loss estimate for each individual in each analysis. To implement the accounting method we design a filter for Rényi differential privacy. A filter is a tool that ensures that the privacy parameter of a composed sequence of algorithms with adaptively-chosen privacy parameters does not exceed a pre-specified budget. Our filter is simpler and tighter than the known filter for $(\\epsilon,\\delta)$-differential privacy by Rogers et al. We apply our results to the analysis of noisy gradient descent and show that personalized accounting can be practical, easy to implement, and can only make the privacy-utility tradeoff tighter.",
      "error": null
    },
    "st": {
      "url": "https://arxiv.org/abs/2012.08347",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:22.174914+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2012.08347",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "Successful collaboration involves sharing information. However, parties may disagree on how the information they need to share should be used. We argue that many of these concerns reduce to &#39;the copy problem&#39;: once a bit of information is copied and shared, the sender can no longer control how the recipient uses it. From the perspective of each collaborator, this presents a dilemma that can inhibit collaboration. The copy problem is often amplified by three related problems which we term the bundling, edit, and recursive enforcement problems. We find that while the copy problem is not solvable, aspects of these amplifying problems have been addressed in a variety of disconnected fields. We observe that combining these efforts could improve the governability of information flows and thereby incentivise collaboration. We propose a five-part framework which groups these efforts into specific capabilities and offers a foundation for their integration into an overarching vision we call &#34;structured transparency&#34;. We conclude by surveying an array of use-cases that illustrate the structured transparency principles and their related capabilities.",
      "error": null
    },
    "borgeaud2022improving": {
      "url": "https://arxiv.org/abs/2112.04426",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:23.307107+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2112.04426",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",
      "error": null
    },
    "rae2021gopher": {
      "url": "https://arxiv.org/abs/2112.11446",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:24.431544+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2112.11446",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model&#39;s behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.",
      "error": null
    },
    "jeong2022emorybreastimagingdataset": {
      "url": "https://arxiv.org/abs/2202.04073",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:25.593621+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2202.04073",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "Developing and validating artificial intelligence models in medical imaging requires datasets that are large, granular, and diverse. To date, the majority of publicly available breast imaging datasets lack in one or more of these areas. Models trained on these data may therefore underperform on patient populations or pathologies that have not previously been encountered. The EMory BrEast imaging Dataset (EMBED) addresses these gaps by providing 3650,000 2D and DBT screening and diagnostic mammograms for 116,000 women divided equally between White and African American patients. The dataset also contains 40,000 annotated lesions linked to structured imaging descriptors and 61 ground truth pathologic outcomes grouped into six severity classes. Our goal is to share this dataset with research partners to aid in development and validation of breast AI models that will serve all patients fairly and help decrease bias in medical AI.",
      "error": null
    },
    "hoffmann2022trainingcomputeoptimallargelanguage": {
      "url": "https://arxiv.org/abs/2203.15556",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:26.718035+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2203.15556",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.",
      "error": null
    },
    "izacard2023atlas": {
      "url": "https://arxiv.org/abs/2208.03299",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:27.853064+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2208.03299",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters.",
      "error": null
    },
    "nguyen2024surveymachineunlearning": {
      "url": "https://arxiv.org/abs/2209.02299",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:28.976269+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2209.02299",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "Today, computer systems hold large amounts of personal data. Yet while such an abundance of data allows breakthroughs in artificial intelligence, and especially machine learning (ML), its existence can be a threat to user privacy, and it can weaken the bonds of trust between humans and AI. Recent regulations now require that, on request, private information about a user must be removed from both computer systems and from ML models, i.e. ``the right to be forgotten&#39;&#39;). While removing data from back-end databases should be straightforward, it is not sufficient in the AI context as ML models often `remember&#39; the old data. Contemporary adversarial attacks on trained models have proven that we can learn whether an instance or an attribute belonged to the training data. This phenomenon calls for a new paradigm, namely machine unlearning, to make ML models forget about particular data. It turns out that recent works on machine unlearning have not been able to completely solve the problem due to the lack of common frameworks and resources. Therefore, this paper aspires to present a comprehensive examination of machine unlearning&#39;s concepts, scenarios, methods, and applications. Specifically, as a category collection of cutting-edge studies, the intention behind this article is to serve as a comprehensive resource for researchers and practitioners seeking an introduction to machine unlearning and its formulations, design criteria, removal requests, algorithms, and applications. In addition, we aim to highlight the key findings, current trends, and new research areas that have not yet featured the use of machine unlearning but could benefit greatly from it. We hope this survey serves as a valuable resource for ML researchers and those seeking to innovate privacy technologies. Our resources are publicly available at this https URL.",
      "error": null
    },
    "ainsworth2022git": {
      "url": "https://arxiv.org/abs/2209.04836",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:30.157330+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2209.04836",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "The success of deep learning is due in large part to our ability to solve certain massive non-convex optimization problems with relative ease. Though non-convex optimization is NP-hard, simple algorithms -- often variants of stochastic gradient descent -- exhibit surprising effectiveness in fitting large neural networks in practice. We argue that neural network loss landscapes often contain (nearly) a single basin after accounting for all possible permutation symmetries of hidden units a la Entezari et al. 2021. We introduce three algorithms to permute the units of one model to bring them into alignment with a reference model in order to merge the two models in weight space. This transformation produces a functionally equivalent set of weights that lie in an approximately convex basin near the reference model. Experimentally, we demonstrate the single basin phenomenon across a variety of model architectures and datasets, including the first (to our knowledge) demonstration of zero-barrier linear mode connectivity between independently trained ResNet models on CIFAR-10. Additionally, we identify intriguing phenomena relating model width and training time to mode connectivity. Finally, we discuss shortcomings of the linear mode connectivity hypothesis, including a counterexample to the single basin theory.",
      "error": null
    },
    "a2019_un": {
      "url": "https://arxiv.org/abs/2301.06167",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:31.289588+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2301.06167",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "This paper describes privacy-preserving approaches for the statistical analysis. It describes motivations for privacy-preserving approaches for the statistical analysis of sensitive data, presents examples of use cases where such methods may apply and describes relevant technical capabilities to assure privacy preservation while still allowing analysis of sensitive data. Our focus is on methods that enable protecting privacy of data while it is being processed, not only while it is at rest on a system or in transit between systems. The information in this document is intended for use by statisticians and data scientists, data curators and architects, IT specialists, and security and information assurance specialists, so we explicitly avoid cryptographic technical details of the technologies we describe.",
      "error": null
    },
    "manakul2023selfcheckgptzeroresourceblackboxhallucination": {
      "url": "https://arxiv.org/abs/2303.08896",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:32.413492+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2303.08896",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose &#34;SelfCheckGPT&#34;, a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.",
      "error": null
    },
    "guo2023towards": {
      "url": "https://arxiv.org/abs/2310.05773",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:33.538321+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2310.05773",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "The ultimate goal of Dataset Distillation is to synthesize a small synthetic dataset such that a model trained on this synthetic set will perform equally well as a model trained on the full, real dataset. Until now, no method of Dataset Distillation has reached this completely lossless goal, in part due to the fact that previous methods only remain effective when the total number of synthetic samples is extremely small. Since only so much information can be contained in such a small number of samples, it seems that to achieve truly loss dataset distillation, we must develop a distillation method that remains effective as the size of the synthetic dataset grows. In this work, we present such an algorithm and elucidate why existing methods fail to generate larger, high-quality synthetic sets. Current state-of-the-art methods rely on trajectory-matching, or optimizing the synthetic data to induce similar long-term training dynamics as the real data. We empirically find that the training stage of the trajectories we choose to match (i.e., early or late) greatly affects the effectiveness of the distilled dataset. Specifically, early trajectories (where the teacher network learns easy patterns) work well for a low-cardinality synthetic set since there are fewer examples wherein to distribute the necessary information. Conversely, late trajectories (where the teacher network learns hard patterns) provide better signals for larger synthetic sets since there are now enough samples to represent the necessary complex patterns. Based on our findings, we propose to align the difficulty of the generated patterns with the size of the synthetic dataset. In doing so, we successfully scale trajectory matching-based methods to larger synthetic datasets, achieving lossless dataset distillation for the very first time. Code and distilled datasets are available at this https URL.",
      "error": null
    },
    "ahmad2023creatingtrustworthyllmsdealing": {
      "url": "https://arxiv.org/abs/2311.01463",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:34.657063+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2311.01463",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "Large language models have proliferated across multiple domains in as short period of time. There is however hesitation in the medical and healthcare domain towards their adoption because of issues like factuality, coherence, and hallucinations. Give the high stakes nature of healthcare, many researchers have even cautioned against its usage until these issues are resolved. The key to the implementation and deployment of LLMs in healthcare is to make these models trustworthy, transparent (as much possible) and explainable. In this paper we describe the key elements in creating reliable, trustworthy, and unbiased models as a necessary condition for their adoption in healthcare. Specifically we focus on the quantification, validation, and mitigation of hallucinations in the context in healthcare. Lastly, we discuss how the future of LLMs in healthcare may look like.",
      "error": null
    },
    "dai2024deepseekmoe": {
      "url": "https://arxiv.org/abs/2401.06066",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:35.780989+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2401.06066",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top-$K$ out of $N$ experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into $mN$ ones and activating $mK$ from them, allowing for a more flexible combination of activated experts; (2) isolating $K_s$ experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5 times the expert parameters and computation. In addition, DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which set the upper bound of MoE models. Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with LLaMA2 7B, with only about 40% of computations. Further, our preliminary efforts to scale up DeepSeekMoE to 145B parameters consistently validate its substantial advantages over the GShard architecture, and show its performance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%) of computations.",
      "error": null
    },
    "xu2024hallucinationinevitableinnatelimitation": {
      "url": "https://arxiv.org/abs/2401.11817",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:36.909246+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2401.11817",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "Hallucination has been widely recognized to be a significant drawback for large language models (LLMs). There have been many works that attempt to reduce the extent of hallucination. These efforts have mostly been empirical so far, which cannot answer the fundamental question whether it can be completely eliminated. In this paper, we formalize the problem and show that it is impossible to eliminate hallucination in LLMs. Specifically, we define a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function. By employing results from learning theory, we show that LLMs cannot learn all the computable functions and will therefore inevitably hallucinate if used as general problem solvers. Since the formal world is a part of the real world which is much more complicated, hallucinations are also inevitable for real world LLMs. Furthermore, for real world LLMs constrained by provable time complexity, we describe the hallucination-prone tasks and empirically validate our claims. Finally, using the formal world framework, we discuss the possible mechanisms and efficacies of existing hallucination mitigators as well as the practical implications on the safe deployment of LLMs.",
      "error": null
    },
    "gabriel2024ethicsadvancedaiassistants": {
      "url": "https://arxiv.org/abs/2404.16244",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:38.051006+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2404.16244",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "This paper focuses on the opportunities and the ethical and societal risks posed by advanced AI assistants. We define advanced AI assistants as artificial agents with natural language interfaces, whose function is to plan and execute sequences of actions on behalf of a user, across one or more domains, in line with the user&#39;s expectations. The paper starts by considering the technology itself, providing an overview of AI assistants, their technical foundations and potential range of applications. It then explores questions around AI value alignment, well-being, safety and malicious uses. Extending the circle of inquiry further, we next consider the relationship between advanced AI assistants and individual users in more detail, exploring topics such as manipulation and persuasion, anthropomorphism, appropriate relationships, trust and privacy. With this analysis in place, we consider the deployment of advanced assistants at a societal scale, focusing on cooperation, equity and access, misinformation, economic impact, the environment and how best to evaluate advanced AI assistants. Finally, we conclude by providing a range of recommendations for researchers, developers, policymakers and public stakeholders.",
      "error": null
    },
    "adler2024personhood": {
      "url": "https://arxiv.org/abs/2408.07892",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:39.175942+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2408.07892",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "Anonymity is an important principle online. However, malicious actors have long used misleading identities to conduct fraud, spread disinformation, and carry out other deceptive schemes. With the advent of increasingly capable AI, bad actors can amplify the potential scale and effectiveness of their operations, intensifying the challenge of balancing anonymity and trustworthiness online. In this paper, we analyze the value of a new tool to address this challenge: &#34;personhood credentials&#34; (PHCs), digital credentials that empower users to demonstrate that they are real people -- not AIs -- to online services, without disclosing any personal information. Such credentials can be issued by a range of trusted institutions -- governments or otherwise. A PHC system, according to our definition, could be local or global, and does not need to be biometrics-based. Two trends in AI contribute to the urgency of the challenge: AI&#39;s increasing indistinguishability from people online (i.e., lifelike content and avatars, agentic activity), and AI&#39;s increasing scalability (i.e., cost-effectiveness, accessibility). Drawing on a long history of research into anonymous credentials and &#34;proof-of-personhood&#34; systems, personhood credentials give people a way to signal their trustworthiness on online platforms, and offer service providers new tools for reducing misuse by bad actors. In contrast, existing countermeasures to automated deception -- such as CAPTCHAs -- are inadequate against sophisticated AI, while stringent identity verification solutions are insufficiently private for many use-cases. After surveying the benefits of personhood credentials, we also examine deployment risks and design challenges. We conclude with actionable next steps for policymakers, technologists, and standards bodies to consider in consultation with the public.",
      "error": null
    },
    "veldanda2024llm": {
      "url": "https://arxiv.org/abs/2409.13054",
      "official_url": null,
      "checked_at": "2026-02-27T22:28:40.302971+00:00",
      "http_status": 200,
      "final_url": "https://arxiv.org/abs/2409.13054",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "subscribe to arXiv mailings",
      "abstract": "Large language models (LLMs) have revolutionized various domains, yet their utility comes with significant challenges related to outdated or problematic knowledge embedded during pretraining. This paper addresses the challenge of modifying LLMs to unlearn problematic and outdated information while efficiently integrating new knowledge without retraining from scratch. Here, we propose LLM Surgery, a framework to efficiently modify LLM behaviour by optimizing a three component objective function that: (1) Performs reverse gradient on unlearning dataset (problematic and outdated information), (2) Performs gradient descent on the update dataset (new and updated information), and (3) Minimizes the KL divergence on the retain dataset (small subset of unchanged text), ensuring alignment between pretrained and modified model outputs. Due to the lack of publicly available datasets specifically tailored for our novel task, we compiled a new dataset and an evaluation benchmark. Using Llama2-7B, we demonstrate that LLM Surgery can achieve significant forgetting on the unlearn set, a 20\\% increase in accuracy on the update set, and maintain performance on the retain set.",
      "error": null
    },
    "Rieke_2020": {
      "url": "http://dx.doi.org/10.1038/s41746-020-00323-1",
      "official_url": "http://dx.doi.org/10.1038/s41746-020-00323-1",
      "checked_at": "2026-02-27T22:43:38.103402+00:00",
      "http_status": null,
      "final_url": null,
      "accessible": false,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": "Timeout after 15 seconds"
    },
    "Abadi_2016": {
      "url": "http://dx.doi.org/10.1145/2976749.2978318",
      "official_url": "http://dx.doi.org/10.1145/2976749.2978318",
      "checked_at": "2026-02-27T22:43:54.589647+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Forbidden"
    },
    "privacy_blocks_medical_sharing": {
      "url": "https://doi.org/10.1001/jamanetworkopen.2023.48422",
      "official_url": "https://doi.org/10.1001/jamanetworkopen.2023.48422",
      "checked_at": "2026-02-27T22:43:55.812960+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Forbidden"
    },
    "us_cancer_screening": {
      "url": "https://doi.org/10.1002/cncr.28771",
      "official_url": "https://doi.org/10.1002/cncr.28771",
      "checked_at": "2026-02-27T22:43:56.987712+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Forbidden"
    },
    "ntoutsi2020bias": {
      "url": "https://doi.org/10.1002/widm.1356",
      "official_url": "https://doi.org/10.1002/widm.1356",
      "checked_at": "2026-02-27T22:43:58.173672+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Forbidden"
    },
    "shu2020combating": {
      "url": "https://doi.org/10.1002/widm.1385",
      "official_url": "https://doi.org/10.1002/widm.1385",
      "checked_at": "2026-02-27T22:43:59.362068+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Forbidden"
    },
    "dwork2006calibrating": {
      "url": "https://doi.org/10.1007/11681878_14",
      "official_url": "https://link.springer.com/chapter/10.1007/11681878_14",
      "checked_at": "2026-02-27T22:29:03.744040+00:00",
      "http_status": 200,
      "final_url": "https://link.springer.com/chapter/10.1007/11681878_14",
      "accessible": true,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "Calibrating Noise to Sensitivity in Private Data Analysis | Springer Nature Link",
      "abstract": "We continue a line of research initiated in [10,11]on privacy-preserving statistical databases. Consider a trusted server that holds a database of sensitive information. Given a query function f mapping databases to reals, the so-called true answer is the result of...",
      "error": null
    },
    "sienkiewicz2025data": {
      "url": "https://doi.org/10.1007/978-3-031-61003-5_29",
      "official_url": "https://link.springer.com/chapter/10.1007/978-3-031-61003-5_29",
      "checked_at": "2026-02-27T22:29:05.365979+00:00",
      "http_status": 200,
      "final_url": "https://link.springer.com/chapter/10.1007/978-3-031-61003-5_29",
      "accessible": true,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "Overstock Problems in a Purchase-to-Pay Process: An Object-Centric Process Mining Case Study | Springer Nature Link",
      "abstract": "This paper addresses overstock issues in a real-life Purchase-to-Pay process in cooperation with the industry leader in pet retail in Europe. It highlights the development of solutions for more efficient inventory management, thereby reducing overstock. Our approach...",
      "error": null
    },
    "adikari2015real": {
      "url": "https://doi.org/10.1007/978-3-319-18714-3_2",
      "official_url": "https://link.springer.com/chapter/10.1007/978-3-319-18714-3_2",
      "checked_at": "2026-02-27T22:29:07.126204+00:00",
      "http_status": 200,
      "final_url": "https://link.springer.com/chapter/10.1007/978-3-319-18714-3_2",
      "accessible": true,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "Real Time Bidding in Online Digital Advertisement | Springer Nature Link",
      "abstract": "Real time bidding (RTB) is becoming the key to target marketing where it could optimize advertiser expectations drastically. Not like the conventional digital advertising, in the process of RTB, the impressions of a mobile application or a website are mapped to a...",
      "error": null
    },
    "boneh2011functional": {
      "url": "https://doi.org/10.1007/978-3-642-19571-6_16",
      "official_url": "https://link.springer.com/chapter/10.1007/978-3-642-19571-6_16",
      "checked_at": "2026-02-27T22:29:08.910637+00:00",
      "http_status": 200,
      "final_url": "https://link.springer.com/chapter/10.1007/978-3-642-19571-6_16",
      "accessible": true,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "Functional Encryption: Definitions and Challenges | Springer Nature Link",
      "abstract": "We initiate the formal study of functional encryption by giving precise definitions of the concept and its security. Roughly speaking, functional encryption supports restricted secret keys that enable a key holder to learn a specific function of encrypted data, but...",
      "error": null
    },
    "loftus2011secure": {
      "url": "https://doi.org/10.1007/978-3-642-21969-6_1",
      "official_url": "https://link.springer.com/chapter/10.1007/978-3-642-21969-6_1",
      "checked_at": "2026-02-27T22:29:10.549388+00:00",
      "http_status": 200,
      "final_url": "https://link.springer.com/chapter/10.1007/978-3-642-21969-6_1",
      "accessible": true,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "Secure Outsourced Computation | Springer Nature Link",
      "abstract": "The development of multi-party computation was one of the early achievements of theoretical cryptography. Since that time a number of papers have been published which look at specific application scenarios (e-voting, e-auctions), different security guarantees...",
      "error": null
    },
    "feige1988zero": {
      "url": "https://doi.org/10.1007/BF02351717",
      "official_url": "https://link.springer.com/article/10.1007/BF02351717",
      "checked_at": "2026-02-27T22:29:12.249264+00:00",
      "http_status": 200,
      "final_url": "https://link.springer.com/article/10.1007/BF02351717",
      "accessible": true,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "Zero-knowledge proofs of identity | Journal of Cryptology | Springer Nature Link",
      "abstract": "In this paper we extend the notion of interactive proofs of assertions to interactive proofs of knowledge. This leads to the definition of unrestricted input zero-knowledge proofs of knowledge in which the prover demonstrates possession of knowledge without revealing any computational information whatsoever (not even the one bit revealed in zero-knowledge proofs of assertions). We show the relevance of these notions to identification schemes, in which parties prove their identity by demonstrating their knowledge rather than by proving the validity of assertions. We describe a novel scheme which is provably secure if factoring is difficult and whose practical implementations are about two orders of magnitude faster than RSA-based identification schemes. The advantages of thinking in terms of proofs of knowledge rather than proofs of assertions are demonstrated in two efficient variants of the scheme: unrestricted input zero-knowledge proofs of knowledge are used in the construction of a",
      "error": null
    },
    "lederer2024statistical": {
      "url": "https://doi.org/10.1007/s10182-022-00467-3",
      "official_url": "https://doi.org/10.1007/s10182-022-00467-3",
      "checked_at": "2026-02-27T22:44:00.514994+00:00",
      "http_status": null,
      "final_url": null,
      "accessible": false,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": "Timeout after 15 seconds"
    },
    "gabriel2020artificial": {
      "url": "https://doi.org/10.1007/s11023-020-09539-2",
      "official_url": "https://link.springer.com/article/10.1007/s11023-020-09539-2",
      "checked_at": "2026-02-27T22:29:30.768439+00:00",
      "http_status": 200,
      "final_url": "https://link.springer.com/article/10.1007/s11023-020-09539-2",
      "accessible": true,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "Artificial Intelligence, Values, and Alignment | Minds and Machines | Springer Nature Link",
      "abstract": "This paper looks at philosophical questions that arise in the context of AI alignment. It defends three propositions. First, normative and technical aspects of the AI alignment problem are interrelated, creating space for productive engagement between people working in both domains. Second, it is important to be clear about the goal of alignment. There are significant differences between AI that aligns with instructions, intentions, revealed preferences, ideal preferences, interests and values. A principle-based approach to AI alignment, which combines these elements in a systematic way, has considerable advantages in this context. Third, the central challenge for theorists is not to identify ‘true’ moral principles for AI; rather, it is to identify fair principles for alignment that receive reflective endorsement despite widespread variation in people’s moral beliefs. The final part of the paper explores three ways in which fair principles for AI alignment could potentially be identif",
      "error": null
    },
    "scott2021coordinating": {
      "url": "https://doi.org/10.1007/s43508-021-00004-z",
      "official_url": "https://link.springer.com/article/10.1007/s43508-021-00004-z",
      "checked_at": "2026-02-27T22:44:17.172990+00:00",
      "http_status": 200,
      "final_url": "https://link.springer.com/article/10.1007/s43508-021-00004-z",
      "accessible": true,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "Coordinating government silos: challenges and opportunities | Global Public Policy and Governance | Springer Nature Link",
      "abstract": "The literature on silos in government often focuses on their failure to engage effectively in horizontal coordination. While this is often true, silos-dominant administrative systems may still find ways to overcome or prevent incoherence in government. The problem is not so much with the structure of silos but with the lack of effective coordination mechanisms between them. Therefore, it is important to identify what mechanisms may enable silos to work successfully with each other and under what conditions, so that there will be no need to pursue a total breakdown of silos, which can be politically and administratively costly. Using Hong Kong examples, we distinguish three different types of coordination and examine their effects on silos: informal or semi-formal coordination where administrative elites and professionals use quid pro quos to overcome coordination problems; formal coordination where political expectations, directions and monitoring may mitigate problems; and remedial po",
      "error": null
    },
    "GRAVEL2023226": {
      "url": "https://doi.org/10.1016/j.mcpdig.2023.05.004",
      "official_url": null,
      "checked_at": "2026-02-27T22:29:49.340345+00:00",
      "http_status": 200,
      "final_url": "https://linkinghub.elsevier.com/retrieve/pii/S2949761223000366",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "Redirecting",
      "abstract": "",
      "error": null
    },
    "dunbar1993coevolution": {
      "url": "https://doi.org/10.1017/S0140525X00032325",
      "official_url": null,
      "checked_at": "2026-02-27T22:29:50.683405+00:00",
      "http_status": 200,
      "final_url": "https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/coevolution-of-neocortical-size-group-size-and-language-in-humans/4290FF4D7362511136B9A15A96E74FEF",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "Logo for Cambridge Core from Cambridge University Press. Click to return to homepage.",
      "abstract": "Group size covaries with relative neocortical volume in nonhuman primates. This regression equation predicts a group size for modern humans very similar to that for hunter-gatherer and traditional horticulturalist societies. Similar group sizes are found in other contemporary and historical societies. Nonhuman primates maintain group cohesion through social grooming; among the Old World monkeys and apes, social grooming time is linearly related to group size. Maintaining stability of human-sized groups by grooming alone would make intolerable time demands. It is therefore suggested (1) that the evolution of large groups in the human lineage depended on developing a more efficient method for time-sharing the processes of social bonding and (2) that language uniquely fulfills this requirement. Data on the size of conversational and other small interacting groups of humans accord with the predicted relative efficiency of conversation compared to grooming as a bonding process. In human con",
      "error": null
    },
    "lecun2015deep": {
      "url": "https://doi.org/10.1038/nature14539",
      "official_url": "https://www.nature.com/articles/nature14539?error=server_error",
      "checked_at": "2026-02-27T22:44:19.024356+00:00",
      "http_status": 200,
      "final_url": "https://www.nature.com/articles/nature14539?error=server_error",
      "accessible": true,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "Close banner",
      "abstract": "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.",
      "error": null
    },
    "summerfield2025impact": {
      "url": "https://doi.org/10.1038/s41562-025-02309-z",
      "official_url": "https://doi.org/10.1038/s41562-025-02309-z",
      "checked_at": "2026-02-27T22:44:26.172522+00:00",
      "http_status": null,
      "final_url": null,
      "accessible": false,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": "Timeout after 15 seconds"
    },
    "kaissis_2020": {
      "url": "https://doi.org/10.1038/s42256-020-0186-1",
      "official_url": "https://www.nature.com/articles/s42256-020-0186-1?error=server_error",
      "checked_at": "2026-02-27T22:30:25.913638+00:00",
      "http_status": 200,
      "final_url": "https://www.nature.com/articles/s42256-020-0186-1?error=server_error",
      "accessible": true,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "Close banner",
      "abstract": "Medical imaging data is often subject to privacy and intellectual property restrictions. AI techniques can help out by offering tools like federated learning to bridge the gap between personal data protection and data utilisation for research and clinical routine, but these tools need to be secure.",
      "error": null
    },
    "granovetter1973strength": {
      "url": "https://doi.org/10.1086/225469",
      "official_url": "https://doi.org/10.1086/225469",
      "checked_at": "2026-02-27T22:44:42.602952+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Forbidden"
    },
    "health_sharing_incentives": {
      "url": "https://doi.org/10.1089/bio.2020.0037",
      "official_url": "https://doi.org/10.1089/bio.2020.0037",
      "checked_at": "2026-02-27T22:44:43.783151+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Forbidden"
    },
    "lawrence1999digital": {
      "url": "https://doi.org/10.1109/2.769447",
      "official_url": "https://ieeexplore.ieee.org/document/769447/",
      "checked_at": "2026-02-27T22:30:38.509226+00:00",
      "http_status": 200,
      "final_url": "https://ieeexplore.ieee.org/document/769447/",
      "accessible": true,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "Digital libraries and autonomous citation indexing | IEEE Journals & Magazine | IEEE Xplore",
      "abstract": "The revolution the Web has brought to information dissemination is not so much due to the availability of data-huge amounts of information has long been available in libraries-but rather the improved efficiency of accessing (improved accessibility to) that information. The Web promises to make more scientific articles more easily available. By making the context of citations easily and quickly browsable, autonomous citation indexing can help to evaluate the importance of individual contributions more accurately and quickly. Digital libraries incorporating ACI can help organize scientific literature and may significantly improve the efficiency of dissemination and feedback. ACI may also help speed the transition to scholarly electronic publishing.",
      "error": null
    },
    "smpc_ml": {
      "url": "https://doi.org/10.1109/ACCESS.2024.3388992",
      "official_url": "https://ieeexplore.ieee.org/document/10498135/",
      "checked_at": "2026-02-27T22:30:39.854462+00:00",
      "http_status": 200,
      "final_url": "https://ieeexplore.ieee.org/document/10498135/",
      "accessible": true,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "Secure Multi-Party Computation for Machine Learning: A Survey | IEEE Journals & Magazine | IEEE Xplore",
      "abstract": "Machine learning is a powerful technology for extracting information from data of diverse nature and origin. As its deployment increasingly depends on data from multiple entities, ensuring privacy for these contributors becomes paramount for the integrity and fairness of machine learning endeavors. This review looks into the recent advancements in secure multi-party computation (SMPC) for machine learning, a pivotal technology championing data privacy. We evaluate these applications from various aspects, including security models, requirements, system types, and service models, aligning with the IEEE’s recommended practices for SMPC. Broadly, SMPC systems are divided into two categories: homomorphic-based systems, which facilitate computations on encrypted data, ensuring data remains confidential, and secret sharing-based systems, which disseminate data across parties in fragmented shares. Our literature analysis highlights certain gaps, such as security requisites, streamlined informa",
      "error": null
    },
    "lobo2023right": {
      "url": "https://doi.org/10.1109/CAI54212.2023.00085",
      "official_url": "https://ieeexplore.ieee.org/document/10195023/",
      "checked_at": "2026-02-27T22:30:41.247766+00:00",
      "http_status": 200,
      "final_url": "https://ieeexplore.ieee.org/document/10195023/",
      "accessible": true,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "The Right to Be Forgotten in Artificial Intelligence: Issues, Approaches, Limitations and Challenges | IEEE Conference Publication | IEEE Xplore",
      "abstract": "The Right To Be Forgotten is widely conceived as a fundamental principle of the human being. It has become a subject of capital importance in domains where sensitive information is collected from individuals, requiring the provision of monitoring, governance and audit tools to control where such information is used. Artificial Intelligence models are not an exception to this statement: since they are learned from data, this fundamental right should allow individuals to have their personal information erased from AI-based systems. However, the application of this right is not straightforward: what does erasing mean in the context of a model learned from data? Is it just a matter of removing the concerned data and retraining the models? This manuscript provides a brief overview of these and more issues, proposing a desiderata for technical advances noted in this direction, and outlining research directions for prospective studies.",
      "error": null
    },
    "bogdanov2014input": {
      "url": "https://doi.org/10.1109/CSF.2014.21",
      "official_url": "https://ieeexplore.ieee.org/document/6957111/",
      "checked_at": "2026-02-27T22:30:42.612598+00:00",
      "http_status": 200,
      "final_url": "https://ieeexplore.ieee.org/document/6957111/",
      "accessible": true,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "From Input Private to Universally Composable Secure Multi-party Computation Primitives | IEEE Conference Publication | IEEE Xplore",
      "abstract": "Secure multi-party computation systems are commonly built from a small set of primitive components. The compos ability of security notions has a central role in the analysis of such systems, as it allows us to deduce security properties of complex protocols from the properties of its components. We show that the standard notions of universally compos able security are overly restrictive in this context and can lead to protocols with sub-optimal performance. As a remedy, we introduce a weaker notion of privacy that is satisfied by simpler protocols and is preserved by composition. After that we fix a passive security model and show how to convert a private protocol into a universally compos able protocol. As a result, we obtain modular security proofs without performance penalties.",
      "error": null
    },
    "Yao1982ProtocolsFS": {
      "url": "https://doi.org/10.1109/SFCS.1982.88",
      "official_url": null,
      "checked_at": "2026-02-27T22:30:43.922704+00:00",
      "http_status": 200,
      "final_url": "https://www.computer.org/csdl/proceedings-article/focs/1982/542800160/12OmNyUnEJP",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "CSDL | IEEE Computer Society",
      "abstract": "",
      "error": null
    },
    "synthetic_data_privacy": {
      "url": "https://doi.org/10.1109/SmartNets58706.2023.10215825",
      "official_url": "https://ieeexplore.ieee.org/document/10215825/",
      "checked_at": "2026-02-27T22:30:45.731867+00:00",
      "http_status": 200,
      "final_url": "https://ieeexplore.ieee.org/document/10215825/",
      "accessible": true,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "Leveraging Generative AI Models for Synthetic Data Generation in Healthcare: Balancing Research and Privacy | IEEE Conference Publication | IEEE Xplore",
      "abstract": "The widespread adoption of electronic health records and digital healthcare data has created a demand for data-driven insights to enhance patient outcomes, diagnostics, and treatments. However, using real patient data presents privacy and regulatory challenges, including compliance with HIPAA [1] and GDPR [2]. Synthetic data generation, using generative AI models like GANs [3] and VAEs [4], offers a promising solution to balance valuable data access and patient privacy protection. In this paper, we examine generative AI models for creating realistic, anonymized patient data for research and training [5], explore synthetic data applications in healthcare, and discuss its benefits, challenges, and future research directions. Synthetic data has the potential to revolutionize healthcare by providing anonymized patient data while preserving privacy and enabling versatile applications.",
      "error": null
    },
    "samuelson2023generative": {
      "url": "https://doi.org/10.1126/science.adi0656",
      "official_url": "https://doi.org/10.1126/science.adi0656",
      "checked_at": "2026-02-27T22:44:46.129061+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Forbidden"
    },
    "goldwasser1989knowledge": {
      "url": "https://doi.org/10.1137/0218012",
      "official_url": "https://doi.org/10.1137/0218012",
      "checked_at": "2026-02-27T22:44:47.309962+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Forbidden"
    },
    "hochreiter1998vanishing": {
      "url": "https://doi.org/10.1142/S0218488598000094",
      "official_url": "https://doi.org/10.1142/S0218488598000094",
      "checked_at": "2026-02-27T22:44:48.536234+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Forbidden"
    },
    "gentry2009fully": {
      "url": "https://doi.org/10.1145/1536414.1536440",
      "official_url": "https://doi.org/10.1145/1536414.1536440",
      "checked_at": "2026-02-27T22:44:49.718544+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "abstract_only",
      "source_type": "book",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Forbidden"
    },
    "goldreich1987towards": {
      "url": "https://doi.org/10.1145/28395.28420",
      "official_url": "https://doi.org/10.1145/28395.28420",
      "checked_at": "2026-02-27T22:44:50.931076+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Forbidden"
    },
    "shamir1979share": {
      "url": "https://doi.org/10.1145/359168.359176",
      "official_url": "https://doi.org/10.1145/359168.359176",
      "checked_at": "2026-02-27T22:44:52.174765+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Forbidden"
    },
    "zuccon_hallucination_attribution": {
      "url": "https://doi.org/10.1145/3624918.3625329",
      "official_url": "https://doi.org/10.1145/3624918.3625329",
      "checked_at": "2026-02-27T22:44:53.833647+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Forbidden"
    },
    "gpu_shortage": {
      "url": "https://doi.org/10.1145/3642970.3655843",
      "official_url": "https://doi.org/10.1145/3642970.3655843",
      "checked_at": "2026-02-27T22:44:54.981522+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Forbidden"
    },
    "ssi": {
      "url": "https://doi.org/10.1145/4372.4373",
      "official_url": "https://doi.org/10.1145/4372.4373",
      "checked_at": "2026-02-27T22:44:56.194671+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Forbidden"
    },
    "haveliwala2002topicsensitive": {
      "url": "https://doi.org/10.1145/511446.511513",
      "official_url": "https://doi.org/10.1145/511446.511513",
      "checked_at": "2026-02-27T22:44:57.340906+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Forbidden"
    },
    "rubin1993statistical": {
      "url": "https://www.scb.se/contentassets/ca21efb41fee47d293bbee5bf7be7fb3/discussion-statistical-disclosure-limitation.pdf",
      "official_url": null,
      "checked_at": "2026-02-27T22:33:44.098570+00:00",
      "http_status": 200,
      "final_url": "https://www.scb.se/contentassets/ca21efb41fee47d293bbee5bf7be7fb3/discussion-statistical-disclosure-limitation.pdf",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": null
    },
    "vaccari_deepfakes_2020": {
      "url": "https://doi.org/10.1177/2056305120903408",
      "official_url": "https://doi.org/10.1177/2056305120903408",
      "checked_at": "2026-02-27T22:44:58.536402+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Forbidden"
    },
    "ascoli2015sharing": {
      "url": "https://doi.org/10.1371/journal.pbio.1002275",
      "official_url": null,
      "checked_at": "2026-02-27T22:31:00.185966+00:00",
      "http_status": 200,
      "final_url": "https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002275",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "Sharing Neuron Data: Carrots, Sticks, and Digital Records | PLOS Biology",
      "abstract": "Routine data sharing is greatly benefiting several scientific disciplines, such as molecular biology, particle physics, and astronomy. Neuroscience data, in contrast, are still rarely shared, greatly limiting the potential for secondary discovery and the acceleration of research progress. Although the attitude toward data sharing is non-uniform across neuroscience subdomains, widespread adoption of data sharing practice will require a cultural shift in the community. Digital reconstructions of axonal and dendritic morphology constitute a particularly “sharable” kind of data. The popularity of the public repository NeuroMorpho.Org demonstrates that data sharing can benefit both users and contributors. Increased data availability is also catalyzing the grassroots development and spontaneous integration of complementary resources, research tools, and community initiatives. Even in this rare successful subfield, however, more data are still unshared than shared. Our experience as developer",
      "error": null
    },
    "fecher_2015_what": {
      "url": "https://doi.org/10.1371/journal.pone.0118053",
      "official_url": null,
      "checked_at": "2026-02-27T22:31:01.865646+00:00",
      "http_status": 200,
      "final_url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118053",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "What Drives Academic Data Sharing? | PLOS One",
      "abstract": "Despite widespread support from policy makers, funding agencies, and scientific journals, academic researchers rarely make their research data available to others. At the same time, data sharing in research is attributed a vast potential for scientific progress. It allows the reproducibility of study results and the reuse of old data for new research questions. Based on a systematic review of 98 scholarly papers and an empirical survey among 603 secondary data users, we develop a conceptual framework that explains the process of data sharing from the primary researcher’s point of view. We show that this process can be divided into six descriptive categories: Data donor, research organization, research community, norms, data infrastructure, and data recipients. Drawing from our findings, we discuss theoretical implications regarding knowledge creation and dissemination as well as research policy measures to foster academic collaboration. We conclude that research data cannot be regarded",
      "error": null
    },
    "dwork2014algorithmic": {
      "url": "https://doi.org/10.1561/0400000042",
      "official_url": "https://doi.org/10.1561/0400000042",
      "checked_at": "2026-02-27T22:44:59.679676+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Forbidden"
    },
    "roozbahani2025review": {
      "url": "https://doi.org/10.22034/kes.2025.2049560.1042",
      "official_url": null,
      "checked_at": "2026-02-27T22:31:04.557831+00:00",
      "http_status": 200,
      "final_url": "https://kes.hmu.ac.ir/article_722552.html",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "A Review of Methods for Reducing Hallucinations in Generative Artificial Intelligence to Enhance Knowledge Economy",
      "abstract": "Generative Artificial Intelligence (AI) models, such as large language models, are transforming various sectors of the knowledge economy, including education, research, development, and data analysis, due to their ability to generate new contents. However, hallucination, which refers to the generation of content that appears plausible but lacks scientific basis, presents a significant challenge to the safe adoption of this technology in critical applications such as financial analysis and market prediction. This study aims to explore the effects of hallucinations on productivity of the knowledge economy and proposes some approaches to mitigate them. Through conducting a systematic literature review and qualitative analysis, different types of hallucinations in generative AI models are identified, and their effects on trust and productivity in knowledge-based systems are examined. The review of hallucination reduction methods indicates that the approaches utilizing reinforcement learnin",
      "error": null
    },
    "burt2003social": {
      "url": "https://doi.org/10.1093/oso/9780195159509.003.0006",
      "official_url": "https://doi.org/10.1093/oso/9780195159509.003.0006",
      "checked_at": "2026-02-27T22:44:44.971415+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Forbidden"
    },
    "wang_2020_selfsovereign": {
      "url": "https://doi.org/10.3389/fbloc.2019.00028",
      "official_url": null,
      "checked_at": "2026-02-27T22:31:06.268760+00:00",
      "http_status": 200,
      "final_url": "https://www.frontiersin.org/journals/blockchain/articles/10.3389/fbloc.2019.00028/full",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "Frontiers | Self-Sovereign Identity in a Globalized World: Credentials-Based Identity Systems as a Driver for Economic Inclusion",
      "abstract": "After introducing key concepts and definitions in the field of digital identity, this paper will investigate the benefits and drawbacks of existing identity systems on the road towards achieving self-sovereign identity. It will explore, in particular,  the use of blockchain technology and biometrics as a means to ensure the “unicity” and “singularity” of identities, and the associated challenges pertaining to the security and confidentiality of personal information. The paper will then propose a model of blockchain-based self-sovereign identity based on attestations, claims, credentials and permissions, which is globally portable across the life of an individual. Such a system is not dependent on any particular government or organization for administration or legitimacy, although it might include government issued identification and biometrics as one of many indicia of identity. Such a solution based on a recorded and signed digital history of actions is a system that best approximates",
      "error": null
    },
    "global_cancer_screening": {
      "url": "https://doi.org/10.3389/fonc.2022.1023714",
      "official_url": null,
      "checked_at": "2026-02-27T22:31:08.655369+00:00",
      "http_status": 200,
      "final_url": "https://www.frontiersin.org/journals/oncology/articles/10.3389/fonc.2022.1023714/full",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "Frontiers | Look how far we have come: BREAST cancer detection education on the international stage",
      "abstract": "The development of screening mammography over 30 years has remarkedly reduced breast cancer–associated mortality by 20%-30% through detection of small cancer lesions at early stages. Yet breast screening programmes may function differently in each nation depending on the incidence rate, national legislation, local health infrastructure and training opportunities including feedback on performance. Mammography has been the frontline breast cancer screening tool for several decades; however, it is estimated that there are 15% to 35% of cancers missed on screening which are owing to perceptual and decision-making errors by radiologists and other readers.  Furthermore, mammography screening is not available in all countries and the increased speed in the number of new breast cancer cases among less developed countries exceeds that of the developed world in recent decades. Studies conducted through the BreastScreen Reader Assessment Strategy (BREAST) training tools for breast screening reade",
      "error": null
    },
    "mider_osint_2024": {
      "url": "https://doi.org/10.4467/20801335PBW.24.030.20807",
      "official_url": null,
      "checked_at": "2026-02-27T22:31:10.877973+00:00",
      "http_status": 200,
      "final_url": "https://ejournals.eu/en/journal/przeglad-bezpieczenstwa-wewnetrznego/article/open-source-intelligence-on-the-internet-categorisation-and-evaluation-of-search-tools",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "Sroll to top",
      "abstract": "Open source intelligence on the internet – categorisation and evaluation of search tools",
      "error": null
    },
    "patel2019bridging": {
      "url": "https://doi.org/10.5121/ijdms.2019.11301",
      "official_url": null,
      "checked_at": "2026-02-27T22:31:13.141680+00:00",
      "http_status": 200,
      "final_url": "https://aircconline.com/ijdms/V11N3/11319ijdms01.pdf",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": null
    },
    "reagle2010good": {
      "url": "https://doi.org/10.7551/mitpress/8051.001.0001",
      "official_url": "https://doi.org/10.7551/mitpress/8051.001.0001",
      "checked_at": "2026-02-27T22:45:00.876103+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "abstract_only",
      "source_type": "book",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Forbidden"
    },
    "bhattacharyya2023high": {
      "url": "https://doi.org/10.7759/cureus.39238",
      "official_url": null,
      "checked_at": "2026-02-27T22:31:17.473859+00:00",
      "http_status": 200,
      "final_url": "https://www.cureus.com/articles/158289-high-rates-of-fabricated-and-inaccurate-references-in-chatgpt-generated-medical-content",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "High Rates of Fabricated and Inaccurate References in ChatGPT-Generated Medical Content | Cureus",
      "abstract": "Background\n\nThe availability of large language models such as Chat Generative Pre-trained Transformer (ChatGPT, OpenAI) has enabled individuals from diverse backgrounds to access medical information. However, concerns exist about the accuracy of ChatGPT responses and the references used to generate medical content.\n\nMethods\n\nThis observational study investigated the authenticity and accuracy of references in medical articles generated by ChatGPT. ChatGPT-3.5 generated 30 short medical papers, each with at least three references, based on standardized prompts encompassing various topics and therapeutic areas. Reference authenticity and accuracy were verified by searching Medline, Google Scholar, and the Directory of Open Access Journals. The authenticity and accuracy of individual ChatGPT-generated reference elements were also determined.\n\nResults\n\nOverall, 115 references were generated by ChatGPT, with a mean of 3.8±1.1 per paper. Among these references, 47% were fabricated, 46% were a",
      "error": null
    },
    "data_sharing_doesnt_happen": {
      "url": "https://web.archive.org/web/20210421105442/http://blogs.nature.com/naturejobs/2015/09/21/data-sharing-why-it-doesnt-happen/",
      "official_url": null,
      "checked_at": "2026-02-27T22:45:20.487275+00:00",
      "http_status": 200,
      "final_url": "https://web.archive.org/web/20210421105442/http://blogs.nature.com/naturejobs/2015/09/21/data-sharing-why-it-doesnt-happen/",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "Data sharing: Why it doesn’t happen : Naturejobs Blog",
      "abstract": "",
      "error": null
    },
    "page1999pagerank": {
      "url": "http://ilpubs.stanford.edu:8090/422/",
      "official_url": null,
      "checked_at": "2026-02-27T22:45:02.039183+00:00",
      "http_status": null,
      "final_url": null,
      "accessible": false,
      "access_type": "unavailable",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": "Timeout after 15 seconds"
    },
    "sutton2019bitter": {
      "url": "http://www.incompleteideas.net/IncIdeas/BitterLesson.html",
      "official_url": null,
      "checked_at": "2026-02-27T22:31:36.926144+00:00",
      "http_status": 200,
      "final_url": "http://www.incompleteideas.net/IncIdeas/BitterLesson.html",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "The Bitter Lesson",
      "abstract": "",
      "error": null
    },
    "dziri_origin_2022": {
      "url": "https://aclanthology.org/2022.naacl-main.387",
      "official_url": null,
      "checked_at": "2026-02-27T22:31:38.091220+00:00",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2022.naacl-main.387/",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "On the Origin of Hallucinations in Conversational Models: Is it the Datasets or the Models? - ACL Anthology",
      "abstract": "Nouha Dziri, Sivan Milton, Mo Yu, Osmar Zaiane, Siva Reddy. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
      "error": null
    },
    "yu-etal-2024-mechanistic": {
      "url": "https://aclanthology.org/2024.findings-emnlp.466/",
      "official_url": null,
      "checked_at": "2026-02-27T22:31:39.348767+00:00",
      "http_status": 200,
      "final_url": "https://aclanthology.org/2024.findings-emnlp.466/",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations - ACL Anthology",
      "abstract": "Lei Yu, Meng Cao, Jackie CK Cheung, Yue Dong. Findings of the Association for Computational Linguistics: EMNLP 2024. 2024.",
      "error": null
    },
    "ecFranceAI2024": {
      "url": "https://ai-watch.ec.europa.eu/countries/france/france-ai-strategy-report_en",
      "official_url": null,
      "checked_at": "2026-02-27T22:31:40.493358+00:00",
      "http_status": 200,
      "final_url": "https://ai-watch.ec.europa.eu/countries/france/france-ai-strategy-report_en",
      "accessible": true,
      "access_type": "open",
      "source_type": "policy",
      "page_title": "France AI Strategy Report - AI Watch - European Commission",
      "abstract": "France AI Strategy Report",
      "error": null
    },
    "ecGermanyAI2024": {
      "url": "https://ai-watch.ec.europa.eu/countries/germany/germany-ai-strategy-report_en",
      "official_url": null,
      "checked_at": "2026-02-27T22:31:42.258669+00:00",
      "http_status": 200,
      "final_url": "https://ai-watch.ec.europa.eu/countries/germany/germany-ai-strategy-report_en",
      "accessible": true,
      "access_type": "open",
      "source_type": "policy",
      "page_title": "Germany AI Strategy Report - AI Watch - European Commission",
      "abstract": "Germany AI Strategy Report",
      "error": null
    },
    "googleAIPrinciples2024": {
      "url": "https://ai.google/responsibility/principles/",
      "official_url": null,
      "checked_at": "2026-02-27T22:31:43.947736+00:00",
      "http_status": 200,
      "final_url": "https://ai.google/principles/",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "AI Principles — Google AI",
      "abstract": "A guiding framework for our responsible development and use of AI, alongside transparency and accountability in our AI development process.",
      "error": null
    },
    "meta2024llama": {
      "url": "https://ai.meta.com/blog/meta-llama-3-1/",
      "official_url": null,
      "checked_at": "2026-02-27T22:45:18.127226+00:00",
      "http_status": 400,
      "final_url": null,
      "accessible": false,
      "access_type": "unavailable",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 400: Bad Request"
    },
    "russell1995modern": {
      "url": "https://aima.cs.berkeley.edu/",
      "official_url": null,
      "checked_at": "2026-02-27T22:31:46.789301+00:00",
      "http_status": 200,
      "final_url": "https://aima.cs.berkeley.edu/",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "Artificial Intelligence: A Modern Approach, 4th US ed.",
      "abstract": "",
      "error": null
    },
    "aiNowAlgorithmicAccountability2024": {
      "url": "https://ainowinstitute.org/publications/algorithmic-accountability-policy-toolkit",
      "official_url": null,
      "checked_at": "2026-02-27T22:31:48.018763+00:00",
      "http_status": 200,
      "final_url": "https://ainowinstitute.org/publications/algorithmic-accountability-policy-toolkit",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "Algorithmic Accountability Policy Toolkit - AI Now Institute",
      "abstract": "The toolkit includes a breakdown of key concepts and questions, an overview of existing research, summaries of algorithmic systems currently used in government, and guidances on advocacy strategies to identify and interrogate the use of these systems.",
      "error": null
    },
    "obrien2025reddit": {
      "url": "https://apnews.com/article/reddit-perplexity-ai-copyright-scraping-lawsuit-3ad8968550dd7e11bcd285a74fb6e2ff",
      "official_url": "https://apnews.com/article/reddit-perplexity-ai-copyright-scraping-lawsuit-3ad8968550dd7e11bcd285a74fb6e2ff",
      "checked_at": "2026-02-27T22:31:49.167433+00:00",
      "http_status": 200,
      "final_url": "https://apnews.com/article/reddit-perplexity-ai-copyright-scraping-lawsuit-3ad8968550dd7e11bcd285a74fb6e2ff",
      "accessible": true,
      "access_type": "paywall",
      "source_type": "news",
      "page_title": "Reddit sues Perplexity, others for user comment scraping | AP News",
      "abstract": "Reddit has sued Perplexity AI and three other entities for allegedly scraping user comments for commercial gain.",
      "error": null
    },
    "bernays1928propaganda": {
      "url": "https://archive.org/details/EdwardBernaysPropaganda1928Liveright",
      "official_url": "https://archive.org/details/EdwardBernaysPropaganda1928Liveright",
      "checked_at": "2026-02-27T22:31:50.362276+00:00",
      "http_status": 200,
      "final_url": "https://archive.org/details/EdwardBernaysPropaganda1928Liveright",
      "accessible": true,
      "access_type": "book",
      "source_type": "book",
      "page_title": "Donate icon",
      "abstract": "Edward Bernays - Propaganda",
      "error": null
    },
    "internet_archive_donation": {
      "url": "https://archive.org/donate/",
      "official_url": null,
      "checked_at": "2026-02-27T22:31:51.943532+00:00",
      "http_status": 200,
      "final_url": "https://archive.org/donate/",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "Internet Archive: Digital Library of Free & Borrowable Texts, Movies, Music & Wayback Machine",
      "abstract": "",
      "error": null
    },
    "grybauskas2023twitter": {
      "url": "https://builtin.com/articles/twitter-rate-limit-scraping",
      "official_url": null,
      "checked_at": "2026-02-27T22:31:53.198057+00:00",
      "http_status": 200,
      "final_url": "https://builtin.com/articles/twitter-rate-limit-scraping",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "Will Twitter’s New Rate Limits Really Stop Scraping? | Built In",
      "abstract": "Elon Musk recently announced controversial rate limits for non-verified Twitter users as a solution for data scraping. But will this plan work?",
      "error": null
    },
    "cross2022fake": {
      "url": "https://cbsaustin.com/news/local/up-to-30-of-online-reviews-are-fake-and-most-consumers-cant-tell-the-difference",
      "official_url": null,
      "checked_at": "2026-02-27T22:31:54.322578+00:00",
      "http_status": 200,
      "final_url": "https://cbsaustin.com/news/local/up-to-30-of-online-reviews-are-fake-and-most-consumers-cant-tell-the-difference",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "twitter icon",
      "abstract": "Fake online reviews are under attack. The Federal Trade Commission just announced it wants new rules to combat deception. The FTC says stricter guidelines and p",
      "error": null
    },
    "ecWhitePaperAI2020": {
      "url": "https://commission.europa.eu/publications/white-paper-artificial-intelligence-european-approach-excellence-and-trust_en",
      "official_url": null,
      "checked_at": "2026-02-27T22:31:55.530466+00:00",
      "http_status": 200,
      "final_url": "https://commission.europa.eu/publications/white-paper-artificial-intelligence-european-approach-excellence-and-trust_en",
      "accessible": true,
      "access_type": "open",
      "source_type": "policy",
      "page_title": "White Paper on Artificial Intelligence: a European approach to excellence and trust - European Commission",
      "abstract": "White Paper on Artificial Intelligence: a European approach to excellence and trust",
      "error": null
    },
    "harvardPrincipledAI2020": {
      "url": "https://cyber.harvard.edu/publication/2020/principled-ai",
      "official_url": null,
      "checked_at": "2026-02-27T22:31:57.501614+00:00",
      "http_status": 200,
      "final_url": "https://cyber.harvard.edu/publication/2020/principled-ai",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "icon-soundcloud",
      "abstract": "Comparing the contents of thirty-six prominent AI principles documents side-by-side",
      "error": null
    },
    "weights2025sweeps": {
      "url": "https://docs.wandb.ai/models/tutorials/sweeps",
      "official_url": null,
      "checked_at": "2026-02-27T22:31:58.885820+00:00",
      "http_status": 200,
      "final_url": "https://docs.wandb.ai/models/integrations",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "Integrations overview - Weights & Biases Documentation",
      "abstract": "Explore W&B integrations with ML frameworks, cloud platforms, and workflow orchestration tools",
      "error": null
    },
    "educating_silicon_2024": {
      "url": "https://www.educatingsilicon.com/2024/05/09/how-much-llm-training-data-is-there-in-the-limit/",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:49.476959+00:00",
      "http_status": 200,
      "final_url": "https://www.educatingsilicon.com/2024/05/09/how-much-llm-training-data-is-there-in-the-limit/",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "How much LLM training data is there, in the limit? – Educating Silicon",
      "abstract": "",
      "error": null
    },
    "wikipedia_commoncrawl_2024": {
      "url": "https://en.wikipedia.org/wiki/Common_Crawl",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:00.534608+00:00",
      "http_status": 200,
      "final_url": "https://en.wikipedia.org/wiki/Common_Crawl",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "Common Crawl Foundation - Wikipedia",
      "abstract": "",
      "error": null
    },
    "sevilla2024training": {
      "url": "https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:01.727544+00:00",
      "http_status": 200,
      "final_url": "https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "Training compute of frontier AI models grows by 4-5x per year | Epoch AI",
      "abstract": "Our expanded AI model database shows that training compute grew 4-5x/year from 2010 to 2024, with similar trends in frontier and large language models.",
      "error": null
    },
    "epoch2025openaicomputespend": {
      "url": "https://epoch.ai/data-insights/openai-compute-spend",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:03.035044+00:00",
      "http_status": 200,
      "final_url": "https://epoch.ai/data-insights/openai-compute-spend",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "Most of OpenAI’s 2024 compute went to experiments | Epoch AI",
      "abstract": "OpenAI spent ~$7B on cloud compute last year—mostly R&D (research, experiments, training). Only a small share went to final model training.",
      "error": null
    },
    "EpochAIModels2025": {
      "url": "https://epoch.ai/data/ai-models",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:04.221639+00:00",
      "http_status": 200,
      "final_url": "https://epoch.ai/data/ai-models",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "Data on AI Models | Epoch AI",
      "abstract": "Our public database, the largest of its kind, tracks over 3200 machine learning models from 1950 to today. Explore data and graphs showing the trajectory of AI.",
      "error": null
    },
    "epoch2024hardware": {
      "url": "https://epoch.ai/data/machine-learning-hardware",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:05.448444+00:00",
      "http_status": 200,
      "final_url": "https://epoch.ai/data/machine-learning-hardware",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "Data on Machine Learning Hardware | Epoch AI",
      "abstract": "We present key data on over 170 AI accelerators, such as graphics processing units (GPUs) and tensor processing units (TPUs), used to develop and deploy machine learning models in the deep learning era.",
      "error": null
    },
    "costan2016intel": {
      "url": "https://eprint.iacr.org/2016/086",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:06.695373+00:00",
      "http_status": 200,
      "final_url": "https://eprint.iacr.org/2016/086",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "Intel SGX Explained",
      "abstract": "Intel's Software Guard Extensions (SGX) is a set of extensions to the Intel architecture that aims to provide integrity and privacy guarantees to security-sensitive computation performed on a computer where all the privileged software (kernel, hypervisor, etc) is potentially malicious.\n\nThis paper analyzes Intel SGX, based on the 3 papers that introduced it, on the Intel Software Developer's Manual (which supersedes the SGX manuals), on an ISCA 2015 tutorial, and on two patents. We use the papers, reference manuals, and tutorial as primary data sources, and only draw on the patents to fill in missing information.\n\nThis paper's contributions are a summary of the Intel-specific architectural and micro-architectural details needed to understand SGX, a detailed and structured presentation of the publicly available information on SGX, a series of intelligent guesses about some important but undocumented aspects of SGX, and an analysis of SGX's security properties.",
      "error": null
    },
    "howley2023nvidia": {
      "url": "https://finance.yahoo.com/news/theres-an-ai-war-and-nvidia-is-the-only-arms-dealer-analyst-174654030.html",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:08.067190+00:00",
      "http_status": 200,
      "final_url": "https://finance.yahoo.com/news/theres-an-ai-war-and-nvidia-is-the-only-arms-dealer-analyst-174654030.html",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "There’s an AI war, and Nvidia is the only arms dealer: Analyst",
      "abstract": "Shares of Nvidia ripped higher Thursday, as the company rides the AI wave.",
      "error": null
    },
    "goldhaber1997attention": {
      "url": "https://doi.org/10.5210/fm.v2i4.519",
      "official_url": null,
      "checked_at": "2026-02-27T22:31:14.490028+00:00",
      "http_status": 200,
      "final_url": "https://firstmonday.org/ojs/index.php/fm/article/view/519",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": null
    },
    "fortune2014klout": {
      "url": "https://fortune.com/2014/03/26/klout-acquired-for-200-million-by-lithium-technologies/",
      "official_url": "https://fortune.com/2014/03/26/klout-acquired-for-200-million-by-lithium-technologies/",
      "checked_at": "2026-02-27T22:32:09.813422+00:00",
      "http_status": 200,
      "final_url": "https://fortune.com/2014/03/26/klout-acquired-for-200-million-by-lithium-technologies/",
      "accessible": true,
      "access_type": "paywall",
      "source_type": "news",
      "page_title": "Pinterest icon",
      "abstract": "It's official: The once-controversial social scoring startup has been scooped up.",
      "error": null
    },
    "pope2023centralizing": {
      "url": "https://forum.effectivealtruism.org/posts/zd5inbT4kYKivincm/ai-is-centralizing-by-default-let-s-not-make-it-worse",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:11.169120+00:00",
      "http_status": 200,
      "final_url": "https://forum.effectivealtruism.org/posts/zd5inbT4kYKivincm/ai-is-centralizing-by-default-let-s-not-make-it-worse",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "AI is centralizing by default; let's not make it worse — EA Forum",
      "abstract": "TL;DR: • AIs will probably be much easier to control than humans due to (1) AIs having far more levers through which to exert control, (2) AIs having…",
      "error": null
    },
    "futureOfLifeAIPrinciples2024": {
      "url": "https://futureoflife.org/open-letter/ai-principles/",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:12.621758+00:00",
      "http_status": 200,
      "final_url": "https://futureoflife.org/open-letter/ai-principles/",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "instagram",
      "abstract": "At a 2017 FLI conference, AI scientists and researchers developed the highly influential Asilomar AI governance principles. Add your signature.",
      "error": null
    },
    "riesen2023imagine": {
      "url": "https://futureoflife.org/podcast/imagine-a-world-what-if-global-challenges-led-to-more-centralization/",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:13.878261+00:00",
      "http_status": 200,
      "final_url": "https://futureoflife.org/podcast/imagine-a-world-what-if-global-challenges-led-to-more-centralization/",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "instagram",
      "abstract": "",
      "error": null
    },
    "cummings2017democracy": {
      "url": "https://global.oup.com/academic/product/democracy-of-sound-9780190625009",
      "official_url": "https://global.oup.com/academic/product/democracy-of-sound-9780190625009",
      "checked_at": "2026-02-27T22:32:15.010677+00:00",
      "http_status": 202,
      "final_url": "https://global.oup.com/academic/product/democracy-of-sound-9780190625009",
      "accessible": true,
      "access_type": "book",
      "source_type": "book",
      "page_title": "",
      "abstract": "",
      "error": null
    },
    "simmons2022gdpr": {
      "url": "https://insights.comforte.com/countries-with-gdpr-like-data-privacy-laws",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:16.417437+00:00",
      "http_status": 200,
      "final_url": "https://insights.comforte.com/countries-with-gdpr-like-data-privacy-laws",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "17 Countries with GDPR-like Data Privacy Laws",
      "abstract": "GDPR wasn't the first data privacy law of its kind and it won't be the last. For a quick overview, here's a list of 17 countries with GDPR-like laws. ",
      "error": null
    },
    "grow2025zuckerberg": {
      "url": "https://medium.com/@Gbgrow/the-zuckerberg-lecun-ai-paradox-a-tale-of-two-visions-811ded5d2298",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:17.578855+00:00",
      "http_status": 200,
      "final_url": "https://medium.com/@Gbgrow/the-zuckerberg-lecun-ai-paradox-a-tale-of-two-visions-811ded5d2298",
      "accessible": true,
      "access_type": "open",
      "source_type": "blog",
      "page_title": "The Zuckerberg-LeCun AI Paradox: A Tale of Two Visions | by Jacob Grow | Medium",
      "abstract": "How Meta’s CEO and Chief AI Scientist are telling very different stories about superintelligence",
      "error": null
    },
    "chomsky2014aspects": {
      "url": "https://mitpress.mit.edu/9780262527408/aspects-of-the-theory-of-syntax/",
      "official_url": "https://mitpress.mit.edu/9780262527408/aspects-of-the-theory-of-syntax/",
      "checked_at": "2026-02-27T22:45:19.346879+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "book",
      "source_type": "book",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Forbidden"
    },
    "montrealDeclaration2024": {
      "url": "https://montrealdeclaration-responsibleai.com/",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:20.197151+00:00",
      "http_status": 200,
      "final_url": "https://montrealdeclaration-responsibleai.com/",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "Montréal Declaration on Responsible AI",
      "abstract": "Signatories to the declaration The Montreal Declaration is a collective effort to put the development of AI at the service of everyone's well-being.",
      "error": null
    },
    "openai2024learning": {
      "url": "https://openai.com/index/learning-to-reason-with-llms",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:21.560918+00:00",
      "http_status": 200,
      "final_url": "https://openai.com/index/learning-to-reason-with-llms/",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "Learning to reason with LLMs | OpenAI",
      "abstract": "We are introducing OpenAI o1, a new large language model trained with reinforcement learning to perform complex reasoning. o1 thinks before it answers—it can produce a long internal chain of thought before responding to the user.",
      "error": null
    },
    "osi_osaid_2024": {
      "url": "https://opensource.org/ai/open-source-ai-definition",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:24.283184+00:00",
      "http_status": 200,
      "final_url": "https://opensource.org/ai/open-source-ai-definition",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "The Open Source AI Definition - 1.0 - Open Source Initiative",
      "abstract": "version 1.0 Preamble Why we need Open Source Artificial Intelligence (AI) Open Source has demonstrated that massive benefits accrue to everyone after removing the barriers to learning, using, sharing and…",
      "error": null
    },
    "vidal2024compelling": {
      "url": "https://opensource.org/blog/compelling-responses-to-ntias-ai-open-model-weights-rfc",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:25.467515+00:00",
      "http_status": 200,
      "final_url": "https://opensource.org/blog/compelling-responses-to-ntias-ai-open-model-weights-rfc",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "Compelling responses to NTIA’s AI Open Model Weights RFC - Open Source Initiative",
      "abstract": "The OSI has compiled a compelling list of responses from nonprofit organizations and companies to NTIA's AI Open Model Weights RFC.",
      "error": null
    },
    "krizhevsky2012imagenet": {
      "url": "https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:26.584982+00:00",
      "http_status": 200,
      "final_url": "https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "ImageNet Classification with Deep Convolutional Neural Networks",
      "abstract": "",
      "error": null
    },
    "bratt2025inference": {
      "url": "https://partners.wsj.com/nasdaq-arm/information-technology/why-there-is-no-ai-without-inference/",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:27.900840+00:00",
      "http_status": 200,
      "final_url": "https://partners.wsj.com/nasdaq-arm/information-technology/why-there-is-no-ai-without-inference/",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "Paid Program: Why There Is No AI Without Inference",
      "abstract": "The rise of AI inference is leading to more efficient outcomes.\r\n",
      "error": null
    },
    "mcmahan2017communication": {
      "url": "https://proceedings.mlr.press/v54/mcmahan17a.html",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:29.423579+00:00",
      "http_status": 200,
      "final_url": "https://proceedings.mlr.press/v54/mcmahan17a.html",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "Communication-Efficient Learning of Deep Networks from Decentralized Data",
      "abstract": "Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve spe...",
      "error": null
    },
    "RLHF": {
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:30.533253+00:00",
      "http_status": 200,
      "final_url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": null
    },
    "elsea2006protection": {
      "url": "https://sgp.fas.org/crs/secrecy/RS21900.pdf",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:31.843992+00:00",
      "http_status": 202,
      "final_url": "https://sgp.fas.org/crs/secrecy/RS21900.pdf",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": null
    },
    "chase2020signal": {
      "url": "https://signal.org/blog/signal-private-group-system/",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:33.018292+00:00",
      "http_status": 200,
      "final_url": "https://signal.org/blog/signal-private-group-system/",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "Signal >> Blog >> Technology Preview: Signal Private Group System",
      "abstract": "Groups are inherently social, and Signal is a social app. Whether you’re planning a surprise party, discussing last night’s book club meeting, exchanging photos with your family, or organizing something important, group messaging has always been a key feature of Signal. Signal provides private gr...",
      "error": null
    },
    "slate2018klout": {
      "url": "https://slate.com/technology/2018/05/klout-is-dead-just-in-time-of-europes-gdpr-privacy-law-thats-not-a-coincidence.html",
      "official_url": "https://slate.com/technology/2018/05/klout-is-dead-just-in-time-of-europes-gdpr-privacy-law-thats-not-a-coincidence.html",
      "checked_at": "2026-02-27T22:32:34.160798+00:00",
      "http_status": 200,
      "final_url": "https://slate.com/technology/2018/05/klout-is-dead-just-in-time-of-europes-gdpr-privacy-law-thats-not-a-coincidence.html",
      "accessible": true,
      "access_type": "paywall",
      "source_type": "news",
      "page_title": "The Slate Group logo",
      "abstract": "Remember Klout? Well, now you can forget about it again. Just don’t expect its parent company to forget anything about you.",
      "error": null
    },
    "sovrin": {
      "url": "https://sovrin.org/the-sovrin-network-and-zero-knowledge-proofs/",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:35.713599+00:00",
      "http_status": 200,
      "final_url": "https://sovrin.org/the-sovrin-network-and-zero-knowledge-proofs/",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "The Sovrin Network and Zero Knowledge Proofs - Sovrin",
      "abstract": "Mike Lodder holds the position of Security Maven for the Sovrin Foundation and is responsible for writing protocols, developing security policies, implementing crypto, and assessing the strength of the Sovrin Network. At a recent ‘Brown Bag Meetup’ hosted at Sovrin Foundation headquarters in Provo, Utah, Mike gave a fantastic presentation about Zero Knowledge Proofs. We […]",
      "error": null
    },
    "ieeeEthicsAI2021": {
      "url": "https://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:37.857909+00:00",
      "http_status": 200,
      "final_url": "https://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "",
      "abstract": "",
      "error": null
    },
    "mehta2024zuckerberg": {
      "url": "https://techcrunch.com/2024/08/01/zuckerberg-says-meta-will-need-10x-more-computing-power-to-train-llama-4-than-llama-3/",
      "official_url": "https://techcrunch.com/2024/08/01/zuckerberg-says-meta-will-need-10x-more-computing-power-to-train-llama-4-than-llama-3/",
      "checked_at": "2026-02-27T22:32:38.989107+00:00",
      "http_status": 200,
      "final_url": "https://techcrunch.com/2024/08/01/zuckerberg-says-meta-will-need-10x-more-computing-power-to-train-llama-4-than-llama-3/",
      "accessible": true,
      "access_type": "paywall",
      "source_type": "news",
      "page_title": "Zuckerberg says Meta will need 10x more computing power to train Llama 4 than Llama 3 | TechCrunch",
      "abstract": "“The amount of computing needed to train Llama 4 will likely be almost 10x more than what we used to train Llama 3,” Zuckerberg said.",
      "error": null
    },
    "wiggers2024openai": {
      "url": "https://techcrunch.com/2024/10/31/openai-ceo-sam-altman-says-lack-of-compute-is-delaying-the-companys-products/",
      "official_url": "https://techcrunch.com/2024/10/31/openai-ceo-sam-altman-says-lack-of-compute-is-delaying-the-companys-products/",
      "checked_at": "2026-02-27T22:32:40.158031+00:00",
      "http_status": 200,
      "final_url": "https://techcrunch.com/2024/10/31/openai-ceo-sam-altman-says-lack-of-compute-is-delaying-the-companys-products/",
      "accessible": true,
      "access_type": "paywall",
      "source_type": "news",
      "page_title": "OpenAI CEO Sam Altman says lack of compute capacity is delaying the company's products | TechCrunch",
      "abstract": "OpenAI CEO Sam Altman admitted a lack of compute capacity is one factor preventing the company from shipping products as often as it'd like.",
      "error": null
    },
    "jurassic": {
      "url": "https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:41.275639+00:00",
      "http_status": 200,
      "final_url": "https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": null
    },
    "accessNowTorontoDeclaration2023": {
      "url": "https://www.accessnow.org/press-release/the-toronto-declaration-protecting-the-rights-to-equality-and-non-discrimination-in-machine-learning-systems/",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:42.378276+00:00",
      "http_status": 200,
      "final_url": "https://www.accessnow.org/press-release/the-toronto-declaration-protecting-the-rights-to-equality-and-non-discrimination-in-machine-learning-systems/",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "The Toronto Declaration: Protecting the rights to equality and non-discrimination in machine learning systems - Access Now",
      "abstract": "Open declaration by Amnesty International and Access Now, endorsed by Human Rights Watch and Wikimedia Foundation at RightsCon Toronto 2018",
      "error": null
    },
    "Kaye2025NvidiaFirstCompany": {
      "url": "https://www.bbc.co.uk/news/articles/cp8e970vn5vo",
      "official_url": "https://www.bbc.co.uk/news/articles/cp8e970vn5vo",
      "checked_at": "2026-02-27T22:32:44.045143+00:00",
      "http_status": 200,
      "final_url": "https://www.bbc.co.uk/news/articles/cp8e970vn5vo",
      "accessible": true,
      "access_type": "paywall",
      "source_type": "news",
      "page_title": "Nvidia becomes world's first $5tn company - BBC News",
      "abstract": "The US chip-maker has rapidly climbed from a niche graphics-chip manufacturer to an AI titan.",
      "error": null
    },
    "bogwasi2025business": {
      "url": "https://www.brimco.io/business-statistics-to-know/",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:45.484580+00:00",
      "http_status": 200,
      "final_url": "https://www.brimco.io/business-statistics-to-know/",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "Business Statistics You Should Know in 2026 | Brimco",
      "abstract": "Discover the key business statistics that every business owner should know. Learn what you need to know to make informed decisions and drive success.",
      "error": null
    },
    "donfro2013yelp": {
      "url": "https://www.businessinsider.com/20-percent-of-yelp-reviews-fake-2013-9",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:47.087170+00:00",
      "http_status": 200,
      "final_url": "https://www.businessinsider.com/20-percent-of-yelp-reviews-fake-2013-9",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "lighning bolt icon",
      "abstract": "A Whopping 20 Percent Of Yelp Reviews Are Fake",
      "error": null
    },
    "Goodfellow-et-al-2016": {
      "url": "https://www.deeplearningbook.org/",
      "official_url": "https://www.deeplearningbook.org/",
      "checked_at": "2026-02-27T22:32:48.387354+00:00",
      "http_status": 200,
      "final_url": "https://www.deeplearningbook.org/",
      "accessible": true,
      "access_type": "book",
      "source_type": "book",
      "page_title": "Deep Learning",
      "abstract": "",
      "error": null
    },
    "berger2025ai": {
      "url": "https://www.forbes.com/sites/virginieberger/2025/03/15/the-ai-copyright-battle-why-openai-and-google-are-pushing-for-fair-use/",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:51.798004+00:00",
      "http_status": 200,
      "final_url": "https://www.forbes.com/sites/virginieberger/2025/03/15/the-ai-copyright-battle-why-openai-and-google-are-pushing-for-fair-use/",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "The AI Copyright Battle: Why OpenAI And Google Are Pushing For Fair Use",
      "abstract": "Artificial intelligence powerhouses OpenAI and Google are aggressively lobbying the U.S. government to classify AI training on copyrighted data as \"fair use.\"",
      "error": null
    },
    "fcdo2022trollfarm": {
      "url": "https://www.gov.uk/government/news/uk-exposes-sick-russian-troll-factory-plaguing-social-media-with-kremlin-propaganda",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:53.850467+00:00",
      "http_status": 200,
      "final_url": "https://www.gov.uk/government/news/uk-exposes-sick-russian-troll-factory-plaguing-social-media-with-kremlin-propaganda",
      "accessible": true,
      "access_type": "open",
      "source_type": "policy",
      "page_title": "GOV.UK",
      "abstract": "UK-funded expert research has exposed how the Kremlin is using a troll factory to spread lies on social media and in comment sections of popular websites. ",
      "error": null
    },
    "hootsuite2024": {
      "url": "https://www.hootsuite.com/resources/digital-trends",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:55.706676+00:00",
      "http_status": 200,
      "final_url": "https://www.hootsuite.com/resources/digital-trends",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "arrow right",
      "abstract": "Get an all-encompassing overview of the latest digital marketing trends with The Global State of Digital 2022 from Hootsuite and We Are Social.",
      "error": null
    },
    "mirvish2011hathaway": {
      "url": "https://www.huffpost.com/entry/the-hathaway-effect-how-a_b_830041",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:57.018281+00:00",
      "http_status": 200,
      "final_url": "https://www.huffpost.com/entry/the-hathaway-effect-how-a_b_830041",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "The Hathaway Effect: How Anne Gives Warren Buffett a Rise | HuffPost Impact",
      "abstract": "It looks like all the automated, robotic stock trading programs are picking up the same chatter on the internet about \"Hathaway\" as the IMDb's StarMeter, and they're applying it to the stock market.",
      "error": null
    },
    "10.36227/techrxiv.171837853.31531482/v1": {
      "url": "https://www.hup.harvard.edu/books/9780674970847",
      "official_url": null,
      "checked_at": "2026-02-27T22:32:58.159785+00:00",
      "http_status": 200,
      "final_url": "https://www.hup.harvard.edu/books/9780674970847",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "The Black Box Society — Harvard University Press",
      "abstract": "Every day, corporations are connecting the dots about our personal behavior—silently scrutinizing clues left behind by our work habits and Internet use. The data compiled and portraits created are incredibly detailed, to the point of being invasive. But who connects the dots about what firms are doing with this information? The Black Box Society argues that we all need to be able to do so—and to set limits on how big data affects our lives.Hidden algorithms can make (or ruin) reputations, decide the destiny of entrepreneurs, or even devastate an entire economy. Shrouded in secrecy and complexity, decisions at major Silicon Valley and Wall Street firms were long assumed to be neutral and technical. But leaks, whistleblowers, and legal disputes have shed new light on automated judgment. Self-serving and reckless behavior is surprisingly common, and easy to hide in code protected by legal and real secrecy. Even after billions of dollars of fines have been levied, underfunded regulators ma",
      "error": null
    },
    "pasquale_2015": {
      "url": "https://www.hup.harvard.edu/books/9780674970847",
      "official_url": "https://www.hup.harvard.edu/books/9780674970847",
      "checked_at": "2026-02-27T22:32:59.348453+00:00",
      "http_status": 200,
      "final_url": "https://www.hup.harvard.edu/books/9780674970847",
      "accessible": true,
      "access_type": "book",
      "source_type": "book",
      "page_title": "The Black Box Society — Harvard University Press",
      "abstract": "Every day, corporations are connecting the dots about our personal behavior—silently scrutinizing clues left behind by our work habits and Internet use. The data compiled and portraits created are incredibly detailed, to the point of being invasive. But who connects the dots about what firms are doing with this information? The Black Box Society argues that we all need to be able to do so—and to set limits on how big data affects our lives.Hidden algorithms can make (or ruin) reputations, decide the destiny of entrepreneurs, or even devastate an entire economy. Shrouded in secrecy and complexity, decisions at major Silicon Valley and Wall Street firms were long assumed to be neutral and technical. But leaks, whistleblowers, and legal disputes have shed new light on automated judgment. Self-serving and reckless behavior is surprisingly common, and easy to hide in code protected by legal and real secrecy. Even after billions of dollars of fines have been levied, underfunded regulators ma",
      "error": null
    },
    "ibmAIPrinciples2018": {
      "url": "https://www.ibm.com/policy/wp-content/uploads/2018/06/IBM_Principles_SHORT.V4.3.pdf",
      "official_url": null,
      "checked_at": "2026-02-27T22:33:00.467033+00:00",
      "http_status": 200,
      "final_url": "https://www.ibm.com/policy",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "IBM Policy | IBM",
      "abstract": "Explore this IBM communications channel devoted to public policy and government outreach and discover how IBM works to shape responsible use of technology.",
      "error": null
    },
    "owenjackson2024opensource": {
      "url": "https://www.ibm.com/think/insights/unregulated-generative-ai-dangers-open-source",
      "official_url": null,
      "checked_at": "2026-02-27T22:33:01.971042+00:00",
      "http_status": 200,
      "final_url": "https://www.ibm.com/think/insights/unregulated-generative-ai-dangers-open-source",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "Open source, open risks: The growing dangers of unregulated generative AI | IBM",
      "abstract": "Generative AI has proved to be a game-changing tool for many organizations. But without regulation, generative AI could be the new cyber crime frontier.",
      "error": null
    },
    "imperva2024bots": {
      "url": "https://www.imperva.com/resources/resource-library/reports/bad-bot-report/",
      "official_url": null,
      "checked_at": "2026-02-27T22:33:03.178298+00:00",
      "http_status": 200,
      "final_url": "https://www.imperva.com/resources/resource-library/reports/bad-bot-report/",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "2022 Bad Bot Report | Evasive Bots Drive Online Fraud | Imperva",
      "abstract": "Bad bots have been leveraging the upsurge in online traffic due to the global pandemic. Sophisticated than ever, mimicking human behavior. Get 2022 Report.",
      "error": null
    },
    "industryAustraliaAI2024": {
      "url": "https://www.industry.gov.au/publications/australias-artificial-intelligence-ethics-principles",
      "official_url": null,
      "checked_at": "2026-02-27T22:45:22.566912+00:00",
      "http_status": null,
      "final_url": null,
      "accessible": false,
      "access_type": "unavailable",
      "source_type": "policy",
      "page_title": "",
      "abstract": "",
      "error": "Timeout after 15 seconds"
    },
    "freeman1977set": {
      "url": "https://www.jstor.org/stable/3033543",
      "official_url": "https://www.jstor.org/stable/3033543",
      "checked_at": "2026-02-27T22:33:20.562733+00:00",
      "http_status": 200,
      "final_url": "https://www.jstor.org/stable/3033543",
      "accessible": true,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "A Set of Measures of Centrality Based on Betweenness on JSTOR",
      "abstract": "Linton C. Freeman, A Set of Measures of Centrality Based on Betweenness, Sociometry, Vol. 40, No. 1 (Mar., 1977), pp. 35-41",
      "error": null
    },
    "crispweed2024alignment": {
      "url": "https://www.lesswrong.com/posts/zWJTcaJCkYiJwCmgx/the-alignment-trap-ai-safety-as-path-to-power",
      "official_url": null,
      "checked_at": "2026-02-27T22:33:22.170977+00:00",
      "http_status": 200,
      "final_url": "https://www.lesswrong.com/posts/zWJTcaJCkYiJwCmgx/the-alignment-trap-ai-safety-as-path-to-power",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "The Alignment Trap: AI Safety as Path to Power — LessWrong",
      "abstract": "Recent discussions about artificial intelligence safety have focused heavily on ensuring AI systems remain under human control. While this goal seems…",
      "error": null
    },
    "macrotrends2024nvidia": {
      "url": "https://www.macrotrends.net/stocks/charts/NVDA/nvidia/revenue",
      "official_url": null,
      "checked_at": "2026-02-27T22:33:23.733018+00:00",
      "http_status": 200,
      "final_url": "https://www.macrotrends.net/stocks/charts/NVDA/nvidia/revenue",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "NVIDIA Revenue 2012-2025 | NVDA | MacroTrends",
      "abstract": "NVIDIA annual/quarterly revenue history and growth rate from 2012 to 2025. Revenue can be defined as the amount of money a company receives from its customers in exchange for the sales of goods or services.  Revenue is the top line item on an income statement from which all costs and expenses are subtracted to arrive at net income.    \r\n\t\t\t\t\r\n\t\t\t\t<ul style='margin-top:10px;'>\r\n\t\t\t\t<li>NVIDIA revenue for the quarter ending October 31, 2025 was <strong>$57.006B</strong>, a <strong>62.49% increase</strong> year-over-year.</li>\r\n\t\t\t\t<li>NVIDIA revenue for the twelve months ending October 31, 2025 was <strong>$187.142B</strong>, a <strong>65.22% increase</strong> year-over-year.</li>\r\n\t\t\t\t<li>NVIDIA annual revenue for 2025 was <strong>$130.497B</strong>, a <strong>114.2% increase</strong> from 2024.</li>\r\n\t\t\t\t<li>NVIDIA annual revenue for 2024 was <strong>$60.922B</strong>, a <strong>125.85% increase</strong> from 2023.</li>\r\n\t\t\t\t<li>NVIDIA annual revenue for 2023 was <strong>$26.974B</stro",
      "error": null
    },
    "microsoftAIPrinciples2024": {
      "url": "https://www.microsoft.com/en-us/ai/principles-and-approach",
      "official_url": null,
      "checked_at": "2026-02-27T22:33:24.885381+00:00",
      "http_status": 200,
      "final_url": "https://www.microsoft.com/en-us/ai/principles-and-approach",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "Your Privacy Choices Opt-Out Icon",
      "abstract": "Discover Microsoft AI tools, industry-specific governance solutions, and responsible AI practices to make smarter, more informed decisions about AI implementation.",
      "error": null
    },
    "grynbaum2023times": {
      "url": "https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html",
      "official_url": null,
      "checked_at": "2026-02-27T22:45:38.793001+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "unavailable",
      "source_type": "news",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Forbidden"
    },
    "veliz_2020_privacy": {
      "url": "https://www.penguin.co.uk/books/314/314359/privacy-is-power/9780552177719.html",
      "official_url": "https://www.penguin.co.uk/books/442343/privacy-is-power-by-carissa-veliz/9780552177719",
      "checked_at": "2026-02-27T22:33:32.289934+00:00",
      "http_status": 200,
      "final_url": "https://www.penguin.co.uk/books/442343/privacy-is-power-by-carissa-veliz/9780552177719",
      "accessible": true,
      "access_type": "book",
      "source_type": "book",
      "page_title": "Penguin Random House",
      "abstract": "An Economist BEST BOOK OF THE YEAR\n\nAs the data economy grows in power, Carissa Véliz exposes how our privacy is eroded by big tech and governments, why that matters and what we can do about it.\n\nThe moment you check your phone in the morning you are giving away your data. Before you've even switched off your alarm, a whole host of organisations have been alerted to when you woke up, where you slept, and with whom. As you check the weather, scroll through your 'suggested friends' on Facebook, you continually compromise your privacy.\n\nWithout your permission, or even your awareness, tech companies are harvesting your information, your location, your likes, your habits, and sharing it amongst themselves. They're not just selling your data. They're selling the power to influence you. Even when you've explicitly asked them not to. And it's not just you. It's all your contacts too.\n\nDigital technology is stealing our personal data and with it our power to make free choices. To reclaim that ",
      "error": null
    },
    "doi:10.1073/pnas.1611835114": {
      "url": "https://www.pnas.org/doi/abs/10.1073/pnas.1611835114",
      "official_url": "https://www.pnas.org/doi/abs/10.1073/pnas.1611835114",
      "checked_at": "2026-02-27T22:45:39.956018+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "abstract_only",
      "source_type": "paper",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Forbidden"
    },
    "ai_bioweapon": {
      "url": "https://www.rand.org/pubs/research_reports/RRA2977-1.html",
      "official_url": "https://www.rand.org/pubs/research_reports/RRA2977-1.html",
      "checked_at": "2026-02-27T22:33:35.611974+00:00",
      "http_status": 200,
      "final_url": "https://www.rand.org/pubs/research_reports/RRA2977-1.html",
      "accessible": true,
      "access_type": "book",
      "source_type": "book",
      "page_title": "The Operational Risks of AI in Large-Scale Biological Attacks: A Red-Team Approach | RAND",
      "abstract": "In this report, the authors address the emerging issue of identifying and mitigating the risks posed by the misuse of artificial intelligence (AI)—specifically, large language models—in the context of biological attacks and present preliminary findings of their research. They find that while AI can generate concerning text, the operational impact is a subject for future research.",
      "error": null
    },
    "wealand2025reducing": {
      "url": "https://www.redhat.com/en/blog/reducing-bias-ai-models-through-open-source",
      "official_url": null,
      "checked_at": "2026-02-27T22:33:36.849840+00:00",
      "http_status": 200,
      "final_url": "https://www.redhat.com/en/blog/reducing-bias-ai-models-through-open-source",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "Red Hat",
      "abstract": "Explore the critical issue of Western-biased AI models and learn how open source AI can help achieve AI sovereignty, reflecting a nation's or community's unique languages, values, and culture. Discover real-world examples and Red Hat's commitment to a more equitable and diverse AI ecosystem.",
      "error": null
    },
    "ntia2024aiweights": {
      "url": "https://www.regulations.gov/document/NTIA-2023-0009-0001/comment",
      "official_url": null,
      "checked_at": "2026-02-27T22:33:38.166923+00:00",
      "http_status": 200,
      "final_url": "https://www.regulations.gov/document/NTIA-2023-0009-0001/comment",
      "accessible": true,
      "access_type": "open",
      "source_type": "policy",
      "page_title": "Regulations.gov",
      "abstract": "",
      "error": null
    },
    "kachwala2024nvidia": {
      "url": "https://www.reuters.com/technology/artificial-intelligence/nvidia-says-new-rule-will-weaken-us-leadership-ai-2025-01-13/",
      "official_url": null,
      "checked_at": "2026-02-27T22:45:41.078311+00:00",
      "http_status": 401,
      "final_url": null,
      "accessible": false,
      "access_type": "unavailable",
      "source_type": "news",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 401: HTTP Forbidden"
    },
    "Hu_Cai_2024": {
      "url": "https://www.reuters.com/technology/artificial-intelligence/openai-remove-non-profit-control-give-sam-altman-equity-sources-say-2024-09-25/",
      "official_url": null,
      "checked_at": "2026-02-27T22:45:42.292224+00:00",
      "http_status": 401,
      "final_url": null,
      "accessible": false,
      "access_type": "unavailable",
      "source_type": "news",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 401: HTTP Forbidden"
    },
    "Tong_Martina_2024": {
      "url": "https://www.reuters.com/technology/artificial-intelligence/us-government-commission-pushes-manhattan-project-style-ai-initiative-2024-11-19/",
      "official_url": null,
      "checked_at": "2026-02-27T22:45:43.453145+00:00",
      "http_status": 401,
      "final_url": null,
      "accessible": false,
      "access_type": "unavailable",
      "source_type": "news",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 401: HTTP Forbidden"
    },
    "laurie2014certificate": {
      "url": "https://www.rfc-editor.org/rfc/rfc6962",
      "official_url": null,
      "checked_at": "2026-02-27T22:33:42.946234+00:00",
      "http_status": 200,
      "final_url": "https://www.rfc-editor.org/rfc/rfc6962",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "RFC 6962: Certificate Transparency",
      "abstract": "",
      "error": null
    },
    "schneier_2015_data": {
      "url": "https://www.schneier.com/books/data-and-goliath/",
      "official_url": "https://www.schneier.com/books/data-and-goliath/",
      "checked_at": "2026-02-27T22:33:45.647717+00:00",
      "http_status": 200,
      "final_url": "https://www.schneier.com/books/data-and-goliath/",
      "accessible": true,
      "access_type": "book",
      "source_type": "book",
      "page_title": "Data and Goliath - Schneier on Security",
      "abstract": "Data and Goliath The Hidden Battles to Collect Your Data and Control Your World A Book by Bruce Schneier A New York Times Best Seller You are under surveillance right now. Your cell phone provider tracks your location and knows who’s with you. Your online and in-store purchasing patterns are recorded, and reveal if you’re unemployed, sick, or pregnant. Your e-mails and texts expose your intimate and casual friends. Google knows what you’re thinking because it saves your private searches. Facebook can determine your sexual orientation without you ever mentioning it...",
      "error": null
    },
    "ming2024nvidia": {
      "url": "https://www.smyg.hk/news/details/13715",
      "official_url": null,
      "checked_at": "2026-02-27T22:33:46.736643+00:00",
      "http_status": 200,
      "final_url": "https://www.smyg.hk/news/details/13715",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "NVIDIA AI GPU Shipments to Hit 4M in 2024 - SmBom",
      "abstract": "NVIDIA's data center and AI GPU shipments are projected to hit nearly 4 million units in 2024, securing a 98% market share and significant revenue growth.  ",
      "error": null
    },
    "statista2025advertising": {
      "url": "https://www.statista.com/outlook/amo/advertising/worldwide",
      "official_url": "https://www.statista.com/outlook/amo/advertising/worldwide?__sso_cookie_checker=failed",
      "checked_at": "2026-02-27T22:33:49.021166+00:00",
      "http_status": 200,
      "final_url": "https://www.statista.com/outlook/amo/advertising/worldwide?__sso_cookie_checker=failed",
      "accessible": true,
      "access_type": "paywall",
      "source_type": "news",
      "page_title": "Advertising - Worldwide | Statista Market Forecast",
      "abstract": "Worldwide: Ad spending in the Advertising market worldwide is forecasted to reach US$1.25tn in 2026. Definition: \nAdvertising spending refers to expenses for promotional strategies with which brands or businesses purchase advertising space to promote products or services.",
      "error": null
    },
    "taylor2024data": {
      "url": "https://www.statista.com/statistics/871513/worldwide-data-created/",
      "official_url": "https://www.statista.com/statistics/871513/worldwide-data-created/?__sso_cookie_checker=failed",
      "checked_at": "2026-02-27T22:33:51.333901+00:00",
      "http_status": 200,
      "final_url": "https://www.statista.com/statistics/871513/worldwide-data-created/?__sso_cookie_checker=failed",
      "accessible": true,
      "access_type": "paywall",
      "source_type": "news",
      "page_title": "Data growth worldwide 2029| Statista",
      "abstract": "The total amount of data created, captured, copied, and consumed globally is forecast to increase rapidly.",
      "error": null
    },
    "Steptoe": {
      "url": "https://www.steptoe.com/en/news-publications/steptechtoe-blog/senate-ai-forum-focuses-on-intellectual-property-issues.html",
      "official_url": null,
      "checked_at": "2026-02-27T22:33:53.247760+00:00",
      "http_status": 200,
      "final_url": "https://www.steptoe.com/en/news-publications/steptechtoe-blog/senate-ai-forum-focuses-on-intellectual-property-issues.html",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "steptoe_favicon_386",
      "abstract": "A series of closed-door forums hosted by Senate Majority Leader Chuck Schumer has been examining various discreet topics on Artificial Intelligence. One session, held on November 30, focused on transp...",
      "error": null
    },
    "associatedpress2025anthropic": {
      "url": "https://www.theguardian.com/technology/2025/sep/05/anthropic-settlement-ai-book-lawsuit",
      "official_url": "https://www.theguardian.com/technology/2025/sep/05/anthropic-settlement-ai-book-lawsuit",
      "checked_at": "2026-02-27T22:33:54.508394+00:00",
      "http_status": 200,
      "final_url": "https://www.theguardian.com/technology/2025/sep/05/anthropic-settlement-ai-book-lawsuit",
      "accessible": true,
      "access_type": "paywall",
      "source_type": "news",
      "page_title": "AI startup Anthropic agrees to pay $1.5bn to settle book piracy lawsuit | AI (artificial intelligence) | The Guardian",
      "abstract": "Settlement could be pivotal after authors claimed company took pirated copies of their work to train chatbots",
      "error": null
    },
    "mann2024ai": {
      "url": "https://www.theregister.com/2024/12/23/nvidia_ai_hardware_competition/",
      "official_url": null,
      "checked_at": "2026-02-27T22:33:55.921784+00:00",
      "http_status": 200,
      "final_url": "https://www.theregister.com/2024/12/23/nvidia_ai_hardware_competition/",
      "accessible": true,
      "access_type": "open",
      "source_type": "paper",
      "page_title": "AMD Instinct, cloudy silicon vie for a slice of Nvidia's pie • The Register",
      "abstract": "Analyst estimates show growing apetite for alternative infrastructure",
      "error": null
    },
    "robison2024openai": {
      "url": "https://www.theverge.com/2024/12/13/24320811/openai-ilya-sutskever-agi-data-training",
      "official_url": "https://www.theverge.com/2024/12/13/24320811/what-ilya-sutskever-sees-openai-model-data-training",
      "checked_at": "2026-02-27T22:33:57.105484+00:00",
      "http_status": 200,
      "final_url": "https://www.theverge.com/2024/12/13/24320811/what-ilya-sutskever-sees-openai-model-data-training",
      "accessible": true,
      "access_type": "paywall",
      "source_type": "news",
      "page_title": "RSS",
      "abstract": "“We’ve achieved peak data and there’ll be no more.”",
      "error": null
    },
    "welle2025aligning": {
      "url": "https://www.theverge.com/ai-artificial-intelligence/776752/center-for-the-alignment-of-ai-alignment-centers",
      "official_url": "https://www.theverge.com/ai-artificial-intelligence/776752/center-for-the-alignment-of-ai-alignment-centers",
      "checked_at": "2026-02-27T22:33:58.598974+00:00",
      "http_status": 200,
      "final_url": "https://www.theverge.com/ai-artificial-intelligence/776752/center-for-the-alignment-of-ai-alignment-centers",
      "accessible": true,
      "access_type": "paywall",
      "source_type": "news",
      "page_title": "RSS",
      "abstract": "If you take this AI alignment center too seriously, you will get Rickrolled",
      "error": null
    },
    "together2023redpajama": {
      "url": "https://www.together.ai/blog/redpajama-data-v2",
      "official_url": null,
      "checked_at": "2026-02-27T22:33:59.990102+00:00",
      "http_status": 200,
      "final_url": "https://www.together.ai/blog/redpajama-data-v2",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "RedPajama-Data-v2: An open dataset with 30 trillion tokens for training large language models",
      "abstract": "",
      "error": null
    },
    "Morales_2024": {
      "url": "https://www.tomshardware.com/tech-industry/artificial-intelligence/musks-concerns-over-google-deepmind-ai-dictatorship-revealed-in-emails-from-2016-communications-released-during-the-recent-openai-court-case",
      "official_url": null,
      "checked_at": "2026-02-27T22:45:44.632477+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "unavailable",
      "source_type": "report",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Access Denied"
    },
    "turingAIethics2019": {
      "url": "https://www.turing.ac.uk/sites/default/files/2019-08/understanding_artificial_intelligence_ethics_and_safety.pdf",
      "official_url": null,
      "checked_at": "2026-02-27T22:34:02.369181+00:00",
      "http_status": 200,
      "final_url": "https://www.turing.ac.uk/sites/default/files/2019-08/understanding_artificial_intelligence_ethics_and_safety.pdf",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "",
      "abstract": "",
      "error": null
    },
    "Samuel_2024": {
      "url": "https://www.vox.com/future-perfect/2024/5/17/24158403/openai-resignations-ai-safety-ilya-sutskever-jan-leike-artificial-intelligence",
      "official_url": null,
      "checked_at": "2026-02-27T22:34:03.589158+00:00",
      "http_status": 200,
      "final_url": "https://www.vox.com/future-perfect/2024/5/17/24158403/openai-resignations-ai-safety-ilya-sutskever-jan-leike-artificial-intelligence",
      "accessible": true,
      "access_type": "open",
      "source_type": "report",
      "page_title": "TikTok",
      "abstract": "Company insiders explain why safety-conscious employees are leaving.",
      "error": null
    },
    "weforumAIGovernance2023": {
      "url": "https://www.weforum.org/publications/ai-governance-a-holistic-approach-to-implement-ethics-into-ai/",
      "official_url": null,
      "checked_at": "2026-02-27T22:45:46.041332+00:00",
      "http_status": 403,
      "final_url": null,
      "accessible": false,
      "access_type": "unavailable",
      "source_type": "report",
      "page_title": "",
      "abstract": "",
      "error": "HTTP 403: Forbidden"
    }
  }
}