<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Attribution-Based Control in AI Systems</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Source+Serif+4:ital,opsz,wght@0,8..60,400;0,8..60,600;1,8..60,400&display=swap"
        rel="stylesheet">
    <!-- MathJax for LaTeX rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-8HDSX11G9D"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'G-8HDSX11G9D');
    </script>
    <!-- Privacy-friendly analytics by Plausible -->
    <script async src="https://plausible.io/js/pa-OtFEayY3RhHZVOzX3iV76.js"></script>
    <script>
        window.plausible = window.plausible || function () { (plausible.q = plausible.q || []).push(arguments) }, plausible.init = plausible.init || function (i) { plausible.o = i || {} };
        plausible.init()
    </script>
</head>

<body>
    <div class="wrapper">
        <nav>
            <a href="index.html" class="active">I. Introduction</a>
            <a href="chapter2.html">II. Deep Voting</a>
            <a href="chapter3.html">III. Network-Source AI</a>
            <a href="chapter4.html">IV. Broad Listening</a>
            <a href="chapter5.html">V. Conclusion</a>
            <a href="https://andrewtrask.com">About</a>
            <a href="appendix1.html">Appendix I</a>
            <a href="appendix2.html">Appendix II</a>
        </nav>

        <div class="page-container">
            <aside class="toc-sidebar">
                <h4>On This Page</h4>
                <ul>
                    <li class="toc-h3"><a href="#abstract">Abstract</a></li>
                    <li class="toc-h3"><a href="#problem-abc">The Problem of ABC</a></li>
                    <li class="toc-h4"><a href="#individual-consequences">Individual Consequences</a></li>
                    <li class="toc-h4"><a href="#institutional-consequences">Institutional Consequences</a></li>
                    <li class="toc-h4"><a href="#societal-consequences">Societal Consequences</a></li>
                    <li class="toc-h4"><a href="#geopolitical-consequences">Geopolitical Consequences</a></li>
                    <li class="toc-h4"><a href="#centralization-consequences">Centralization Consequences</a></li>
                    <li class="toc-h3"><a href="#underlying-causes">Underlying Causes</a></li>
                    <li class="toc-h4"><a href="#addition-problem">The Addition Problem</a></li>
                    <li class="toc-h4"><a href="#copy-problem">The Copy Problem</a></li>
                    <li class="toc-h4"><a href="#branching-problem">The Branching Problem</a></li>
                    <li class="toc-h4"><a href="#natural-problem">The Natural Problem</a></li>
                    <li class="toc-h3"><a href="#thesis-outline">Thesis Outline</a></li>
                    <li class="toc-h4"><a href="#chapter-2-outline">Chapter 2: Deep Voting</a></li>
                    <li class="toc-h4"><a href="#chapter-3-outline">Chapter 3: NSAI</a></li>
                    <li class="toc-h4"><a href="#chapter-4-outline">Chapter 4: Broad Listening</a></li>
                    <li class="toc-h4"><a href="#chapter-5-outline">Chapter 5: Conclusion</a></li>
                </ul>
                <div class="ascii-decoration">
                    ·
                    /|\
                    / | \
                    · · ·
                    /|\ · /|\
                    · · · · · ·

                    who controls
                    the flow?
                </div>
            </aside>

            <!-- Floating margin art -->
            <div class="ascii-margin">
                <div class="art-piece">
                    ?→╔═══╗→?
                    ║ ? ║
                    ?→╚═══╝→?
                    black box
                </div>
                <div class="art-piece">
                    1+6=7
                    (lost)

                    "1"+"6"="16"
                    (kept)
                </div>
                <div class="art-piece">
                    ╭──→──╮
                    │ │
                    ↑ ◉ ↓
                    │ │
                    ╰──←──╯
                    central
                    control
                </div>
            </div>

            <main>
                <header>
                    <h1 style="white-space: nowrap;">Attribution-Based Control in AI Systems</h1>
                    <p class="author">Andrew Trask</p>
                    <p class="affiliation">University of Oxford</p>
                </header>

                <section class="abstract" id="abstract">
                    <h2>Abstract</h2>

                    <p>
                        Many of AI’s risks in areas like privacy, value alignment, copyright, concentration
                        of power, and hallucinations can be reduced to the problem of <a
                            href="https://attribution-based-control.ai/">attribution-based control</a> (ABC) in AI
                        systems,
                        which itself can be reduced to the overuse of addition,
                        copying, and branching within gradient descent. This thesis synthesizes a handful
                        of recently proposed techniques in deep learning, cryptography, and distributed
                        systems, revealing a viable path to reduce addition, copying, and branching; provide
                        ABC; and address many of AI’s primary risks while accelerating its benefits by
                        unlocking 6+ orders of magnitude more data, compute, and associated AI capability
                    </p>
                </section>

                <figure class="full-width">
                    <img src="1411.3146/broad_listening_v4.png" alt="Traditional AI vs ABC-enabled AI systems">
                    <figcaption>Traditional open/closed-source AI systems (left) centralize data collection and
                        decision-making, while ABC-enabled AI systems (right) enable direct communication between
                        those with data and those seeking insights.</figcaption>
                </figure>

                <section>
                    <p>While AI systems learn from data, AI users cannot know or control which data points inform
                        which predictions because AI lacks <a
                            href="https://attribution-based-control.ai/"><em>attribution-based control</em></a> (ABC).
                        Consequently, AI draws
                        upon sources whose exact integrity cannot be verified. And when AI draws upon incomplete
                        or problematic sources, it struggles with hallucinations, disinformation, value alignment, and
                        a greater collective action problem: while an AI model in a democratic nation is as capable
                        as the amount of data and compute a company can collect, an AI model in an authoritarian
                        nation may become as capable as the amount of data and compute an entire nation can collect.
                        Consequently, to win the AI race, democratic nations must either embrace the centralization of
                        data and compute (i.e., centralize AI powered decision making across their society) or
                        relinquish
                        AI advantage to nations who do.</p>

                    <p>This thesis describes how these problems stem from a single technical source: AI’s overreliance
                        on
                        copying, addition, and branching operations during training and inference. By
                        synthesizing recent breakthroughs in cryptography, deep learning, and distributed systems, this
                        thesis proposes a new method for AI with attribution-based control. When fully bloomed, this
                        breakthrough can enable each AI user to control which data sources drive an AI’s predictions,
                        and enable an AI’s data sources to control which AI users they wish to support, transforming AI
                        from a centralized tool that produces intelligence to a communication tool that connects people.
                    </p>

                    <p>Taken together, this thesis describes an ongoing technical shift, driven by the hunt for 6-
                        orders of magnitude more data and compute, and driven by value alignment to powerful market
                        and geo-political forces of data owners and AI users, which is transforming AI from a tool of
                        central intelligence into something radically different: a communication tool for <em>broad
                            listening</em>.
                    </p>
                </section>

                <section>
                    <h2 id="problem-abc">The Problem of Attribution-Based Control (ABC)</h2>

                    <div class="definition-box">
                        <h3>Definition: Attribution-Based Control</h3>
                        <p>An AI system provides attribution-based control
                            (ABC) when it enables a bidirectional relationship between two parties:</p>
                        <ul>
                            <li><strong>Data Sources:</strong> control which AI outputs they support with AI capability,
                                and
                                calibrate the
                                degree to which they offer that support.</li>
                            <li><strong>AI Users:</strong> control which data sources they rely upon for AI capability,
                                and
                                calibrate the
                                degree to which they rely on each source.</li>
                        </ul>
                        <p>They negotiate which sources are leveraged and how much capability to create.</p>
                    </div>

                    <p>At the present moment, AI systems do not comprehensively enable AI users to know or
                        control which data sources inform which AI outputs (or vice versa) which is to say they do not
                        offer attribution-based control (ABC). While RAG and similar algorithms might appear to offer
                        inference-level control, AI systems fail to provide formal guarantees that data provided to the
                        input of a model is truly being leveraged to create the output. Taken together, with respect to
                        data provided during pre-training, fine-tuning, or inference, AI systems do not offer ABC.</p>

                    <p>Consequently, AI is conceived of (by people in the world) as a system that gains intelligence
                        by harvesting information about the world, as opposed to a communication technology which
                        enables billions of people with information to aid billions seeking insights. In the same way,
                        an AI model is conceived of as the asset which produces intelligence, instead of merely as a
                        temporary cache of information making its way from sources to users. If AI were a telephone, it
                        would be one without buttons, and its users would believe they are talking to the phone itself,
                        as
                        opposed to talking with people <em>through the phone</em>.</p>

                    <p>This design decision triggers a cascade of consequences, consequences which are felt
                        uniquely at each level of society: individual, institutional, societal, and geo-political. And
                        as a
                        result, each level of society is incubating deep desires for something more than today’s AI has
                        to
                        offer. The next four subsections survey this increasing demand for ABC in AI by surveying the
                        felt consequences of the lack of ABC in AI.</p>

                    <figure class="full-width">
                        <img src="1411.3146/abc_2_v5.png" alt="ABC underpins problems at several levels">
                        <figcaption>Insufficient ABC underpins problems at several levels.</figcaption>
                    </figure>
                </section>

                <section>
                    <h3 id="individual-consequences">Individual Consequences</h3>

                    <p>
                        The lack of ABC presents AI users with a basic authenticity problem, discussed publicly using
                        phrases
                        like hallucination, deepfake, and disinformation (<a href="#ref-1"
                            class="cite"><sup>N</sup></a><sup>; </sup><a href="#ref-2" class="cite"><sup>C</sup></a>).
                        When an AI generates
                        a
                        response, users often cannot verify its sources; unlike
                        a Google search where one can examine original documents (<a href="#ref-3"
                            class="cite"><sup>G</sup></a><sup>; </sup> <a href="#ref-22" class="cite">
                            <sup>E</sup></a>). Consider a
                        medical student using AI to research rare diseases. They
                        cannot tell whether an AI’s detailed symptom description comes from high-quality sources (e.g.,
                        peer-reviewed journals) or less qualified sources (<a href="#ref-4"
                            class="cite"><sup>J</sup></a><sup>; </sup> <a href="#ref-24" class="cite"><sup>M</sup></a>).
                        Meanwhile, the
                        researchers who authored medical papers
                        feeding the AI lose control over how
                        their work is used, dis-incentivizing them to share information (<a href="#ref-25"
                            class="cite"><sup>T</sup></a>).This
                        mutual blindness creates the opportunity for hallucination and disinformation
                    </p>

                    <figure class="full-width">
                        <img src="1411.3146/individual_consequences_v3.png"
                            alt="Traditional AI systems delete attribution information">
                        <figcaption>Traditional open/closed-source AI systems (left) mathematically delete information
                            linking source and prediction. Once this information is gone, neither adding more data
                            (RLHF),
                            looking for source information (influence functions), or providing oversight over AI systems
                            addresses the core issue. Instead, ABC-enabled AI systems (right) enable direct
                            communication
                            between those with data and those seeking insights. In ABC-enabled AI, attribution and
                            control
                            flow with the information, enabling verification of sources and protection of contributor
                            interests.</figcaption>
                    </figure>

                    <p>
                        <strong>Hallucination:</strong> To see ABC’s contribution to the problem of hallucination,
                        consider
                        the mechanics of an AI hallucination. In some cases, an AI model makes a prediction regarding a
                        subject either not properly covered in its training data or prompt, or not properly recalled
                        during
                        inference (<a href="#ref-6" class="cite"><sup>L</sup></a>). But since an AI model’s
                        predictions
                        are necessarily based on data,
                        the AI will attempt to use insights from less related documents <a href="#sidenote-1"
                            class="sidenote-ref"><sup>1</sup></a>
                        to generate a plausible-sounding
                        response: a hallucination. Yet, because of a lack of ABC, the AI user has little ability to spot
                        an
                        AI model sourcing from such unqualified documents, or to make adjustments to ensure the AI’s
                        prediction is properly sourced. The AI user, for example, has no ability to say, ”I queried this
                        AI model about Java development, but I notice it’s relying upon data sources about Indonesian
                        coffee cultivation instead of programming language documentation.” Instead, the AI user merely
                        sees the output and wonders, ”Is this AI generated statement true or is it merely grammatical?”.
                    </p>

                    <div class="sidenote" id="sidenote-1">
                        <p>
                            <sup>1</sup>
                            By ”less related documents”, I mean that since AI models only output information based on
                            data,
                            that
                            if they
                            cannot find data relevant to the current prompt, they will still output results based on
                            <em>some
                                kind
                                of data.</em> In the limit,
                            an AI model will leverage its general corpus to create a response which is grammatical, even
                            if
                            no
                            training data
                            document is relevant to the current query. The plausibility of such outputs despite their
                            lack
                            of
                            grounding is the
                            root behind their characterization as <em>hallucination</em>.
                        </p>
                    </div>

                    <p>
                        <strong>Disinformation:</strong> To see ABC’s contribution to the problem of disinformation,
                        consider the
                        mechanics of a misleading prediction. In some cases, an AI model makes a prediction by
                        combining concepts which ought-not be combined if they are to yield something truthful. An AI
                        model queried to create a fake picture of a celebrity performing a controversial act, for
                        example,
                        might recall (pre-training) data about the likeness of a celebrity with data about unrelated
                        people
                        performing that controversial act, yielding an insidious form of disinformation: a deepfake.
                    </p>

                    <p>
                        In scientific literature, journalism, or law, a reader might easily spot such disinformation by
                        consulting a written work’s bibliography and considering whether original sources are relevant,
                        reputable, and being synthesized properly into a novel insight. Not so with AI. Because of a
                        lack
                        of ABC, the end user viewing synthetic media has no source bibliography (and thus no ability to
                        spot an AI prediction sourcing from such unqualified pre-training/RLHF/etc. data points, or to
                        make adjustments to ensure the AI’s prediction ignores/leverages data points appropriately
                        <a href="#sidenote-2" class="sidenote-ref"><sup>2</sup></a>).
                        The AI user, for example, has no ability to say, ”I queried this AI model for a picture of
                        London,
                        so why is it also sourcing from Hilton Hotels promotional materials instead of only photos of
                        the
                        English capital?”. Consequently, an AI prediction can contain misleading information whose
                        message is compelling but whose deceitful source is imperceptible to the viewer.
                    </p>

                    <div class="sidenote" id="sidenote-2">
                        <p>
                            <sup>2</sup>
                            As before, a reader might object and say that RAG or other context window information might
                            inform the user
                            of this problem, and thus provide ABC. However, an AI model need not actually use any of its
                            inputs. Consequently,
                            AI users have no ability to know whether an AI model used information fed into the input,
                            nor do they have the
                            ability to know which non-input information the AI model relies upon for a prediction.
                        </p>
                    </div>


                    <p>
                        <strong>Proposed Remedies:</strong>
                        Within the context of LLMs, recent work has attempted to address
                        this through a myriad of methods such as, data cleaning, fine-tuning more data into AI models
                        (e.g., RLHF, InstructGPT), direct oversight of deployed AI (e.g., HITL, auditing), measures of
                        AI confidence (i.e., self checking), improving prompts (e.g., chain of thought, RAG), source
                        attribution methods to reverse engineer the source-prediction relationship (i.e., influence
                        functions),
                        and others (<a href="#ref-26" class="cite"><sup>L</sup></a><sup>;</sup> <a href="#ref-27"
                            class="cite"><sup>Z</sup></a><sup>;</sup> <a href="#ref-28"
                            class="cite"><sup>M</sup></a><sup>;</sup> <a href="#ref-29" class="cite"><sup>P</sup></a>).
                    </p>

                    <p>
                        These efforts, however, face two fundamental limitations which prevent them from solving
                        the issue. First, it has been shown that LLMs will always hallucinate because they cannot learn
                        all computable functions between a computable LLM and a computable ground truth function
                        (<a href="#ref-5" class="cite"><sup>Z</sup></a>) (a proof which focuses on LLMs but requires
                        no
                        specific tie to language or
                        transformers). And second, since AI will always hallucinate to some extent, the question is
                        whether or not detection is possible and addressable. Since AIs only know how to output true
                        facts based on the data they have consumed, this concerns measuring and controlling whether
                        an AI model is sourcing from appropriate experts for each of its statements. That is, detecting
                        disinformation, hallucinations, and other authenticity issues is not <em>first</em> a problem of
                        intelligence,
                        it’s <em>first</em> a problem of attribution (and then perhaps a problem of intelligence).
                    </p>

                    <p>
                        However, as this thesis will soon show, when neural networks combine information through
                        addition operations during training, they irreversibly compress the relationship between sources
                        and predictions. No amount of post-training intervention (e.g. fine-tuning, auditing, influence
                        functions, etc.) can fully restore these lost connections, solve the lack of ABC, and empower AI
                        users to better avoid and address AI hallucinations, disinformation, or deepfakes. Yet, if
                        attribution was solved, while it would still be possible to compel an AI to produce false
                        information,
                        it may become difficult (if not impossible) to do so without also revealing the inappropriate
                        sources used to generate such false statements <a href="#sidenote-3"
                            class="sidenote-ref"><sup>3</sup></a>. Altogether, AI needs a
                        bibliography.
                    </p>

                    <div class="sidenote" id="sidenote-3">
                        <p>
                            <sup>3</sup>
                            This would be because AI models do not generate predictions out of thin air. They are
                            data-trained machines,
                            and thus must pull information from sources in some respect. Consequently, if such sources
                            were visible to the
                            final AI user, they would have a significant leg up (or definitive ability) to detect
                            disinformation and hallucinations.
                        </p>
                    </div>
                </section>

                <section>
                    <h3 id="institutional-consequences">Institutional Consequences</h3>

                    <figure class="full-width">
                        <img src="1411.3146/abc_3_v8.png" alt="Insufficient ABC underpins institutional problems">
                        <figcaption>Insufficient ABC underpins problems for data-owning institutions.</figcaption>
                    </figure>

                    <div class="cite-box-wrapper">
                        <p>
                            The lack of ABC cre`ates a basic incentive problem at the institutional level. Data-owning
                            institutions can either share data to create AI, or they can decline and maintain control over
                            their data, a dilemma discussed publicly using words like copyright, privacy, transparency, and
                            misuse (<span id="cite-box-ref-1" class="cite-box-ref" data-box="cite-box-1"><a href="#ref-30" class="cite"><sup>A</sup></a></span><sup>; </sup><span id="cite-box-ref-2" class="cite-box-ref" data-box="cite-box-2"><a href="#ref-31"
                                class="cite"><sup>A</sup></a></span>). Leading cancer centers, for example, struggle to
                            train AI to detect breast cancer using more than 3-4 million mammograms (the largest and most
                            diverse dataset known to this author being (<a href="#ref-32" class="cite"><sup>J</sup></a>).
                            Yet hundreds of millions of
                            mammograms are produced annually worldwide (<a href="#ref-33"
                                class="cite"><sup>P</sup></a><sup>; </sup>
                            <a href="#ref-34" class="cite"><sup>r</sup></a>).
                        </p>
                        <div id="cite-box-1" class="cite-box" data-ref="cite-box-ref-1" data-paper-url="https://arxiv.org/abs/2204.07931">
                            <div class="cite-box-paper">
                                <div class="cite-box-thumbnail-wrapper">
                                    <img src="citations/img/paper_2204.07931.png" alt="Paper thumbnail" class="cite-box-thumbnail">
                                </div>
                                <div class="cite-box-title">On the Origin of Hallucinations in Conversational Models: Is it the Datasets or the Models?</div>
                            </div>
                            <div class="cite-box-avatars">
                                <img src="citations/img/nouha_dziri.jpeg" alt="Nouha Dziri"
                                    data-name="Nouha Dziri"
                                    data-affiliation="Allen Institute for AI (Ai2)"
                                    data-verified="Verified email at allenai.org - <a href='#'>Homepage</a>"
                                    data-topics="Artificial Intelligence<br>Natural Language Processing"
                                    data-scholar-url="https://scholar.google.com/citations?user=NJ7CzqMAAAAJ">
                                <img src="citations/img/sivan_milton.png" alt="Sivan Milton"
                                    data-name="Sivan Milton"
                                    data-affiliation="PhD Student, University of Edinburgh"
                                    data-verified="Verified email at ed.ac.uk"
                                    data-topics="semantics<br>syntax<br>NLP"
                                    data-scholar-url="https://scholar.google.com/citations?user=example1">
                                <img src="citations/img/mo_yu.jpeg" alt="Mo Yu"
                                    data-name="Mo Yu"
                                    data-affiliation="WeChat AI, Tencent"
                                    data-verified="Verified email at tencent.com - <a href='#'>Homepage</a>"
                                    data-topics="NLP<br>Question Answering<br>Information Extraction"
                                    data-scholar-url="https://scholar.google.com/citations?user=example2">
                                <img src="citations/img/osmar_zaiane.jpeg" alt="Osmar Zaiane"
                                    data-name="Osmar Zaiane"
                                    data-affiliation="University of Alberta - Alberta Machine Intelligence Institute, Canada CIFAR AI Chair"
                                    data-verified="Verified email at cs.ualberta.ca - <a href='#'>Homepage</a>"
                                    data-topics="Data Mining<br>Social Network Analysis<br>Health Informatics"
                                    data-scholar-url="https://scholar.google.com/citations?user=example3">
                                <img src="citations/img/silva_reddy.jpeg" alt="Siva Reddy"
                                    data-name="Siva Reddy"
                                    data-affiliation="McGill University, Mila Quebec AI Institute"
                                    data-verified="Verified email at cs.mcgill.ca - <a href='#'>Homepage</a>"
                                    data-topics="Natural Language Processing<br>Computational Linguistics<br>Deep Learning"
                                    data-scholar-url="https://scholar.google.com/citations?user=example4">
                            </div>
                            <div class="cite-box-author">
                                <img src="citations/img/nouha_dziri.jpeg" alt="Nouha Dziri" class="cite-box-author-photo">
                                <div class="cite-box-author-info">
                                    <div class="cite-box-author-name">Nouha Dziri</div>
                                    <div class="cite-box-author-affiliation">Allen Institute for AI (Ai2)</div>
                                    <div class="cite-box-author-verified">Verified email at allenai.org - <a href="#">Homepage</a></div>
                                    <div class="cite-box-author-topics">Artificial Intelligence<br>Natural Language Processing</div>
                                </div>
                            </div>
                            <div class="cite-box-scholar">
                                <span style="color:#4285f4">G</span><span style="color:#ea4335">o</span><span style="color:#fbbc05">o</span><span style="color:#4285f4">g</span><span style="color:#34a853">l</span><span style="color:#ea4335">e</span> <span style="color:#555">Scholar</span>
                            </div>
                        </div>
                        <div id="cite-box-2" class="cite-box" data-ref="cite-box-ref-2" data-paper-url="https://arxiv.org/abs/2204.07931">
                            <div class="cite-box-paper">
                                <div class="cite-box-thumbnail-wrapper">
                                    <img src="citations/img/paper_2204.07931.png" alt="Paper thumbnail" class="cite-box-thumbnail">
                                </div>
                                <div class="cite-box-title">On the Origin of Hallucinations in Conversational Models: Is it the Datasets or the Models?</div>
                            </div>
                            <div class="cite-box-avatars">
                                <img src="citations/img/nouha_dziri.jpeg" alt="Nouha Dziri"
                                    data-name="Nouha Dziri"
                                    data-affiliation="Allen Institute for AI (Ai2)"
                                    data-verified="Verified email at allenai.org - <a href='#'>Homepage</a>"
                                    data-topics="Artificial Intelligence<br>Natural Language Processing"
                                    data-scholar-url="https://scholar.google.com/citations?user=NJ7CzqMAAAAJ">
                                <img src="citations/img/sivan_milton.png" alt="Sivan Milton"
                                    data-name="Sivan Milton"
                                    data-affiliation="PhD Student, University of Edinburgh"
                                    data-verified="Verified email at ed.ac.uk"
                                    data-topics="semantics<br>syntax<br>NLP"
                                    data-scholar-url="https://scholar.google.com/citations?user=example1">
                                <img src="citations/img/mo_yu.jpeg" alt="Mo Yu"
                                    data-name="Mo Yu"
                                    data-affiliation="WeChat AI, Tencent"
                                    data-verified="Verified email at tencent.com - <a href='#'>Homepage</a>"
                                    data-topics="NLP<br>Question Answering<br>Information Extraction"
                                    data-scholar-url="https://scholar.google.com/citations?user=example2">
                                <img src="citations/img/osmar_zaiane.jpeg" alt="Osmar Zaiane"
                                    data-name="Osmar Zaiane"
                                    data-affiliation="University of Alberta - Alberta Machine Intelligence Institute, Canada CIFAR AI Chair"
                                    data-verified="Verified email at cs.ualberta.ca - <a href='#'>Homepage</a>"
                                    data-topics="Data Mining<br>Social Network Analysis<br>Health Informatics"
                                    data-scholar-url="https://scholar.google.com/citations?user=example3">
                                <img src="citations/img/silva_reddy.jpeg" alt="Siva Reddy"
                                    data-name="Siva Reddy"
                                    data-affiliation="McGill University, Mila Quebec AI Institute"
                                    data-verified="Verified email at cs.mcgill.ca - <a href='#'>Homepage</a>"
                                    data-topics="Natural Language Processing<br>Computational Linguistics<br>Deep Learning"
                                    data-scholar-url="https://scholar.google.com/citations?user=example4">
                            </div>
                            <div class="cite-box-author">
                                <img src="citations/img/silva_reddy.jpeg" alt="Siva Reddy" class="cite-box-author-photo">
                                <div class="cite-box-author-info">
                                    <div class="cite-box-author-name">Siva Reddy</div>
                                    <div class="cite-box-author-affiliation">McGill University, Mila Quebec AI Institute</div>
                                    <div class="cite-box-author-verified">Verified email at cs.mcgill.ca - <a href="#">Homepage</a></div>
                                    <div class="cite-box-author-topics">Natural Language Processing<br>Computational Linguistics<br>Deep Learning</div>
                                </div>
                            </div>
                            <div class="cite-box-scholar">
                                <span style="color:#4285f4">G</span><span style="color:#ea4335">o</span><span style="color:#fbbc05">o</span><span style="color:#4285f4">g</span><span style="color:#34a853">l</span><span style="color:#ea4335">e</span> <span style="color:#555">Scholar</span>
                            </div>
                        </div>
                    </div>

                    <p>This gap exists because medical facilities face an impossible decision: either withhold their
                        data to protect patient privacy and maintain control (i.e. protect privacy, IP, security, legal
                        risk,
                        etc.), or share it and lose all ability to control how it’s used (<a href="#ref-30"
                            class="cite"><sup>A</sup></a>). In 2023,
                        multiple
                        major medical centers explicitly cited this dilemma when declining to participate in AI research
                        (<a href="#ref-35" class="cite"><sup>A</sup></a>). Yet their complaint is not unique to
                        medical information, it exists for any
                        data owner who has some incentive to care about when and how their data is used (<a
                            href="#ref-36" class="cite"><sup>J</sup></a>).</p>

                    <p>
                        The consequences of unmotivated data owners are staggering to consider. Despite AI drawing the
                        attention of the world, even the most famous AI models have not inspired most of the
                        world’s data or compute owners to collectively act to train it. As described in chapter 2, 6
                        orders
                        of magnitude more data, and 6 orders of magnitude more compute productivity remain untapped,
                        such that even the most powerful AI models are trained on less than a millionth of the possible
                        resources available.
                    </p>

                    <p><strong>Fighting Against Data and Compute for AI&mdash;Copyright, Siloing, and
                            Rate-limiting:</strong> The resistance against AI training is widespread and highly vocal.
                        It
                        can be found regarding
                        websites as big as Reddit, the New York Times, and Twitter, and as small as individual bloggers
                        who are fighting against onerous scrapers (<a href="#ref-38" class="cite"><sup>D</sup></a><sup>;
                        </sup>
                        <a href="#ref-39" class="cite"><sup>M</sup></a><sup>; </sup> <a href="#ref-40"
                            class="cite"><sup>P</sup></a><sup>; </sup>
                        <a href="#ref-41" class="cite"><sup>M</sup></a>). Resistance can be found across media
                        and
                        entertainment, but also in
                        the siloing of the world’s structured data ( <a href="#ref-42" class="cite"><sup>J</sup></a>).
                        This
                        resistance has cultivated a profound
                        contradiction (unpacked in Chapter 2): the AI community feels there is a lack of data and
                        compute (enough for NVIDIA chips to be in short supply and Ilya Sutzskever to announce ”peak
                        data”) while around 1,000,000x more of each remains untapped (<a href="#ref-43"
                            class="cite"><sup>K</sup></a>).
                    </p>

                    <figure class="full-width">
                        <img src="1411.3146/institutional_consequences.png"
                            alt="Traditional AI systems copy data to providers">
                        <figcaption>Traditional open/closed-source AI systems (left) copy data to AI providers, giving
                            them unilateral control over the resulting model and its predictions. Instead, ABC-enabled
                            AI
                            systems (right) enable direct communication between those with data and those seeking
                            insights.
                            In ABC-enabled AI, attribution and control flow with the information, enabling data sources
                            to
                            retain control over which predictions they seek to support.</figcaption>
                    </figure>

                    <p>
                        Recent work has suggested that privacy-enhancing technologies (PETs) can solve this
                        collective action problem, suggesting technologies like federated learning, secure multi-party
                        computation, and synthetic data (<a href="#ref-44" class="cite"><sup>H</sup></a><sup>; </sup> <a
                            href="#ref-45" class="cite"><sup>N</sup></a><sup>; </sup> <a href="#ref-46"
                            class="cite"><sup>I</sup></a><sup>; </sup> <a href="#ref-47" class="cite">
                            <sup>A</sup></a>). These efforts, however, face a fundamental limitation: they may
                        enable data owners to retain control over the only copy of their information, but each of these
                        technologies alone do not enable data owners to obtain fine-grained control over which AI
                        predictions they wish to support. Consequently, none provide attribution-based control.
                    </p>

                    <p>
                        Federated learning and synthetic data, for example, avoid the need to share raw data, enabling
                        them to retain control over that data, but they don’t enable data providers to
                        <em>collectively</em>
                        control
                        how an AI model they create is used because they do not provide a user-specific (i.e.
                        attributionbased) control mechanism (<a href="#ref-44" class="cite"><sup>B</sup></a><sup>;</sup>
                        <a href="#ref-48" class="cite"><sup>D</sup></a>). Secure multi-party
                        computation
                        schemes (secure enclaves, SPDZ, etc.) might allow for joint control over an AI model, but they
                        do not inherently provide the metadata or control necessary for data providers to add or
                        withdraw
                        their support for specific AI predictions (i.e. attribution) (<a href="#ref-49"
                            class="cite"><sup>A</sup></a>). To use SMPC for ABC,
                        one would need to re-train AI models whenever a part of an AI model’s pre-training data needs
                        to be removed (such as for compliance with <em>right to be forgotten</em> laws (<a
                            href="#ref-49" class="cite"><sup>J</sup></a>).
                    </p>

                    <p>
                        That is to say, unless AI models are continuously retrained, the SMPC problem is one of
                        decision bundling (analogous to economic bundling), such that an entire group would need
                        to decide whether to leverage an entire model... or not. Taken together, while some specific
                        privacy concerns have been addressed by PETs, the broader incentive issues averting collective
                        action have not. PETs offer collective production of AI models through encryption, but PETs do
                        not offer fine-grained control by each contributing data source (i.e. attribution-based
                        control)...
                        because AI models don’t reveal source attribution per prediction.
                    </p>

                    <p>
                        Yet, if attribution was solved in AI systems (in a more efficient manner than model retraining),
                        it could be combined with techniques like FL or SMPC to provide ABC. Thus, the complementary
                        challenge is attribution (i.e., the ability to enable each data participant to efficiently elect
                        to
                        support or not support specific AI predictions) without needing to relinquish or economically
                        bundle control within AI models. This attribution task is also referred to as <em>machine
                            unlearning</em>,
                        which at the time of writing remains an open problem in the literature (<a href="#ref-51"
                            class="cite"><sup>T</sup></a>).
                    </p>

                    <p>
                        In chapter 2, this thesis reveals existing, overlooked remedies to attribution/unlearning,
                        which enables chapter three to combine those ingredients with SMPC and related privacy
                        enhancing technologies. Together, chapters 2 and 3 reveal existing, overlooked techniques for
                        both attribution and control, revealing the AI community’s viable (albeit presently overlooked)
                        path to ABC, and a means to reverse incentives presently blocking 6+ orders of magnitude more
                        data and compute productivity.
                    </p>
                </section>

                <section>
                    <h3 id="societal-consequences">Societal Consequences</h3>

                    <figure class="full-width">
                        <img src="1411.3146/abc_4_v4.png" alt="Insufficient ABC underpins societal problems">
                        <figcaption>
                            Insufficient ABC underpins representation dilemmas for societies seeking to solve
                            bias and value alignment problems.
                        </figcaption>
                    </figure>

                    <p>
                        The lack of ABC creates a governance crisis unprecedented in scale at the societal level.
                        This crisis manifests publicly as concerns about AI safety, value alignment, and algorithmic
                        bias (<a href="#ref-51" class="cite"><sup>D</sup></a><sup>; </sup> <a href="#ref-51"
                            class="cite"><sup>I</sup></a> <sup>; </sup> <a href="#ref-78" class="cite"><sup>E</sup></a>
                        ).
                        Crisis arises because AI systems
                        can derive their capabilities from millions of contributors yet remain controlled by whomever
                        possesses a copy of an AI. This control structure sparks concern that either the AI or its owner
                        will use the AI’s intelligence in ways that conflict with society’s values or interests (Gabriel
                        et al., 2024). An LLM trained on chemistry research papers, for example, could be fine-tuned to
                        generate instructions for bioweapons, with no way for the training data creators to prevent this
                        misuse (<a href="#ref-54" class="cite"><sup>C</sup></a>)
                    </p>

                    <p>
                        Recent work has proposed various oversight mechanisms and safety guidelines in response
                        (<a href="#ref-55" class="cite"><sup>E</sup></a><sup>; </sup>
                        <a href="#ref-56" class="cite"><sup>E</sup></a><sup>; </sup>
                        <a href="#ref-57" class="cite"><sup>E</sup></a><sup>; </sup> <a href="#ref-58"
                            class="cite"><sup>I</sup></a><sup>; </sup> <a href="#ref-59"
                            class="cite"><sup>F</sup></a><sup>; </sup> <a href="#ref-60"
                            class="cite"><sup>A</sup></a><sup>; </sup>
                        <a href="#ref-61" class="cite"> <sup>M</sup></a><sup>; </sup> <a href="#ref-62" class="cite">
                            <sup>H</sup></a><sup>; </sup>
                        <a href="#ref-63" class="cite"><sup>A</sup></a><sup>; </sup> <a href="#ref-64"
                            class="cite"><sup>G</sup></a><sup>; </sup>
                        <a href="#ref-65" class="cite"><sup>M</sup></a><sup>; </sup> <a href="#ref-66"
                            class="cite"><sup>y</sup></a><sup>; </sup>
                        <a href="#ref-67" class="cite"><sup>M</sup></a><sup>; </sup><a href="#ref-66" class="cite">
                            <sup>I</sup></a><sup>; </sup>
                        <a href="#ref-67" class="cite"><sup>b</sup></a>). However, these efforts
                        face a limitation history has shown before: attempting to solve the problem of serving many by
                        empowering few (<a href="#ref-70" class="cite"><sup>G</sup></a>;<a href="#ref-701" class="cite">
                            <sup>Q</sup></a>; <a href="#ref-72" class="cite"><sup>C</sup></a>;
                        <a href="#ref-73" class="cite"><sup>C</sup></a>). When
                        individuals, small groups, or tech companies train AI models on global internet data, decisions
                        about content filtering and model behavior end up being made by AI models or small teams (at
                        most thousands attempting to represent billions). AI governance therefore becomes an exercise
                        in altruism, operating under the hope that AI models, IT staff, or institutions controlling them
                        will forego selfish incentives to serve humanity’s broader interests (<a href="#ref-74"
                            class="cite"><sup>E</sup></a>). These hopes
                        may be disappointed: despite grand claims, leading AI labs may not be successfully evading
                        their selfish incentives (<a href="#ref-75" class="cite"><sup>S</sup></a>; <a href="#ref-76"
                            class="cite"><sup>C</sup></a>; <a href="#ref-75" class="cite"><sup>J</sup></a>).
                    </p>

                    <figure class="full-width">
                        <img src="1411.3146/societal_challenges.png"
                            alt="AI model trained on regional data used by others">
                        <figcaption>A fictional illustration of an AI model trained on data from people across the US
                            Pacific coast, and used by people in the eastern half of the United States. However,
                            irrespective
                            of where the training data comes from or the predictions go to, a handful of people at the
                            AI organization retains centralized control over the AI model’s capability and use. This
                            calls
                            into question how/whether the AI or its creators will forego selfish incentives and use the
                            AI’s
                            intelligence to serve society broadly..</figcaption>
                    </figure>

                    <p>
                        Attempted solutions to these problems remain insufficient because they don’t address the
                        underlying representative control problem. This problem stems from a fundamental decoupling:
                        AI capability and AI alignment operate independently. Due to scaling laws, an AI model
                        becomes more capable when its owner can centralize more data and compute (<sup><a href="#ref-75"
                                class="cite">J</a>; <a href="#ref-80" class="cite">J</a></sup>). Yet its alignment with
                        human values depends on the AI model and/or its
                        supervisors choosing to forego selfish incentives, two forces that are not naturally correlated
                        (<sup><a href="#ref-75" class="cite">C</a></sup>). This decoupling creates a dangerous
                        asymmetry: unlike democratic systems,
                        where a leader’s power ideally scales with public support, AI systems concentrate power without
                        requiring <em>ongoing public consent</em>. This asymmetry intensifies as AI becomes more
                        capable,
                        threatening democratic values through greater autonomy, adoption, and complexity.
                    </p>

                    <p>
                        reatening democratic values through greater autonomy, adoption, and complexity.
                        This represents not merely a policy problem, but an architectural one rooted in how AI
                        systems process information (<sup><a href="#ref-71" class="cite">Q</a></sup>). As this thesis
                        shows, when neural networks
                        irreversibly: 1) combine data through addition operations and 2) decouple models from source
                        data through copy operations, and 3) route the selection of trusted sources and users through
                        ultra-high branching intermediaries, they sever the connection between data contributors and AI
                        users, shifting control over an AI model to the model itself and/or its owner. This
                        architectural
                        design means no amount of after-the-fact regulation can restore the lost capability for
                        continuous,
                        granular, public consent over how AI systems use society’s knowledge (consider, for example,
                        music piracy) (<sup><a href="#ref-30" class="cite">A</a></sup>). The model remains controlled by
                        whomever has a copy,
                        and whomever has a copy (person or AI) may or may not elect to forego selfish incentives and
                        serve the interests of society broadly. Society therefore faces a growing, collective need for
                        representative control over AI systems, motivating a technical need for ABC within AI systems.
                    </p>

                    <p>
                        Yet, even if such AI-creating institutions could be made align-able to society, AI systems
                        possess an even deeper flaw. An AI system’s behavior is determined by its training data (first
                        by pre-training data, then overridden in specific ways through RLHF/RLVF training updates).
                        However, centralized institutions are gathering training data at a scale far greater than their
                        ability to read and vet for data poisoning attacks. Consequently, as corporations, governments,
                        and billions around the world consider relying upon AI systems (most especailly agents) for
                        information and decision making, they open themselves up to an uncomfortable fact: anyone
                        could publish a few hundreds pages to the internet and change the behavior of their AI system in
                        the future, likely in a way which is undetectable by AI companies.
                    </p>

                    <p>
                        As an early example of this, in 2011 the Huffington Post broke a story about the <em>Hathaway
                            Effect</em>, that when famous actress Anne Hathaway experiences a significant career moment,
                        similarly-named Berkshire Hathaway stock rises as a result of algorithmic agents failing to
                        disambiguate in online sentiment analysis (<sup><a href="#ref-81" class="cite">D</a></sup>). The
                        setup is a direct comparison
                        to LLMs. These stock agents crawl the web looking for content, aggregating sentiment about
                        companies (and their products, services, management teams, etc.), and translate these indicators
                        into stock signals used for training at major financial institutions. However, while the
                        internet
                        contains useful signal for trading stocks, it also presents itself as a wide-open funnel for
                        noise.
                        And through sheer chance, Anne Hathaway’s last name provides the accidental noise some early
                        AI agents can fall for
                    </p>

                    <p>
                        And while this somewhat humorous societal phenomena is perhaps harmless, it is a very
                        real example of a more disturbing societal risk for modern AI agents. In a very real sense,
                        as web-trained AI agents make their way into government agencies, job application portals,
                        healthcare systems, financial models, security/surveillance systems, customer service portals,
                        credit rating models, they are all making their decision making process available for anyone
                        in the world to edit, simply by uploading a webpage with the right type of data poisoning
                        content, and making that webpage available to the right web crawlers. Consequently, even
                        if centralized AI companies were altruistic, the data they rely upon for intelligence need not
                        be. The simultaneous problems of poisonous data and non-altruistic intermediaries provides
                        technical motivation which goes beyond mere representative control; to solve these problems,
                        society needs representative control which can be source-weighted for veracity in a specific
                        context, attribution-based control in AI systems.
                    </p>
                </section>

                <section>
                    <h3 id="geopolitical-consequences">Geopolitical Consequences</h3>

                    <figure class="full-width">
                        <img src="1411.3146/abc_5_v5.png" alt="Insufficient ABC underpins geopolitical problems">
                        <figcaption> Insufficient ABC underpins dilemmas for democratic societies seeking to maintain
                            their values while winning the AI race against authoritarian nations.</figcaption>
                    </figure>

                    <p>
                        The lack of ABC creates tension between AI capability and democratic values at the
                        geopolitical level. AI systems become more powerful with more data and compute, as described
                        by so-called ”Scaling Laws” (<sup><a href="#ref-79" class="cite">J</a>;<a href="#ref-80"
                                class="cite">J</a></sup>) and implied by the
                        ”Bitter Lesson” (<sup><a href="#ref-17" class="cite">R</a></sup>). But without ABC, acquiring
                        data (and compute) means acquiring
                        a copy of it within a company, and acquiring a copy means centralizing unilateral control...
                        over
                        the data, over the compute, and over the AI systems this centralization creates.
                    </p>

                    <p>
                        This dilemma constrains democratic nations to two normative paths. Corporate AI caps AI
                        capability at whatever data and compute companies can acquire (e.g. Google, Meta, X, etc.).
                        National AI caps AI capability at whatever data and compute nations can centralize (e.g.,
                        China).
                        Democratic nations face this tension most clearly in situations like the US-China AI
                        competition:
                        they must either compromise their principles by centralizing data and compute, or cede AI
                        leadership to autocratic nations who face no such constraints.
                    </p>

                    <p>
                        Some policymakers have responded to this crisis not by addressing the underlying ABC
                        problem, but by proposing a ”Manhattan Project” for AI (<sup><a href="#ref-82"
                                class="cite">A</a></sup>). These
                        proposals suggest competing with China’s centralization by out-centralizing them... meeting
                        authoritarian AI capability by building a larger, even more centralized AI capability
                        domestically.
                    </p>

                    <figure class="full-width">
                        <img src="1411.3146/geopolitical_consequences.png" alt="Traditional AI training across nations">
                        <figcaption>
                            A fictional illustration of a traditional AI model trained within organizations across
                            the US (left) and trained across all organizations in China (right). It is meant to
                            illustrate the
                            difference between resources acquired at the corporation level and at the nation level.
                        </figcaption>
                    </figure>

                    <p>
                        Such an approach would face multiple challenges. To win, it would require overlooking
                        democratic principles (e.g., privacy, personal property, freedom of choice, etc.) to centralize
                        AI-powered decision making in a single, maximally-powerful, state-sponsored AI. It would still
                        face uncertainty about exceeding China’s AI capability; while the US may have a head-start on
                        compute and talent availability, China has a 4x larger population from which to draw data.
                    </p>

                    <p>
                        Yet this dilemma only exists because of the lack of ABC. With ABC, vast swaths of data
                        providers could collectively create individual AI predictions. And with ABC, AI users could
                        request which data sources they wish to use for their specific AI predictions. ABC would
                        subvert the need to centralize data, compute, and talent by creating a third AI paradigm beyond
                        corporate AI or national AI... AI as powerful as the amount of data, compute, and talent
                        across the global free market. For now, however, emerging ABC remains largely unrecognized,
                        while policymakers consider the centralization of data and AI-powered decision making at an
                        unprecedented scale.
                    </p>

                    <figure class="full-width">
                        <img src="1411.3146/abc_6_v3.png"
                            alt="Insufficient ABC creates advantages for centralized power">
                        <figcaption>Insufficient ABC creates advantages for centralized power.</figcaption>
                    </figure>
                </section>

                <section>
                    <h3 id="centralization-consequences">Centralization Consequences</h3>

                    <p>
                        At its core, AI intermediates a flow of information from billions of people (i.e. training data
                        sources) to billions of people (i.e. AI users). However, the lack of ABC in AI systems
                        obfuscates
                        this underlying communication paradigm and presents AI as a tool of central decision making.
                        Yes, AI systems promise to process humanity’s knowledge at unprecedented scale. Yet their very
                        architecture forces central AI creators to first accumulate that knowledge (even for
                        ”open-source”
                        AI), then guide how society accesses and uses it. Rather than enabling direct communication
                        between those who have knowledge and those who seek it, AI systems become intermediaries
                        that extract knowledge from many to serve the interests of few (albeit by guiding the decisions
                        of many). It reminds one of the rise of the mainframe computer in the 1950s and 60s, which
                        obfuscated computing’s forthcoming dominant use as a communication technology (i.e. the
                        internet), presenting computing instead as a centralized coordination apparatus.
                    </p>

                    <p>
                        In AI, this architectural obfuscation creates stark dilemmas. Users must trust central
                        authorities about what sources inform AI outputs. Data owners must choose between sharing
                        knowledge and maintaining control over how it’s used, starving AI of all but 0.0001% of the
                        world’s data. Society must accept governance by tiny groups over systems trained on global
                        knowledge. And democracies must either embrace centralized control over data, compute, and
                        AI-powered decision-making or cede AI leadership to nations who do. Each of these dilemmas
                        stems from the same technical limitation: AI systems which fail to enable AI users and data
                        sources to control when and how AI is used... ceding that power to central AI creators instead.
                    </p>

                    <figure class="full-width">
                        <img src="1411.3146/centralization_vs_communication.png"
                            alt="AI as intermediary vs communication technology">
                        <figcaption>(left) An illustration of AI as an intermediary between those who have information
                            and
                            those who seek insights. (right) An illustration of AI as a communication technology
                            connecting
                            people directly.</figcaption>
                    </figure>

                    <p>Yet these challenges also reveal an opportunity. By reformulating how AI systems process
                        information
                        (shifting from operations that break ABC to ones that preserve it) society is actively
                        overcoming
                        the perception of AI as a tool of central intelligence, revealing it to be a communication
                        paradigm
                        of great magnitude. And as described above, this change will not just solve technical problems;
                        it
                        could realign AI development with democratic values. The path to this transformation begins with
                        the
                        mathematical operations which preserve or destroy ABC in AI.</p>
                </section>

                <section>
                    <h2 id="underlying-causes">Underlying Causes and Contributing Factors</h2>

                    <figure class="full-width">
                        <img src="1411.3146/abc_7_v7.png"
                            alt="ABC is underpinned by the addition, branching, and copy problems">
                        <figcaption>ABC is underpinned by the addition, branching, and copy problems.</figcaption>
                    </figure>

                    <h3 id="addition-problem">Cause: Attribution and The Addition Problem</h3>

                    <p>
                        When a person concatenates two numbers together, they preserve the integrity of each
                        contributing piece of information. However, when a person adds two numbers together, they
                        produce a
                        result which loses knowledge of the original values, and thus also loses the ability to
                        precisely
                        remove one of those values later. Consider the following example:
                    </p>

                    <div class="definition-box">
                        <p><strong>Concatenation preserves sources and enables removal:</strong></p>
                        <p>"1" + "6" = "16" (can identify and remove either "1" or "6")</p>
                        <p>"2" + "5" = "25" (can identify and remove either "2" or "5")</p>
                        <p><strong>Addition erases them:</strong></p>
                        <p>1 + 6 = 7 (cannot identify and then remove either input)</p>
                        <p>2 + 5 = 7 (cannot identify and then remove either input)</p>
                    </div>

                    <p>
                        In the example above, receiving the number “7” provides no information on what numbers
                        were used to create it, while receiving the number “16” preserves the numbers used to create it.
                    </p>

                    <p>
                        In the same way, when a deep learning AI model is trained, each datapoint is translated into
                        a ”gradient update” which is <em>added into the previous weights of the model</em>, losing the
                        ability to
                        know which aspects of each weight were derived from which datapoints and thus the ability to
                        later remove specific datapoints’ influence (<sup><a href="#ref-18" class="cite">I</a></sup>).
                        Thus, despite recent
                        work,
                        when an AI model makes a prediction, no-one can verify which source datapoints are informing
                        that prediction (i.e. influence functions), nor can they cleanly remove problematic datapoints’
                        influence (i.e. ”unlearning”) without retraining the entire model. Despite attempts, influence
                        function based unlearning remains an unsolved challenge (<sup><a href="#ref-51"
                                class="cite">T</a></sup>), blocking an
                        ABC solution... because addition fundamentally destroys information which cannot then be
                        recovered. Addition blocks attribution, which blocks attribution-based control in AI systems.
                    </p>

                    <h3 id="copy-problem">Cause: Control and The Copy Problem</h3>

                    <p>
                        When a person makes a copy of their information and gives it to another, they lose the ability
                        to enforce how that information might be used (<sup><a href="#ref-30" class="cite">A</a></sup>).
                        And upon this core flaw,
                        even if AI attribution was solved (e.g. unlearning and influence functions were solved), certain
                        dominoes would still fall and avert ABC within AI systems.
                    </p>

                    <p>
                        To begin, consider that when training data providers give a copy of their training data to an
                        AI organization, they lose the ability to control how that data might be used, thereby losing
                        any enforceable control over how the resulting model might be used <a href="#sidenote-4"
                            class="sidenote-ref"><sup>4</sup></a>
                        . Similarly, if one gives
                        an AI model to someone else, the former loses the ability to control how that model would
                        be used. Taken together, because of the overuse of copying, AI is unilaterally controlled by
                        whomever has a copy of the trained model. At a minimum, this always includes the organization
                        responsible for its training (i.e. ”closed source AI”). In the maximum, this includes any person
                        who downloads a model from the internet (i.e., ”open source AI”).
                    </p>

                    <div class="sidenote" id="sidenote-4">
                        <p>
                            <sup>4</sup>
                            or how it might be value aligned
                        </p>
                    </div>

                    <p>
                        While many debate the merits of so-called ”closed source” or ”open source” AI models, the
                        copy problem underpins a constructive criticism of both: both are systems of unilateral control
                        because of the over-use of copying. And the over-use of copying doesn’t just create systems
                        which <em>can</em> be unilaterally controlled, it creates systems which <em>must be unilaterally
                            controlled</em>...
                        systems which explicitly prohibit collective control. Taken together, addition blocks
                        attribution,
                        copying blocks collective control,and together these foundational flaws block attribution-based
                        control in AI systems.
                    </p>

                    <h3 id="branching-problem">Cause: Bi-directional Delegation and The Branching Problem</h3>

                    <p>
                        When a person needs to synthesize information from a small number of sources, they can often
                        leverage their existing trust networks (people, brands, and various groups or institutions they
                        are
                        familiar with) to obtain high-veracity information they trust. Similarly, when a person needs to
                        broadcast information to a small number of recipients, they can often evaluate each recipient’s
                        trustworthiness before sharing, and in-so-doing protect their privacy, further their interests,
                        and
                        broadly avert what they might consider to be information mis-use. However, when a person
                        needs to synthesize information from billions of sources or broadcast to billions of recipients,
                        they necessarily cannot evaluate each party individually (<sup><a href="#ref-83"
                                class="cite">R</a></sup>) they are forced to
                        delegate evaluation to intermediaries who scale trust evaluation on their behalf. And upon this
                        core constraint, even if AI attribution was solved (e.g., unlearning and influence functions
                        were
                        solved) and collective control was solved (e.g. the copy problem), certain dominoes would still
                        fall and avert ABC within AI systems.
                    </p>

                    <p>
                        To begin, consider that when sources contribute training data to high-branching AI systems
                        (where billions of sources branch into a single model), they lose the ability to evaluate which
                        of billions of users will leverage their contribution... thereby losing any enforceable control
                        over whether those users are trustworthy or align with their values. Similarly, when users query
                        high-branching AI systems (where a single model branches out to billions of users), they lose
                        the ability to evaluate which of billions of sources inform their predictions... thereby losing
                        any enforceable control over whether those sources are reliable, independent, or free from
                        conflicts of interest. Taken together, because of the overuse of high-branching aggregation, AI
                        concentrates source and user selection at whichever entities facilitate high-branching
                        operations:
                        the platforms connecting billions of sources to billions of users (<sup><a href="#ref-84"
                                class="cite">R</a>;<a href="#ref-85" class="cite">R</a></sup>).
                    </p>

                    <p>
                        While platforms may attempt to perform this selection faithfully, the branching problem
                        underpins a structural limitation: high-branching operations mathematically necessitate
                        centralized selection authority at information bottlenecks (<sup><a href="#ref-20"
                                class="cite">L</a></sup>). And the overuse of
                        high-branching aggregation doesn’t just create systems which <em>can</em> have centralized
                        selection,
                        it creates systems which <em>must</em> have centralized selection, systems which explicitly
                        prohibit
                        distributed bilateral authority over source and user selection. Taken together, addition blocks
                        attribution, copying blocks collective control, and high-branching blocks distributed selection.
                        Tkaen together, addition, branching, and copying avert attribution-based control in AI systems.
                    </p>

                    <h3 id="natural-problem">Contributing Factors: Consensus and the Natural Problem</h3>

                    <p>
                        Yet technical barriers alone may not explain ABC’s absence. Perhaps surprisingly, this thesis
                        will describe how the technical ingredients for an ABC solution already operate at scale in
                        specific AI contexts. This disconnect between possibility and adoption demands explanation.
                    </p>

                    <p>
                        Consider the incentives: democratic nations seek to compete with authoritarian AI programs,
                        data owners want to maximize revenue, and societies aim to enhance AI capabilities while
                        ensuring those capabilities benefit society broadly. ABC could advance all these goals, and, as
                        this thesis surveys, local incentives have inspired local solutions to emerge and become tested
                        at
                        scale. Yet ABC remains conspicuously absent from mainstream AI development. Why?
                    </p>

                    <p>
                        While a complete sociological analysis is beyond the technical scope of this thesis, examining
                        current AI development reveals a striking pattern: researchers consistently draw inspiration
                        from a specific conception of biological cognition (<sup><a href="#ref-18" class="cite">I</a>;
                            <a href="#ref-18" class="cite">S</a>; <a href="#ref-87" class="cite">Y</a></sup>), cognition
                        that provides no attribution-based control over its predictions.
                        For
                        example, humans do not ask permission from their parents, friends, co-workers, or evolutionary
                        ancestors every time they seek to use a concept taught to them by these external parties (i.e.
                        every time they open their mouth or move their body... all actions learned at some point from
                        another party). For those who see AI development as fundamentally about imitating this meme
                        of biological cognition, ABC represents a deviation from nature’s blueprint. <a
                            href="#sidenote-5" class="sidenote-ref"><sup>5</sup></a>
                    </p>

                    <div class="sidenote" id="sidenote-5">
                        <p>
                            <sup>5</sup>
                            However, there are other conceptions of natural intellignece which are compatible with this
                            vision, such as
                            collective or networked intelligence, although they have somewhat fallen out of favor since
                            the peak of theories like
                            cybernetics. It remains to be seen whether the popular meme of intelligence will expand to a
                            more networked form.
                    </div>

                    <p>
                        Moreover, ABC may threaten established business models. Large AI companies have built
                        multi-billion (soon, multi-trillion?) dollar valuations in part on their ability to use training
                        data
                        without compensating providers (<sup><a href="#ref-88" class="cite">V</a></sup>), and to bend
                        the behavior of AI models in a
                        way that suits them. For example, the targeted advertising industry, projected to reach over
                        a trillion dollars by 2030 (<sup><a href="#ref-89" class="cite">S</a></sup>), relies heavily on
                        AI systems that
                        obscure rather than expose the sources influencing their recommendations (<sup><a href="#ref-90"
                                class="cite">E</a></sup>), and
                        enable the AI owner to sell influence over the model to the highest bidder (e.g. ads in newsfeed
                        algorithms) (<sup><a href="#ref-91" class="cite">S</a></sup>). An AI deployment with true ABC
                        might disrupt both of
                        these profitable arrangements, arrangements which are the primary business models of several
                        of the largest AI developers.
                    </p>

                    <p>
                        This relationship to ABC reflects a deeper tension in AI development: the choice between
                        transparency and mystique. In the case that ABC provides precise, controllable, and attributable
                        intelligence, many influential voices may prefer the allure and profitability of black-box
                        systems
                        that more closely mirror a specific conception of biological cognition. Yet, this dichotomy does
                        not just shape technical decisions, but who retains power over AI, and the future of AI’s role
                        in
                        society. As readers proceed through this thesis, they might consider: what if this preference
                        for
                        black-box AI systems stems not from technical necessity, from safety maximization, or from
                        economic/political incentive, but from deeply held assumptions about what artificial
                        intelligence
                        should be? For now, the thesis will overlook such concerns, returning to them in closing notes.
                    </p>
                </section>

                <section>
                    <h2 id="thesis-outline">Thesis Outline</h2>

                    <figure class="full-width">
                        <img src="1411.3146/abc_8_v4.png"
                            alt="The addition, branching, and copy problems addressed in chapters 2, 3, and 4">
                        <figcaption>The addition, branching, and copy problems addressed in chapters 2, 3, and 4 through
                            the
                            use of concatenation, encryption, and recursion to provide attribution-based control.
                        </figcaption>
                    </figure>

                    <p>
                        Across three chapters, this thesis progressively constructs a new paradigm for AI development
                        and deployment by describing how breakthroughs in deep learning, distributed systems,
                        and cryptography might: avert the addition, branching, and copy problems, provide a means for
                        AI systems with attribution-based control, and better align AI with free and democratic society.
                    </p>

                    <h3 id="chapter-2-outline">Chapter 2: Addition&mdash;From Deep Learning to Deep Voting</h3>

                    <p>The thesis begins at the mathematical center of modern AI, artificial neural networks. Chapter
                        2 reveals how reliance on addition operations in deep learning averts attribution and promotes
                        centralized control into AI systems.</p>

                    <figure class="full-width">
                        <img src="1411.3146/deep_learning_to_deep_voting_v2.png"
                            alt="Deep learning to deep voting transformation">
                        <figcaption>
                            Traditional open/closed-source deep learning systems (left) pool all data into a
                            deep learning model (i.e. by adding weight updates) which is later used for predictions,
                            while
                            deep voting systems (right) learn source-specific model parameters which remain partitioned
                            until prediction time, at which point they can be rapidly synthesized to make a prediction.
                            In
                            such rapid synthesis, attribution metadata flows (concatenated) with predictions, enabling
                            source
                            verification. Darker lines indicate information flow being used for training or prediction.
                            Lighter
                            lines indicate data sources which could have participated in the prediction, but were
                            excluded.
                        </figcaption>
                    </figure>

                    <p>
                        Chapter 2 surveys recent developments in sparse deep learning architectures and cryptographic
                        protocols that displace some addition operations with concatenation operations. The
                        chapter describes how these architectural modifications preserve mappings between training
                        examples and their influence on specific predictions, a property that standard neural networks
                        lose through parameter addition and dense inference. The chapter formalizes these architectures
                        as <em>deep voting</em>: ensemble methods where model components trained on disjoint data
                        subsets
                        (and which remain in a concatenated state after training) only merge through addition during
                        inference in a more linear fashion, and perform that merge in a way which is bounded by
                        <em>intelligence budgets</em>. In this way, it describes how recent work in deep learning
                        sparsity and
                        cryptography can be combined to control source-prediction influence in deep learning systems.
                    </p>

                    <p>
                        The chapter then documents empirical results demonstrating that deep voting architectures
                        achieve comparable accuracy to standard deep learning while maintaining per-source attribution.
                        This property enables two capabilities unavailable in addition-based architectures: users can
                        specify weights over training sources after training (dynamic source selection), and sources
                        can withdraw their contributions from specific predictions (selective unlearning). These results
                        establish that AI <em>attribution</em> (maintaining verifiable mappings from predictions back to
                        contributing training sources) is architecturally feasible at scale, addressing a necessary
                        requirement for
                        attribution-based control in AI systems.
                    </p>

                    <h3 id="chapter-3-outline">Chapter 3: Copying&mdash;From Deep Voting to Network-source AI</h3>

                    <p>
                        While Chapter 2 synthesizes techniques for AI with attribution, it surfaces a deeper challenge:
                        how can we enable selective sharing of information without losing control through copying?
                        Chapter 3 surveys recent cryptographic breakthroughs and reframes them in an end-to-end
                        framework: <em>structured transparency</em>, describing how combinations of cryptographic
                        algorithms
                        can allow data owners to collectively enable some uses of their data without enabling other
                        uses.
                    </p>

                    <figure class="full-width">
                        <img src="1411.3146/deep_voting_to_network_source_ai_v2.png"
                            alt="Deep voting to network source AI">
                        <figcaption>
                            Deep voting systems (left) require the copying of data from source to the holder of
                            an AI model, while network-source AI systems (right) leverage cryptography and distributed
                            systems to enable data holders to retain control over the only copy of their information...
                            and
                            the predictive capability that information lends to an AI system. In network-source AI, AI
                            users directly query vast numbers of data owners requesting predictive capability for a
                            single
                            prediction, learning only the output of their prediction in the process. Darker lines
                            indicate
                            sources being used for a prediction.
                        </figcaption>
                    </figure>

                    <p>
                        When applied to the attribution-preserving networks from Chapter 2, structured transparency
                        enables a new kind of AI system, one where predictive power flows freely through society without
                        requiring centralized data collection. Data owners can contribute to collective intelligence
                        while
                        maintaining strict control over which predictions their information is used to support,
                        seemingly
                        fulfilling the technical requirements for attribution-based control (ABC) in AI systems.
                    </p>

                    <h3 id="chapter-4-outline">Chapter 4: Branching&mdash;From Network-Src AI to Broad Listening</h3>

                    <p>
                        Chapters 2 and 3 provide the technical foundation for ABC in AI, enabling data sources to
                        calibrate who they wish to imbue with predictive power, and enabling AI users to calibrate who
                        they trust to provide them with value-aligned predictive power. However, given the billions of
                        people in the world, a new problem arises: how can data sources or AI users know who to trust?
                    </p>

                    <h4>The Scale Problem</h4>

                    <p>
                        ABC requires distributed authority over billions of potential sources, but humans can sustain
                        high-trust relationships with approximately 150 entities (i.e., Dunbar’s Number). This creates
                        a gap of seven orders of magnitude between what ABC requires (∼ 10<sup>9</sup>
                        sources) and what
                        individuals can evaluate (∼ 10<sup>2</sup>
                        relationships). Delegation becomes architecturally necessary,
                        but which delegation architecture preserves ABC’s requirement for distributed authority?
                    </p>

                    <div class="callout">
                        <p class="callout-title">The Dunbar Gap</p>

                        <p>
                            Consider your close friends (people you’d call if you needed help moving or advise about a
                            major life decision). Most people sustain perhaps a dozen such relationships. Expand to
                            include colleagues you genuinely know (not just recognize), neighbors you actually talk to,
                            family you stay in touch with, (etc.). Dunbar suggests this number caps around 150.
                        </p>

                        <p>
                            Now consider what ABC requires: evaluating which of billions of potential sources to trust,
                            which queries to allow, or which contexts are appropriate. The gap isn’t close, its seven
                            orders of magnitude off. It’s not like needing to manage 200 relationships when you can
                            handle 150. It’s needing to manage <em>billions</em> when you can handle <em>hundreds</em>.
                        </p>

                        <p>
                            Asking individuals to directly evaluate trust at ABC’s required scale is like asking
                            someone to maintain close friendships with every person in China (not theoretically
                            impossible, just absurdly incompatible with how human relationships actually work).
                        </p>
                    </div>

                    <h4>1st Criterion: Maximizing Breadth</h4>

                    <p>
                        To address this scale problem while preserving ABC’s distributed authority, Chapter 4 identifies
                        two criteria that any delegation architecture must satisfy. The first is the <em>breadth
                            criterion</em>.
                        Truthfinding requires aggregating from many independent sources because coordinated deception
                        becomes logistically harder when information must be consistent across many
                        independentlyweighted witnesses. However, filtering through centralized bottlenecks reduces this
                        advantage.
                        If all sources must pass through a single institution, coordination difficulty decreases since
                        fewer
                        parties must align their story to coordinate a compelling deception.
                    </p>

                    <figure class="full-width">
                        <img src="1411.3146/breadth_assumption_v2.png" alt="Breadth criterion for trust evaluation">
                        <figcaption>
                            Coordinating deception becomes harder as the number of independent sources
                            increases, but only when those sources can be independently weighted without passing through
                            centralized bottlenecks. Aggregating from few sources (left) reduces coordination
                            difficulty,
                            while aggregating from many independent sources (right) makes coordination harder to achieve
                            undetected. Darker lines indicate sources being used for a prediction.
                        </figcaption>
                    </figure>

                    <h4>2nd Criterion: Maximizing Depth</h4>

                    <p>
                        The second criterion is the <em>depth criterion</em>. Trust evaluation requires sustained
                        relationships
                        that enable: observing behavior over repeated interactions, understanding context to verify
                        independence, experiencing consequences through social closure, and maintaining accountability
                        through ongoing reciprocity. This cannot be achieved through weak ties or at web-scale per
                        evaluator; trust assessment requires strong ties maintained within bounded relationship
                        capacity.
                    </p>

                    <figure class="full-width">
                        <img src="1411.3146/depth_assumption_v2.png" alt="Depth criterion for trust evaluation">
                        <figcaption>
                            Trust evaluation requires sustained relationships that enable observation over time,
                            verification of independence, and reputational consequences through social closure.
                            Aggregating
                            from untrusted relationships (left) reduces information authenticity relative to aggregating
                            from
                            trusted relationships (right), because relationships create mutual stakes... co-dependence
                            on
                            truthful information exchange now and in the future. Darker lines indicate sources being
                            used
                            for a prediction. Line sparsity indicates trust level.
                        </figcaption>
                    </figure>

                    <h4>The Conflict Between Breadth/Depth and Dunbar's Number</h4>

                    <p>
                        These criteria are in direct conflict. The breadth criterion would be maximized by aggregating
                        from billions of sources to make coordination difficult, while the depth criterion would be
                        maximized by only trusting parties with whom one has spent great deals of time developing a
                        high-trust relationship (approximately 150 sustained relationships according to Dunbar).
                    </p>

                    <p>
                        One might surmise that the only possible solution to this conflict is delegation, to maintain
                        150 strong relationships with parties who possess the scale to create relationships with
                        billions
                        of entities in the world: web-scale institutions. However, such a high-branching social graph
                        (high branching in two places, as such institutions accept information from billions of sources
                        and provide services to billions of recipients) violates the definition of attribution-based
                        control.
                    </p>

                    <p>
                        When individuals connect to institutions and institutions connect to billions of sources, all
                        information flow passes through institutional nodes that occupy structural holes... positions
                        controlling information flow between otherwise disconnected parties. This bottleneck structure
                        violates the depth criterion through multiple mechanisms: monitoring costs prevent institutional
                        evaluation of independence across billions of source pairs, relationships between institutions
                        themselves and their sources become weak ties at scale (e.g., problems within Trust and Safety
                        in
                        online platforms, Sybil attacks, and Spam) as fixed institutional resources divide across
                        billions
                        of sources, social closure disappears due to negligible network overlap between sources at
                        billion
                        scale, and institutions necessarily control source selection from their bottleneck positions.
                        The
                        19
                        architectural bottleneck thus concentrates authority over source selection at institutional
                        nodes,
                        violating ABC’s requirement for distributed authority regardless of institutional intentions or
                        policies. Taken together, high-branching social institutions are unfit to offer ABC in AI
                        systems.
                    </p>

                    <h4>The Solution: Low-Branching Recursive Delegation</h4>

                    <figure class="full-width">
                        <img src="1411.3146/scale_assumption.png" alt="High-branching vs low-branching delegation">
                        <figcaption>
                            High-branching delegation (left) connects directly to many sources but creates bottlenecks
                            and exceeds evaluation capacity. Low-branching recursion (right) achieves exponential
                            reach through depth while preserving evaluation capacity at each hop. Each person maintains
                            ∼50 verifiable connections; over 6 hops this reaches 50<sup>6</sup> = 15.6 billion sources.
                            Darker
                            lines
                            indicate sources being used for a prediction. The architecture leverages small-world network
                            properties where people are connected through short paths.
                        </figcaption>
                    </figure>

                    <p>
                        Chapter 4 proposes low-branching recursive delegation as an alternative architecture that
                        satisfies both criteria by achieving exponential reach, propagating trust recursively over
                        <em>H</em> hops.
                        For example, if <em>H</em> = 6 (e.g. six degrees of separation), this approach would achieve
                        exponential
                        reach of 50<sup>6</sup> = 15.6 billion sources.
                    </p>

                    <figure class="full-width">
                        <img src="1411.3146/network_source_ai_to_broad_listening_v2.png"
                            alt="Network source AI to broad listening">
                        <figcaption>
                            Network-source AI (left) sources information directly from eye-witnesses while
                            broad listening (right) leverages information from the same sources weighted through a
                            social
                            graph. Darker lines indicate sources being used for a prediction. Dotted sparsity indicates
                            trust.
                        </figcaption>
                    </figure>

                    <p>
                        This pattern has precedent in systems achieving billion-scale reach with distributed authority:
                        PageRank (pages link to ∼10-100 pages, authority distributes to authors), academic citations
                        (papers cite ∼10-50 sources, authority distributes to researchers), PGP web of trust (users
                        sign ∼10-50 keys, authority distributes to users), and Wikipedia (articles cite ∼10-50 sources,
                        authority distributes to editors).
                    </p>

                    <figure class="full-width">
                        <img src="1411.3146/deep_learning_to_broad_listening_v2.png"
                            alt="From deep learning to broad listening">
                        <figcaption>
                            Traditional AI (left) compresses all sources into a model, obfuscating source
                            influence and control while broad listening (right) leverages information from relevant
                            sources,
                            weighted through a social graph. Darker lines indicate sources being used for a prediction.
                        </figcaption>
                    </figure>

                    <p>
                        This massive path redundancy eliminates bottlenecks. No single party controls access
                        to sources, and users can route around failing or captured intermediaries through alternative
                        verified paths. The architecture simultaneously satisfies the depth criterion by constraining
                        each
                        participant to approximately 50 connections (k = 50 < 150), preserving capacity for strong ties
                            with sustained evaluation, verification of independence across manageable pairs (\(
                            \binom{50}{2}=1{,}225 \) pairs), and social closure through network overlap. </p>

                            <p>
                                Because verification occurs at each hop rather than concentrating at institutional
                                nodes, the
                                quality of trust evaluation remains high throughout the recursive chain. Each person
                                evaluates
                                their 50 connections through sustained relationships, and those connections evaluate
                                their 50
                                through sustained relationships, creating verified paths from users to billions of
                                ultimate sources.
                                Chapter 4 formalizes this architecture for ABC by integrating recursive trust
                                propagation with
                                intelligence budgets from Chapter 2 and privacy-preserving structured transparency from
                                Chapter
                                3. The result: each person can synthesize information from billions of sources, weighted
                                through
                                local trust relationships, and verified at each hop, transforming network-source AI’s
                                raw listening
                                capacity into trust-weighted ”word-of-mouth at global scale.” Taken together, they
                                provide a
                                viable path towards attribution-based control in AI systems.
                            </p>




                            <h3 id="chapter-5-outline">Chapter 5: Conclusion&mdash;AI as a Communication Tool</h3>

                            <p>
                                Chapter 5 returns to examine how the technical developments in Chapters 2-4 address the
                                cascading consequences of missing ABC laid out in this introduction. It demonstrates how
                                concatenation-based networks solve the addition problem by preserving source
                                attribution, how
                                encrypted computation solves the copy problem by enabling sharing without loss of
                                control,
                                and how recursion solves the branching problem by enabling a digital word-of-mouth at
                                scale
                                through AI systems.
                            </p>

                            <figure class="full-width">
                                <img src="1411.3146/abc_final_v4.png" alt="Solutions propagate through the cascade">
                                <figcaption>Solutions in addition, branching, and copy problems propagate to societal
                                    problems.</figcaption>
                            </figure>

                            <p>
                                The chapter then analyzes how these capabilities fundamentally transform the nature of
                                artificial intelligence. When AI relies on addition, copying, and branching it
                                necessarily becomes
                                a tool of central control, forcing us to trust whoever possesses a copy of the model.
                                But as AI
                                comes to preserve ABC, it enables something radically different: a communication tool
                                that
                                connects those who have knowledge with those who seek insights. Beyond a technical
                                shift,
                                ABC actively re-frames AI from a system that produces intelligence through
                                centralization into
                                a system that enables intelligence to flow freely through society.
                            </p>

                            <figure class="full-width">
                                <img src="1411.3146/broad_listening_global.png" alt="Broad listening at global scale">
                                <figcaption>A fictional illustration of the premise of attribution-based control
                                    enabling
                                    <em>broad listening</em> predictions which traverse the entire free market's data
                                    and
                                    compute — as opposed to merely the data and compute which can be centralized by a
                                    single
                                    organization or nation.
                                </figcaption>
                            </figure>

                            <p>
                                The chapter concludes by arguing that democratic societies face an architectural choice
                                that will determine the future of AI: continue building systems that mathematically
                                require
                                central control, or embrace AI architectures that enable broad listening. It
                                demonstrates why
                                broad listening uniquely addresses the individual problems of hallucination and
                                disinformation
                                through source verification, the institutional problems of data sharing through aligned
                                contributor
                                incentives, the societal problems of governance through continuous public consent, and
                                the
                                geopolitical challenge of competing with authoritarian AI by harnessing the collective
                                resources
                                of the free market. Taken together, the chapter argues that ABC isn’t just technically
                                possible,
                                it’s essential for transforming AI from a technology that advantages central control
                                into one that
                                advantages democratic values. The future of AI need not be a race to centralize. Through
                                broad
                                listening, it can become a race to connect through AI systems with attribution-based
                                control.
                            </p>
                </section>

                <section class="references">
                    <h2>References</h2>
                    <ol>
                        <li id="ref-1"><span class="authors">Dziri, N., Milton, S., Yu, M., Zaiane, O., & Reddy,
                                S.</span>
                            (2022). <a href="https://aclanthology.org/2022.naacl-main.387"><span class="title">On the
                                    Origin
                                    of Hallucinations in Conversational Models: Is it the Datasets or the
                                    Models?</span></a>
                            <span class="venue">Proceedings of NAACL 2022</span>, 5271–5285.
                        </li>
                        <li id="ref-2"><span class="authors">Vaccari, C., & Chadwick, A.</span> (2020). <a
                                href="https://doi.org/10.1177/2056305120903408"><span class="title">Deepfakes and
                                    Disinformation: Exploring the Impact of Synthetic Political Video on Deception,
                                    Uncertainty, and Trust in News.</span></a> <span class="venue">Social Media +
                                Society</span>, 6(1).</li>
                        <li id="ref-3"><span class="authors">Zuccon, G., Koopman, B., & Shaik, R.</span> (2023). <a
                                href="https://doi.org/10.1145/3624918.3625329"><span class="title">ChatGPT Hallucinates
                                    when
                                    Attributing Answers.</span></a> <span class="venue">SIGIR-AP '23</span>, 46–51.</li>
                        <li id="ref-4"><span class="authors">Gravel, J., D'Amours-Gravel, M., & Osmanlliu, E.</span>
                            (2023).
                            <a href="https://doi.org/10.1016/j.mcpdig.2023.05.004"><span class="title">Learning to Fake
                                    It:
                                    Limited Responses and Fabricated References Provided by ChatGPT for Medical
                                    Questions.</span></a> <span class="venue">Mayo Clinic Proceedings: Digital
                                Health</span>, 1(3), 226–234.
                        </li>
                        <li id="ref-5"><span class="authors">Xu, Z., Jain, S., & Kankanhalli, M.</span> (2024). <a
                                href="https://arxiv.org/abs/2401.11817"><span class="title">Hallucination is Inevitable:
                                    An
                                    Innate Limitation of Large Language Models.</span></a> <span
                                class="venue">arXiv:2401.11817</span>.</li>
                        <li id="ref-6"><span class="authors">Yu, L., Cao, M., Cheung, J. C., & Dong, Y.</span> (2024).
                            <a href="https://aclanthology.org/2024.findings-emnlp.466"><span class="title">Mechanistic
                                    Understanding and Mitigation of Language Model Non-Factual
                                    Hallucinations.</span></a>
                            <span class="venue">Findings of EMNLP 2024</span>, 7943–7956.
                        </li>
                        <li id="ref-7"><span class="authors">Trask, A., Bluemke, E., Garfinkel, B., Ghezzou
                                Cuervas-Mons,
                                C., & Dafoe, A.</span> (2020). <a href="https://arxiv.org/abs/2012.08347"><span
                                    class="title">Beyond Privacy Trade-offs with Structured Transparency.</span></a>
                            <span class="venue">arXiv:2012.08347</span>.
                        </li>
                        <li id="ref-8"><span class="authors">Youssef, A., et al.</span> (2023). <a
                                href="https://doi.org/10.1001/jamanetworkopen.2023.48422"><span
                                    class="title">Organizational
                                    Factors in Clinical Data Sharing for Artificial Intelligence in Health
                                    Care.</span></a>
                            <span class="venue">JAMA Network Open</span>, 6, e2348422.
                        </li>
                        <li id="ref-9"><span class="authors">McMahan, H. B., Moore, E., Ramage, D., & Agüera y Arcas,
                                B.</span> (2016). <a href="https://arxiv.org/abs/1602.05629"><span
                                    class="title">Federated
                                    Learning of Deep Networks using Model Averaging.</span></a> <span
                                class="venue">arXiv:1602.05629</span>.</li>
                        <li id="ref-10"><span class="authors">Rieke, N., et al.</span> (2020). <a
                                href="https://doi.org/10.1038/s41746-020-00323-1"><span class="title">The future of
                                    digital
                                    health with federated learning.</span></a> <span class="venue">npj Digital
                                Medicine</span>, 3(1).</li>
                        <li id="ref-11"><span class="authors">Nguyen, T. T., et al.</span> (2024). <a
                                href="https://arxiv.org/abs/2209.02299"><span class="title">A Survey of Machine
                                    Unlearning.</span></a> <span class="venue">arXiv:2209.02299</span>.</li>
                        <li id="ref-12"><span class="authors">Amodei, D., et al.</span> (2016). <a
                                href="https://arxiv.org/abs/1606.06565"><span class="title">Concrete Problems in AI
                                    Safety.</span></a> <span class="venue">arXiv:1606.06565</span>.</li>
                        <li id="ref-13"><span class="authors">Gabriel, I.</span> (2020). <a
                                href="https://doi.org/10.1007/s11023-020-09539-2"><span class="title">Artificial
                                    intelligence, values, and alignment.</span></a> <span class="venue">Minds and
                                Machines</span>, 30(3), 411–437.</li>
                        <li id="ref-14"><span class="authors">Ntoutsi, E., et al.</span> (2020). <a
                                href="https://doi.org/10.1002/widm.1356"><span class="title">Bias in data-driven
                                    artificial
                                    intelligence systems—An introductory survey.</span></a> <span class="venue">WIREs
                                Data
                                Mining and Knowledge Discovery</span>, 10(3), e1356.</li>
                        <li id="ref-15"><span class="authors">Kaplan, J., et al.</span> (2020). <a
                                href="https://arxiv.org/abs/2001.08361"><span class="title">Scaling Laws for Neural
                                    Language
                                    Models.</span></a> <span class="venue">arXiv:2001.08361</span>.</li>
                        <li id="ref-16"><span class="authors">Hoffmann, J., et al.</span> (2022). <a
                                href="https://arxiv.org/abs/2203.15556"><span class="title">Training Compute-Optimal
                                    Large
                                    Language Models.</span></a> <span class="venue">arXiv:2203.15556</span>.</li>
                        <li id="ref-17"><span class="authors">Sutton, R.</span> (2019). <a
                                href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"><span class="title">The
                                    Bitter Lesson.</span></a> <span class="venue">Incomplete Ideas (blog)</span>.</li>
                        <li id="ref-18"><span class="authors">Goodfellow, I., Bengio, Y., & Courville, A.</span> (2016).
                            <a href="https://www.deeplearningbook.org/"><span class="title">Deep Learning.</span></a>
                            <span class="venue">MIT Press</span>.
                        </li>
                        <li id="ref-19"><span class="authors">Dunbar, R. I. M.</span> (1993). <a
                                href="https://doi.org/10.1017/S0140525X00032325"><span class="title">Coevolution of
                                    neocortical size, group size and language in humans.</span></a> <span
                                class="venue">Behavioral and Brain Sciences</span>, 16(4), 681–735.</li>
                        <li id="ref-20"><span class="authors">Freeman, L. C.</span> (1977). <a
                                href="https://www.jstor.org/stable/3033543"><span class="title">A set of measures of
                                    centrality based on betweenness.</span></a> <span class="venue">Sociometry</span>,
                            35–41.</li>
                        <li id="ref-21"><span class="authors">Gabriel, I., et al.</span> (2024). <a
                                href="https://arxiv.org/abs/2404.16244"><span class="title">The Ethics of Advanced AI
                                    Assistants.</span></a> <span class="venue">arXiv:2404.16244</span>.</li>
                        <li id="ref-22"><span class="authors">Mouton, C. A., Lucas, C., & Guest, E.</span> (2023). <a
                                href="https://www.rand.org/pubs/research_reports/RRA2977-1.html"><span class="title">The
                                    Operational Risks of AI in Large-Scale Biological Attacks: A Red-Team
                                    Approach.</span></a> <span class="venue">RAND Corporation</span>.</li>
                        <li id="ref-23">
                            <span class="authors">E. Schmidt and J. Rosenberg. </span> 2014.
                            <em>How google works</em>
                            <span>Grand Central Publishing</span>.
                        </li>

                        <li id="ref-24">
                            <span class="authors">M. Bhattacharyya, V. M. Miller, D. Bhattacharyya, and L. E. Miller.
                                2023.
                                High rates of
                                fabricated and inaccurate references in chatgpt-generated medical content. </span>
                            <em>Cureus,</em>
                            <span> 15(5)</span>.
                        </li>

                        <li id="ref-25">
                            <span class="authors">T. Devriendt, M. Shabani, and P. Borry. 2021. </span>
                            Data sharing in biomedical sciences: A systematic
                            review of incentives.
                            <em> Biopreservation and Biobanking</em>, 19(3):219–227. PMID: 33926229
                        </li>

                        <li id="ref-26">
                            <span class="authors"> L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C.
                                Zhang,
                                S. Agarwal,
                                K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell,
                                P. Welinder, P. F. Christiano, J. Leike, and R. Lowe. 2022. Training language models
                                to follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal,
                                D. Belgrave, K. Cho, and A. Oh, editors
                            </span>
                            <em> Advances in Neural Information Processing
                                Systems</em>, volume 35, pages 27730–27744. Curran Associates, Inc.
                        </li>

                        <li id="ref-27">
                            <span class="authors">Z. Roozbahani. 2025. A review of methods for reducing hallucinations
                                in
                                generative artificial
                                intelligence to enhance knowledge economy.
                            </span>
                            <em>Knowledge Economy Studies.</em>
                        </li>

                        <li id="ref-28">
                            <span class="authors"> M. A. Ahmad, I. Yaramis, and T. D. Roy. 2023. Creating trustworthy
                                llms:
                                Dealing with
                                hallucinations in healthcare ai.
                            </span>
                        </li>

                        <li id="ref-29">
                            <span class="authors">P. Manakul, A. Liusie, and M. J. F. Gales. 2023. Selfcheckgpt:
                                Zero-resource
                                black-box
                                hallucination detection for generative large language models
                            </span>
                        </li>

                        <li id="ref-30">
                            <span class="authors">A. Trask, E. Bluemke, B. Garfinkel, C. G. Cuervas-Mons, and A. Dafoe.
                                2020.
                                Beyond privacy
                                trade-offs with structured transparency.
                            </span>

                            <em>CoRR</em>, abs/2012.08347.
                        </li>

                        <li id="ref-31">
                            <span class="authors"> A. Golodny. 2023. Senate ai forum focuses on intellectual property
                                issues, 12.
                            </span>
                        </li>

                        <li id="ref-32">
                            <span class="authors">J. Jeong, B. L. Vey, A. Reddy, T. Kim, T. Santos, R. Correa, R. Dutt,
                                M.
                                Mosunjac, G. OpreaIlies, G. Smith, M. Woo, C. R. McAdams, M. S. Newell, I. Banerjee, J.
                                Gichoya, and H. Trivedi. 2022.
                            </span>
                            <span class="venue">The emory breast imaging dataset (embed): A racially diverse, granular
                                dataset of 3.5m screening and diagnostic mammograms</span>
                        </li>

                        <li id="ref-33">
                            <span class="authors"> P. D. Y. Trieu, C. Mello-Thoms, M. Barron, and S. Lewis. 2023
                            </span>
                            <span class="venue">Look how far we have come:
                                Breast cancer detection education on the international stage</span>

                            <em>Frontiers in Oncology</em>, 12, 01.
                        </li>

                        <li id="ref-34">
                            <span class="authors">T. Onega, E. Beaber, B. Sprague, W. Barlow, J. Haas, A. Tosteson, M.
                                Schnall, K. Armstrong,
                                M. Schapira, B. Geller, D. Weaver, and E. Conant. 2014.
                            </span>
                            <span class="venue">Breast cancer screening in an
                                era of personalized regimens a conceptual model and national cancer institute initiative
                                for
                                risk-based and preference-based approaches at a population level. </span>

                            <em>Cancer</em>, 120, 10.
                        </li>

                        <li id="ref-35">
                            <span class="authors">A. Youssef, M. Ng, J. Long, T. Hernandez-Boussard, N. Shah, A. Miner,
                                D.
                                Larson, and
                                C. Langlotz. 2023.
                            </span>
                            <span class="venue">Organizational factors in clinical data sharing for artificial
                                intelligence
                                in health care. </span>

                            <em>JAMA Network Open</em>, 6:e2348422, 12.
                        </li>

                        <li id="ref-36">
                            <span class="authors">J. Gould. 2015.
                            </span>
                            <span class="venue">Data sharing: Why it doesn’t happen. </span>

                            <em>Nature Jobs.</em>
                        </li>

                        <li id="ref-38">
                            <span class="authors">D. Grybauskas. 2023.
                            </span>
                            <span class="venue">Will twitter’s new rate limits really stop scraping? </span>

                            <em>Built In,</em> July. Online
                            article.
                        </li>

                        <li id="ref-39">
                            <span class="authors">M. O’Brien. 2025.
                            </span>
                            <span class="venue">Reddit sues ai company perplexity and others for ’industrial-scale’
                                scraping
                                of user comments. </span>

                            <em>Associated Press,</em> , October. Updated 4:41 PM GMT-4, October 22, 2025
                        </li>

                        <li id="ref-40">
                            <span class="authors">P. Samuelson. 2023.
                            </span>
                            <span class="venue">Generative ai meets copyright. </span>

                            <em>Science</em>, 381(6654):158–161.
                        </li>

                        <li id="ref-41">
                            <span class="authors">M. M. Grynbaum and R. Mac. 2023.
                            </span>
                            <span class="venue">The times sues openai and microsoft over a.i. use of
                                copyrighted work. </span>

                            <em>The New York Times</em>, December
                        </li>

                        <li id="ref-42">
                            <span class="authors">J. Patel. 2019.
                            </span>
                            <span class="venue">Bridging data silos using big data integration </span>

                            <em> International Journal of Database
                                Management Systems</em>, 11(3):01–06.
                        </li>

                        <li id="ref-43">
                            <span class="authors">K. Robison. 2024.
                            </span>
                            <span class="venue">OpenAI cofounder Ilya Sutskever says the way AI is built is about to
                                change.
                            </span>

                            <em> The Verge</em>, December. Accessed: December 31, 2024.
                        </li>

                        <li id="ref-44">
                            <span class="authors">H. B. McMahan, E. Moore, D. Ramage, and B. A. y Arcas. 2016.
                            </span>
                            <span class="venue">Federated learning of deep
                                networks using model averaging.
                            </span>

                            <em> CoRR</em>, abs/1602.05629.
                        </li>

                        <li id="ref-45">
                            <span class="authors">N. Rieke, J. Hancox, W. Li, F. Milletar`ı, H. R. Roth, S. Albarqouni,
                                S.
                                Bakas, M. N. Galtier,
                                B. A. Landman, K. Maier-Hein, S. Ourselin, M. Sheller, R. M. Summers, A. Trask, D. Xu,
                                M. Baust, and M. J. Cardoso. 2020.
                            </span>
                            <span class="venue">The future of digital health with federated learning.
                            </span>

                            <em> npj
                                Digital Medicine</em>, 3(1), September.
                        </li>

                        <li id="ref-46">
                            <span class="authors">
                                I. Zhou, F. Tofigh, M. Piccardi, M. Abolhasan, D. Franklin, and J. Lipman. 2024.
                            </span>
                            <span class="venue">
                                Secure
                                multi-party computation for machine learning: A survey.
                            </span>

                            <em>
                                IEEE Access
                            </em>, 12:53881–53899.
                        </li>

                        <li id="ref-47">
                            <span class="authors">
                                A. Jadon and S. Kumar. 2023.
                            </span>
                            <span class="venue">
                                Leveraging generative ai models for synthetic data generation
                                in healthcare: Balancing research and privacy. In
                            </span>

                            <em>
                                2023 International Conference on Smart
                                Applications, Communications and Networking (SmartNets)
                            </em>, pages 1–4.
                        </li>

                        <li id="ref-48">
                            <span class="authors">
                                D. B. Rubin. 1993.
                            </span>

                            <span class="venue">
                                Statistical disclosure limitation.
                            </span>

                            <em>
                                2 Journal of official Statistics
                            </em>, 9(2):461–468.
                        </li>

                        <li id="ref-49">
                            <span class="authors">
                                A. C. Yao. 1982b.
                            </span>

                            <span class="venue">
                                Protocols for secure computations.
                            </span>pages 160–164.
                        </li>

                        <li id="ref-50">
                            <span class="authors">
                                J. L. Lobo, S. Gil-Lopez, and J. Del Ser. 2023.
                            </span>

                            <span class="venue">
                                The right to be forgotten in artificial intelligence:
                                issues, approaches, limitations and challenges
                            </span>
                            In
                            <em>2023 IEEE Conference on Artificial
                                Intelligence (CAI)</em>
                            , pages 179–180. IEEE.
                        </li>

                        <li id="ref-51">
                            <span class="authors">
                                T. Nguyen, T. T. Huynh, Z. Ren, P. L. Nguyen, A. W.-C. Liew, H. Yin, and Q. V. H.
                                Nguyen.
                                2024.
                            </span>

                            <span class="venue">
                                A survey of machine unlearning.
                            </span>
                        </li>

                        <li id="ref-52">
                            <span class="authors">
                                D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Mane. 2016.
                            </span>

                            <span class="venue">
                                Concrete
                                problems in ai safety.
                            </span>
                        </li>

                        <li id="ref-53">
                            <span class="authors">
                                I. Gabriel. 2020.
                            </span>

                            <span class="venue">
                                Artificial intelligence, values, and alignment.
                            </span>

                            <em> , 30(3):411–
                                437.</em>
                        </li>

                        <li id="ref-54">
                            <span class="authors">
                                C. A. Mouton, C. Lucas, and E. Guest. 2023.
                            </span>

                            <em> The Operational Risks of AI in Large-Scale
                                Biological Attacks: A Red-Team Approach.</em>
                            RAND Corporation, Santa Monica, CA.
                        </li>

                        <li id="ref-55">
                            <span class="authors">
                                E. Australian Government Department of Industry, Science and Resources. 2024.
                                Australia’s
                                ai
                                ethics principles.
                            </span>

                            <a href="https://www.industry.gov.au/publications/australias-ai-ethics-principles"><span
                                    class="title"> Accessed 12-09-2024.</span>
                            </a>
                        </li>

                        <li id="ref-56">
                            <span class="authors">
                                E. Commission. 2024b. Germany ai strategy report.
                            </span>

                            <a href="https://ai-watch.ec.europa.eu/countries/germany/germany-ai-strategy-report_en"><span
                                    class="title"> Accessed 12-09-2024</span>
                            </a>
                        </li>

                        <li id="ref-57">
                            <span class="authors">
                                E. Commission. 2024a. France ai strategy report.
                            </span>

                            <a href="https://ai-watch.ec.europa.eu/countries/france/france-ai-strategy-report_en"><span
                                    class="title"> Accessed 12-09-
                                    2024</span>
                            </a>
                        </li>

                        <li id="ref-58">
                            <span class="authors">
                                E. Commission. 2020.
                            </span>
                            White paper on artificial intelligence: A european approach
                            to excellence and trust.

                            <a
                                href="https://commission.europa.eu/publications/white-paper-artificial-intelligence-european-approach-excellence-and-trust_en"><span
                                    class="title"> Accessed 12-09-2024</span>
                            </a>
                        </li>

                        <li id="ref-59">
                            <span class="authors">
                                I. G. I. on Ethics of Autonomous and I. Systems. 2021.
                            </span>

                            <a href="https://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf"><span
                                    class="title"> Ethically aligned design: A
                                    vision for prioritizing human well-being with artificial intelligence and autonomous
                                    systems.
                                </span>
                            </a>
                            [Accessed 12-09-2024].
                        </li>

                        <li id="ref-60">
                            <span class="authors">
                                F. of Life Institute. 2024
                            </span>

                            <a href="https://futureoflife.org/open-letter/ai-principles/"><span class="title"> Ai
                                    principles.
                                </span>
                            </a>
                            [Accessed 12-09-2024].
                        </li>

                        <li id="ref-61">
                            <span class="authors">
                                A. Now. 2023.
                            </span>

                            <a
                                href="https://www.accessnow.org/press-release/the-toronto-declaration-protecting-the-rights-to-equality-and-non-discrimination-in-machine-learning-systems/"><span
                                    class="title"> The toronto declaration: Protecting the rights to equality and
                                    non-discrimination
                                    in machine learning systems.
                                </span>
                            </a>
                            [Accessed 12-09-2024].
                        </li>

                        <li id="ref-62">
                            <span class="authors">
                                A. N. Institute. 2024
                            </span>

                            <a href="https://ainowinstitute.org/publications/algorithmic-accountability-policy-toolkit"><span
                                    class="title">Algorithmic accountability policy
                                    toolkit.
                                    in machine learning systems.
                                </span>
                            </a>
                            [Accessed 12-09-2024].
                        </li>

                        <li id="ref-63">
                            <span class="authors">
                                M. Declaration. 2024
                            </span>

                            <a href="https://montrealdeclaration-responsibleai.com/"><span class="title">Montreal
                                    declaration for a responsible development of artificial
                                    intelligence.
                                </span>
                            </a>
                            [Accessed 12-09-2024].
                        </li>

                        <li id="ref-64">
                            <span class="authors">
                                W. E. Forum. 2023.
                            </span>

                            <a
                                href="https://www.weforum.org/publications/ai-governance-a-holistic-approach-to-implement-ethics-into-ai/"><span
                                    class="title">Ai governance: A holistic approach to implement ethics into ai.
                                </span>
                            </a>
                            [Accessed 12-09-2024].
                        </li>

                        <li id="ref-65">
                            <span class="authors">
                                H. B. K. C. for Internet Society. 2020
                            </span>

                            <a href="https://cyber.harvard.edu/publication/2020/principled-ai"><span
                                    class="title">Principled ai.
                                </span>
                            </a>
                            [Accessed 12-09-2024].
                        </li>

                        <li id="ref-66">
                            <span class="authors">
                                A. T. Institute. 2019.
                            </span>

                            <a
                                href="https://www.turing.ac.uk/sites/default/files/2019-08/understanding_artificial_intelligence_ethics_and_safety.pdf"><span
                                    class="title">Understanding artificial intelligence ethics and safety
                                </span>
                            </a>
                            [Accessed 12-09-2024].
                        </li>

                        <li id="ref-67">
                            <span class="authors">
                                Google. 2024.
                            </span>

                            <a href="https://ai.google/principles/"><span class="title">Ai principles
                                </span>
                            </a>
                            [Accessed 12-09-2024].
                        </li>

                        <li id="ref-68">
                            <span class="authors">
                                Microsoft. 2024.
                            </span>

                            <a href="https://www.microsoft.com/en-us/ai/principles-and-approach"><span class="title"> Ai
                                    principles and approach
                                </span>
                            </a>
                            [Accessed 12-09-2024].
                        </li>

                        <li id="ref-69">
                            <span class="authors">
                                IBM. 2018.
                            </span>

                            <a href="https://www.ibm.com/policy"><span class="title"> . Ibm principles for trust and
                                    transparency.
                                </span>
                            </a>
                            [Accessed 12-09-2024].
                        </li>

                        <li id="ref-70">
                            <span class="authors">
                                G. Riesen. 2023.
                            </span>

                            <span class="venue">
                                Imagine A World: What if global challenges led to more centralization?
                            </span>
                            Audio podcast episode, sep. Accessed: 2025-01-03
                        </li>

                        <li id="ref-71">
                            <span class="authors">
                                Q. Pope. 2023.
                            </span>

                            <span class="venue">
                                AI is centralizing by default; let’s not make it worse. Effective Altruism Forum,
                                sep.
                            </span>
                            Accessed: 2025-01-03.
                        </li>

                        <li id="ref-72">
                            <span class="authors">
                                crispweed. 2024.
                            </span>

                            <span class="venue">
                                The Alignment Trap: AI Safety as Path to Power. LessWrong, oct.
                            </span>

                            Accessed:
                            2025-01-03.
                        </li>

                        <li id="ref-73">
                            <span class="authors">
                                C. Summerfield, L. P. Argyle, M. Bakker, T. Collins, E. Durmus, T. Eloundou, I. Gabriel,
                                D. Ganguli, K. Hackenburg, G. K. Hadfield, et al. 2025.
                            </span>

                            <span class="venue">
                                The impact of advanced ai systems
                                on democracy
                            </span>

                            <em>Nature Human Behaviour,</em>

                            pages 1–11.
                        </li>

                        <li id="ref-74">
                            <span class="authors">
                                E. Welle. 2025
                            </span>

                            <span class="venue">
                                Aligning those who align AI, one satirical website at a time.
                            </span>

                            <em>The Verge,</em>

                            September. Accessed: 2025-10-23.
                        </li>

                        <li id="ref-75">
                            <span class="authors">
                                S. Samuel. 2024.
                            </span>

                            <span class="venue">
                                “i lost trust”: Why the openai team in charge of safeguarding humanity
                                imploded, May.
                            </span>
                        </li>

                        <li id="ref-76">
                            <span class="authors">
                                C. Hu and K. Cai. 2024. Sep.
                            </span>
                        </li>

                        <li id="ref-77">
                            <span class="authors">
                                J. Morales. 2024.
                            </span>

                            <span class="venue">
                                Musk’s concerns over google deepmind “ai dictatorship” revealed in emails
                                from 2016 - communications released during the recent openai court case, Nov.
                            </span>
                        </li>

                        <li id="ref-78">
                            <span class="authors">
                                E. Ntoutsi, P. Fafalios, U. Gadiraju, V. Iosifidis, W. Nejdl, M.-E. Vidal, S. Ruggieri,
                                F. Turini,
                                S. Papadopoulos, E. Krasanakis, et al. 2020.
                            </span>

                            <span class="venue">
                                Bias in data-driven artificial intelligence systems—an introductory survey.
                                <em>Wiley Interdisciplinary Reviews: Data Mining and Knowledge
                                    Discovery</em>
                            </span>, 10(3):e1356
                        </li>

                        <li id="ref-79">
                            <span class="authors">
                                J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A.
                                Radford, J. Wu, and D. Amodei. 2020.
                            </span>

                            <span class="venue">
                                Scaling laws for neural language models. <em>CoRR,</em> abs/2001.08361.
                            </span>
                        </li>

                        <li id="ref-80">
                            <span class="authors">
                                J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las
                                Casas,
                                L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den
                                Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals,
                                and L. Sifre. 2022
                            </span>

                            <span class="venue">
                                Training compute-optimal large language models.
                            </span>
                        </li>

                        <li id="ref-81">
                            <span class="authors">
                                D. Mirvish. 2017.
                            </span>

                            <span class="venue">
                                The Hathaway effect: How Anne gives Warren Buffett a rise, 3. Published
                                March 2, 2011.
                            </span>
                        </li>

                        <li id="ref-82">
                            <span class="authors">
                                A. Tong and M. Martina. 2024.
                            </span>

                            <span class="venue">
                                Nov.
                            </span>
                        </li>

                        <li id="ref-83">
                            <span class="authors">
                                R. I. Dunbar. 1993.
                            </span>

                            <span class="venue">
                                Coevolution of neocortical size, group size and language in humans.
                                <em>Behavioral and brain sciences</em>, 16(4):681–694.
                            </span>
                        </li>

                        <li id="ref-84">
                            <span class="authors">
                                R. S. BURT. 1992
                            </span>

                            <span class="venue">
                                <em> Structural Holes: The Social Structure of Competition</em>. Harvard University
                                Press.
                            </span>
                        </li>

                        <li id="ref-85">
                            <span class="authors">
                                R. S. Burt. 2003.
                            </span>

                            <span class="venue">
                                The social structure of competition. <em> Networks in the knowledge economy</em>,
                                13(2):57–91.
                            </span>
                        </li>

                        <li id="ref-86">
                            <span class="authors">
                                S. Russell, P. Norvig, and A. Intelligence. 1995.
                            </span>

                            <span class="venue">
                                A modern approach. <em>Artificial Intelligence.
                                    Prentice-Hall, Egnlewood Cliffs</em>, 25(27):79–80.
                            </span>
                        </li>

                        <li id="ref-87">
                            <span class="authors">
                                Y. LeCun, Y. Bengio, and G. Hinton. 2015.
                            </span>

                            <span class="venue">
                                Deep learning. <em>nature</em>, 521(7553):436–444.
                            </span>
                        </li>

                        <li id="ref-88">
                            <span class="authors">
                                V. Berger. 2025.
                            </span>

                            <span class="venue">
                                The ai copyright battle: Why openai and google are pushing for fair use.
                                <em>Forbes</em>, March. Online article.
                            </span>
                        </li>

                        <li id="ref-89">
                            <span class="authors">
                                Statista Market Insights. 2025.
                            </span>

                            <span class="venue">
                                Advertising - worldwide. Market Outlook, most recent update:
                                Aug 2025.
                            </span>
                        </li>

                        <li id="ref-90">
                            <span class="authors">
                                E. L. Bernays. 1928.
                            </span>

                            <span class="venue">
                                <em>Propaganda</em>. Ig publishing.
                            </span>
                        </li>

                        <li id="ref-91">
                            <span class="authors">
                                S. Adikari and K. Dutta. 2015.
                            </span>

                            <span class="venue">
                                Real time bidding in online digital advertisement. In <em>International Conference on
                                    Design Science Research in Information Systems</em>, pages 19–38.
                                Springer.
                            </span>
                        </li>
                    </ol>
                </section>

                <nav class="chapter-nav">
                    <span></span>
                    <a href="chapter2.html" class="next">Chapter II: From Deep Learning to Deep Voting</a>
                </nav>
            </main>
        </div>

        <footer>
            <p>Andrew Trask &middot; University of Oxford</p>
        </footer>
    </div><!-- /.wrapper -->

    <img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQCa5kJX0IfZqgsgGUg5LB470wWtz3nfi_DuQ&s"
        alt="University of Oxford" class="oxford-logo-fixed">

    <script src='main.js' type='text/javascript' defer></script>
</body>

</html>