<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter II: From Deep Voting to Network-Source AI | ABC in AI</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Source+Serif+4:ital,opsz,wght@0,8..60,400;0,8..60,600;1,8..60,400&display=swap"
        rel="stylesheet">
    <!-- MathJax for LaTeX rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-8HDSX11G9D"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'G-8HDSX11G9D');
    </script>
    <!-- Privacy-friendly analytics by Plausible -->
    <script async src="https://plausible.io/js/pa-OtFEayY3RhHZVOzX3iV76.js"></script>
    <script>
        window.plausible = window.plausible || function () { (plausible.q = plausible.q || []).push(arguments) }, plausible.init = plausible.init || function (i) { plausible.o = i || {} };
        plausible.init()
    </script>
</head>

<body>
    <div class="wrapper">
        <nav>
            <a href="index.html">I. Introduction</a>
            <a href="chapter2.html">II. Deep Voting</a>
            <a href="chapter3.html" class="active">III. Network-Source AI</a>
            <a href="chapter4.html">IV. Broad Listening</a>
            <a href="chapter5.html">V. Conclusion</a>
            <a href="references.html">References</a>
            <a href="appendix1.html">Appendix I</a>
            <a href="appendix2.html">Appendix II</a>
            <a href="https://andrewtrask.com">About</a>
        </nav>

        <div class="page-container">
            <aside class="toc-sidebar">
                <h4>On This Page</h4>
                <ul>
                    <li class="toc-h3"><a href="#chapter-summary">Chapter Summary</a></li>
                    <li class="toc-h3"><a href="#problem">The Problem: Deep Voting Alone</a></li>
                    <li class="toc-h3"><a href="#second-why">Second Why: Open/Closed-Source</a></li>
                    <li class="toc-h3"><a href="#third-why">Third Why: The Copy Problem</a></li>
                    <li class="toc-h3"><a href="#third-hypothesis">Third Hypothesis: Structured Transparency</a></li>
                    <li class="toc-h4"><a href="#semi-input-privacy">Semi-Input Privacy</a></li>
                    <li class="toc-h4"><a href="#full-input-privacy">Full Input Privacy</a></li>
                    <li class="toc-h4"><a href="#output-privacy">Output Privacy</a></li>
                    <li class="toc-h4"><a href="#input-verification">Input Verification</a></li>
                    <li class="toc-h4"><a href="#output-verification">Output Verification</a></li>
                    <li class="toc-h4"><a href="#flow-governance">Flow Governance</a></li>
                    <li class="toc-h3"><a href="#second-hypothesis">Second Hypothesis: Network-Source AI</a></li>
                    <li class="toc-h3"><a href="#first-hypothesis">First Hypothesis: Collective Control</a></li>
                </ul>
                <div class="ascii-decoration">
                    ╭──╮
                    │ │
                    ╭─┴──┴─╮
                    │ ●── │
                    ╰──────╯

                    structured
                    transparency
                </div>
            </aside>

            <!-- Floating margin art -->
            <div class="ascii-margin">
                <div class="art-piece">
                    copy →
                    control
                    lost

                    [A]→[A][A]
                    ↓ ↓
                    ? ?
                </div>
                <div class="art-piece">
                    ZKP:
                    prove
                    without
                    showing

                    ✓ ← [███]
                </div>
                <div class="art-piece">
                    secret
                    sharing:

                    ╭─╮╭─╮╭─╮
                    │▓││▓││▓│
                    ╰─╯╰─╯╰─╯
                    n of m
                </div>
            </div>

            <main>

                <div class="chapter-header">
                    <p class="chapter-number">Chapter III</p>
                    <h1>From Deep Voting to Network-Source AI</h1>
                </div>

                <figure class="full-width">
                    <img src="1411.3146/abc_ch3_1_v6.png" alt="ABC and the copy problem">
                    <figcaption>The lack of ABC creates problems which are underpinned by the overuse of copying
                        within deep learning systems.</figcaption>
                </figure>

                <section id="chapter-summary" class="abstract">
                    <h2>Chapter Summary</h2>

                    <p>
                        The previous chapter outlined how the offsetting of addition with concatenation in deep learning
                        systems can lead to source-partitioned knowledge in AI systems, unlocking a viable path towards
                        the attribution needed for ABC: deep voting. However, deep voting does not offer
                        attribution based <em>control</em>. It only offers attribution-based <em>suggestions</em> which
                        the holder of an AI
                        model may
                        ignore. This chapter unpacks the underlying cause of voluntary participation in both open-source
                        and closed-source AI. It begins by describing how ABC is thwarted by the only control paradigm
                        of AI systems, unilateral control: that whomever obtains a copy of information can unilaterally
                        use that information for any purpose. This suggests a direction for a solution: to reduce the
                        copying of information within AI systems, averting unilateral control. Building upon this
                        diagnosis, the chapter calls upon the framework of <em>structured transparency</em> (<a
                            href="#ref-1" class="cite">Trask et al., 2020</a>)
                        to avert the copy problem, revealing a new alternative to open-source and closed-source AI:
                        <strong>network-source AI</strong> — and a viable path towards true attribution-based
                        <em>control</em> in AI systems.
                    </p>
                </section>

                <figure class="full-width">
                    <img src="1411.3146/deep_voting_to_network_source_ai_v3.png" alt="Deep Voting to Network-Source AI">
                    <figcaption>Deep voting systems (left) require the copying of data from source to the holder of
                        an AI model, while network-source AI systems (right) leverage cryptography and distributed
                        systems to enable data holders to retain control over the only copy of their information (and
                        the predictive capability that information lends to an AI system). In network-source AI, AI
                        users directly query vast numbers of data owners requesting predictive capability for a single
                        prediction, learning only the output of their prediction in the process. AI system partitions
                        (as
                        in source-separated parameters from Deep Voting... pictured above as pie slices) are rapidly
                        synthesized to form a model used for a specific prediction. Darker lines/slices indicate
                        syntactic
                        and semantic information being used for a particular prediction. Lighter lines/slices indicate
                        information not being used for a particular prediction. The circle on top of the slices
                        represents a
                        dense model capable of rapidly synthesizing slices and semantic information in practice (which
                        itself must be trained to do so). Metadata (which tracks the synthesis and subsequent use of
                        semantic queries and syntactic slices for an AI prediction) enables ABC.
                    </figcaption>
                </figure>

                <div class="ascii-art float">
                    THE COPY PROBLEM

                    ┌─────────┐
                    │ ORIGINAL│
                    │ DATA │ "Once copied,
                    └────┬────┘ control is lost"
                    │
                    ┌───────┴───────┐
                    ▼ ▼
                    ┌───────┐ ┌───────┐
                    │ COPY │ │ COPY │ ──▶ used anywhere
                    └───────┘ └───────┘ by anyone
                    │ │ for anything
                    ? ?
                    <span class="caption">structured transparency solves this</span>
                </div>

                <section id="problem">
                    <h2>The Problem of ABC in Deep Voting Systems</h2>

                    <p>
                        The previous chapter offered a vision to unlock 6+ orders of magnitude more data and compute for
                        AI training through deep voting. By maintaining clear attribution through retrieval
                        mechanisms while enabling efficient parameter sharing for general patterns, deep voting
                        architectures offer a theoretical framework for safe data sharing across institutional
                        boundaries.
                        Yet a fundamental problem threatens to undermine its vision. Even with perfect implementation of
                        intelligence budgets and attribution guarantees, deep voting alone cannot ensure true
                        attribution-based <em>control</em>.
                    </p>

                    <p>
                        The reason is structural: a deep voting model, (like any AI system today) is unilaterally
                        controlled by whomever possesses a copy. For ABC to occur, the owner of an AI model would
                        have to voluntarily choose to uphold the wishes of data sources (i.e. respect and enforce
                        privacy
                        budgets calcuated by deep voting algorithms). Deep voting’s careful attribution mechanisms are
                        not a system of control, they are mere suggestions to give credit to data sources for
                        predictions.
                    </p>

                    <div class="callout">
                        <p class="callout-title">A Library Analogy</p>

                        <p>
                            Consider a bookstore which purchases books once but then allows customers to come in
                            and print copies of the books for sale in their store (without tracking how many copies
                            people make). Naturally, authors of books might be hesitant to sell their books to the store
                            because they presumably make far less money than if they sell to a store which does not
                            enable copies.
                        </p>

                        <p>
                            This book store analogy corresponds closely to the current practice of deep learning models,
                            which offer unmeasured, unrestricted copying of information due to their
                            inability to offer attribution with their predictions. Fortunately, deep voting offers a way
                            for
                            attribution metadata to be revealed and configured, but does this solve the issue?
                        </p>

                        <p>
                            Not necessarily, because of an inherent information asymmetry: in a bookstore
                            where the bookstore owner knows how many copies are made, the bookstore owner has far
                            more information on whether their clientele are likely to purchase copies of the book than
                            the author of the book does. Thus, even if the author tries to raise their prices to make up
                            for the bookstore making copies, the original author still struggles to know what the true
                            price might be relative to the bookstore owner.
                        </p>

                        <p>
                            Instead, to maximize incentives for the authors of books (and maximize the number of books
                            being offered in stores), authors should instead be able to sell their books one
                            copy at a time. In this way, the author can set their price and will make more money if
                            more books are sold. Not only will this better motivate authors to sell their books in more
                            stores, but it will also serve to attract more people to become book authors.
                        </p>
                    </div>

                    <h3>First Why: Unilateral Control Averts A-Based Control</h3>

                    <p>
                        This unilateral control problem reflects a deeper pattern that has constrained information
                        systems
                        throughout history. When one party gives information to another, the giver loses control over
                        how that information will be used. AI systems, despite their sophistication, remain bound by
                        this same fundamental limitation. Organizations sharing their data with an AI system have no
                        technical mechanism to enforce their attribution preferences; they must simply trust the model’s
                        operator to honor them
                    </p>

                    <p>Consider how this manifests in practice. A deep voting model might faithfully track attribution
                        and maintain intelligence budgets, but these mechanisms remain under unilateral control.
                        The model’s owner can simply choose to ignore or override these constraints at will.</p>

                    <p>This creates a trust barrier that blocks deep voting’s potential. Medical institutions, for
                        example, cite exactly these control and attribution concerns as primary barriers to sharing
                        their
                        vast repositories of valuable data (<a href="#ref-2" class="cite">Youssef et al., 2023</a>).
                        Without a way to technically enforce
                        how their information will be used and attributed, they cannot safely contribute to AI training.
                        This pattern repeats across domains, from scientific research (<a href="#ref-18"
                            class="cite">Fecher et al., 2015</a>; <a href="#ref-19" class="cite">Ascoli 2015</a>)
                        to financial data (<a href="#ref-20" class="cite">Sienkiewicz 2025</a>) to government records
                        (<a href="#ref-21" class="cite">Scott and Gong 2021</a>); vast stores
                        of valuable information remain siloed because we lack mechanisms to ensure attribution-based
                        control, and the corresponding incentives for collaboration that ABC would bring to AI systems
                    </p>

                    <figure class="full-width">
                        <img src="1411.3146/nsai_3_3_v5.png" alt="First Why concept graph">
                        <figcaption>Subset of conceptual graph highlighting the First Why and the focus of this section.
                        </figcaption>
                    </figure>

                    <p>
                        The challenge before us is clear: we need more than just better attribution mechanisms; we
                        need a fundamentally new approach to how AI systems are controlled. We need techniques
                        that transcend the limitations of unilateral control that have constrained information systems
                        throughout history. The next sections examine why current approaches fundamentally fail to
                        solve this problem, and introduce a new paradigm that could deliver on deep voting’s promise.
                    </p>
                </section>

                <section id="second-why">
                    <h2>Second Why: Open/Closed-Source AI Use Unilateral Control</h2>

                    <p>
                        The debate between open and closed source AI crystallizes how society grapples with fundamental
                        questions of control over artificial intelligence (<a href="#ref-22" class="cite">Vidal
                            2024</a>). This debate has become a
                        proxy for broader concerns about privacy, disinformation, copyright, safety, bias, and alignment
                        (<a href="#ref-23" class="cite">National Telecommunications and Information Administration
                            2024</a>). Yet when examined
                        through the lens of <a href="https://attribution-based-control.ai/">attribution-based
                            control</a> (ABC), both approaches fundamentally fail to
                        address these concerns, though they fail in opposite and illuminating ways. Consider how each
                        paradigm attempts to address these critical issues:
                    </p>

                    <p><strong>Copyright and Intellectual Property:</strong> The closed source approach could better
                        respect IP
                        rights through licensing and usage restrictions negotiated between formal AI companies and
                        data sources (<a href="#ref-24" class="cite">Associated Press 2025</a>). Open source suggests
                        that unrestricted sharing better
                        serves creators by enabling innovation and creativity (<a href="#ref-25" class="cite">Open
                            Source Initiative 2024</a>). Yet through
                        ABC’s lens, neither addresses creators’ fundamental need to control how their work informs AI
                        outputs. Closed source consolidates control under corporate management, while open source
                        eliminates control entirely.
                    </p>

                    <p><strong>Safety and Misuse Prevention:</strong> Closed source proponents claim centralized
                        oversight
                        prevents harmful applications (<a href="#ref-26" class="cite">Owen-Jackson 2024</a>). Open
                        source advocates argue that collective
                        scrutiny better identifies risks (<a href="#ref-27" class="cite">Grow 2025</a>). Yet ABC reveals
                        that both approaches fail to
                        provide
                    </p>

                    <figure class="full-width">
                        <img src="1411.3146/nsai_3_4_v4.png" alt="Second Why concept graph">
                        <figcaption> Subset of concept graph highlighting the 2nd Why and focus of this section.
                        </figcaption>
                    </figure>

                    <p>
                        what’s actually needed: the ability for contributors to withdraw support when their data enables
                        harmful outcomes. Closed source requires trust in corporate judgment, while open source
                        enables unrestricted modification of safety measures.
                    </p>

                    <p><strong>Bias and Representation:</strong> Closed source teams promise careful curation to prevent
                        bias
                        (<a href="#ref-28" class="cite">Gabriel et al., 2024</a>). Open source suggests community
                        oversight ensures fair representation
                        (<a href="#ref-29" class="cite">Wealand 2025</a>). Yet ABC shows how both approaches fail to
                        give communities ongoing control
                        over how their perspectives inform AI decisions. Closed source centralizes these choices under
                        corporate teams (<a href="#ref-30" class="cite">Pasquale 2015</a>), while open source allows
                        anyone to modify how perspectives
                        are weighted.</p>

                    <p>
                        This pattern reveals why neither approach can deliver true attribution-based control. Consider
                        first the closed source approach. Proponents argue that centralized control ensures responsible
                        development and deployment of AI systems. A company operating a closed source model can
                        implement deep voting’s attribution mechanisms and maintain them intact. Yet this merely
                        transforms ABC into corporate benevolence (exactly the kind of centralized control that has
                        already failed to unlock the world’s data and compute resources). Medical institutions withhold
                        valuable research data (<a href="#ref-3" class="cite">Kaissis et al., 2020</a>; <a
                            href="#ref-31">Gould 2015</a>),
                        publishers restrict access to their
                        work
                        (<a href="#ref-32" class="cite">Grynbaum and Mac 2023</a>) precisely because they reject this
                        model of centralized corporate
                        control. The very centralization that supposedly enables control actually undermines it,
                        replacing
                        genuine ABC with a hope that power will be used wisely.
                    </p>

                    <p>
                        The open source approach appears to solve this by eliminating centralized control entirely.
                        If anyone can inspect and modify the code, surely this enables true collective control? Yet
                        this intuition breaks down precisely because of unrestricted copying. Once a model is open
                        sourced, anyone can modify it to bypass attribution tracking entirely. The same problems that
                        plague closed source systems (hallucination, disinformation, misuse of data, etc.) remain hard
                        to address when control is abdicated to any AI user. Open source trades one form of unilateral
                        control for another (from centralized corporate control to uncontrolled proliferation). But ABC
                        requires a collective bargaining between data sources and AI users, not total ownership by
                        either
                        (or by an AI creator).
                    </p>

                    <p>
                        This reveals a deeper issue: our current paradigms of software distribution make true ABC
                        impossible by design. Open source sacrifices control for transparency, while closed source
                        sacrifices transparency for control. Neither approach can provide both. Yet ABC requires exactly
                        this combination: transparent verification that attribution mechanisms are working as intended,
                        while ensuring those mechanisms cannot be bypassed.
                    </p>

                    <p>
                        The AI community has largely accepted this dichotomy as inevitable, debating the relative
                        merits of open versus closed source as though these were our only options. But what if this
                        frame is fundamentally wrong? What if the very premise that AI systems must exist as copyable
                        software is the root of our problem? The next section examines this possibility, revealing why
                        the copy problem itself may be what we need to solve.
                    </p>
                </section>

                <section id="third-why">
                    <h2>Third Why: Copy Problem Necessitates Unilateral Control</h2>

                    <figure class="full-width">
                        <img src="1411.3146/nsai_3_5_v3.png" alt="Third Why concept graph">
                        <figcaption> Subset of conceptual graph highlighting the Third Why and the focus of this
                            section.
                        </figcaption>
                    </figure>

                    <p>
                        The previous section demonstrated that both open and closed source paradigms fail to
                        enable attribution-based control. This failure reflects a more fundamental limitation: control
                        over information is lost when that information exists as a copy. This ”copy problem” manifests
                        acutely in AI systems. When a model is trained, it creates a derived representation of
                        information
                        from its training data encoded in model weights. These weights can then be replicated and
                        modified by whoever possesses them, with the following consequences:
                    </p>

                    <p><strong>Closed Source Models:</strong> Even with perfect attribution tracking and intelligence
                        budgets,
                        these mechanisms remain under unilateral control of the model owner. Data contributors
                        must trust that their attribution preferences will be honored, lacking technical enforcement
                        mechanisms.
                    </p>

                    <p><strong>Open Source Models:</strong>
                        Once a model is released, anyone can replicate and modify it to
                        bypass attribution mechanisms entirely. Public distribution necessarily surrenders control over
                        use.
                    </p>

                    <p>
                        Both approaches treat AI models as copyable software artifacts, rendering attribution-based
                        control infeasible. Once a copy exists, technical enforcement of attribution becomes impossible.
                    </p>

                    <p>
                        This limitation extends beyond AI. The music industry’s struggles with digital piracy (<a
                            href="#ref-14" class="cite">Cummings
                            2017</a>), society’s challenges with viral misinformation (<a href="#ref-15"
                            class="cite">Shu et al., 2020</a>), and government
                        efforts to control classified information (<a href="#ref-33" class="cite">Elsea 2006</a>) share
                        the same fundamental issue:
                        information, once copied, escapes control (<a href="#ref-16" class="cite">Schneier 2015</a>; <a
                            href="#ref-17" class="cite">Veliz 2020</a>).
                    </p>

                    <p>
                        This analysis reveals why current approaches to AI governance are fundamentally insufficient.
                        Attribution tracking, usage restrictions, and oversight mechanisms cannot solve the problem
                        while AI systems exist as copyable artifacts. The architecture of AI distribution and operation
                        itself prevents attribution-based control.
                    </p>

                    <p>
                        Consequently, if AI systems remain copyable software, we cannot ensure proper source
                        attribution, prevent data misuse, motivate a new class of data owners to participate in
                        training,
                        or maintain democratic control over increasingly powerful systems
                    </p>

                    <p>
                        This limitation necessitates a fundamentally different approach. Rather than choosing
                        between open and closed source (different modes of copying and controlling software) we must
                        reconsider how AI systems are distributed and operated. The following sections introduce such
                        an approach.
                    </p>
                </section>

                <section id="third-hypothesis">
                    <h2>Third Hypothesis: From Copying to Structured Transparency</h2>

                    <p>
                        The copy problem reveals that our current approaches to AI control are fundamentally flawed
                        because the current approaches to information control in general are fundamentally flawed. Yet,
                        the copy problem naturally suggests a direction: don’t copy information.
                    </p>

                    <p>
                        Yet, what would this mean practically in the context of AI? Could a deep voting system
                        be constructed wherein the various data sources never revealed a copy of their information
                        to each other (or to AI users)? Might it be possible for them to collaborate without copying
                        (to produce AI predictions using deep voting’s algorithms) but without anyone obtaining any
                        copies of information they didn’t already have during the process (except of course the AI user
                        receiving their prediction)? Indeed, this might be the case. Let us begin by introducing design
                        patterns which could make this possible.
                    </p>

                    <h3 id="semi-input-privacy">Semi-input Privacy: Federated Computation</h3>

                    <p>
                        To begin, we call upon the concept of semi-input privacy, which enables multiple parties to
                        jointly compute a function together where <em>at least some of the parties</em> don’t have to
                        reveal
                        their
                        data to each other. Perhaps the most famous semi-input privacy technique is
                        <em>on-device/federated
                            learning</em> (<a href="#ref-34" class="cite">McMahan et al., 2017</a>), wherein a
                        computation moves to the data instead of the
                        data
                        being centralized for computation.
                    </p>

                    <figure class="full-width">
                        <img src="1411.3146/nsai_3_6_v3.png" alt="Third Hypothesis concept graph">
                        <figcaption>Subset of concept graph highlighting the 3rd Hypothesis and the of this section.
                        </figcaption>
                    </figure>

                    <p>
                        Deep voting could leverage federated computation (<a href="#ref-3" class="cite">Kaissis et al.,
                            2020</a>) to allow data sources
                        to train their respective section of an AI model without sharing raw data. More specifically,
                        recall
                        that a deep voting architecture partitions an AI model’s weights into source-specific sections,
                        which are merged through either semantic (e.g., RAG) or semantic (e.g., model merging) means.
                        In theory, instead of each of these partitions being created and/or stored by the AI user or
                        singular
                        model owner, each data source (i.e. Reddit, the New York Times, and other data sources) could
                        run a <em>web server they control</em>, and create and store their part of a deep voting model
                        on that
                        server.
                    </p>

                    <p>
                        In this way, whenever and AI user wanted to use a deep voting model, they would need to
                        ping the corresponding servers, who would view the AI query, create the right semantic query
                        (i.e. RAG query), and send the query results <em>and a syntactic model partition</em> to the AI
                        user for
                        final forward propagation.
                    </p>

                    <p>
                        In this way, each source could maintain control of their data while still contributing to the
                        overall system. However, semi-input privacy alone proves insufficient. While the raw training
                        data remains private, each data source would see every query being used against the model.
                        Furthermore, the AI user learns a tremendous amount about each data source in each query,
                        receiving not only relevant sematic query results (i.e. RAG results) from each data source, but
                        also their syntactic model partition. This creates two problems: the AI user has to be willing
                        to
                        reveal their queries to many (potentially thousands or even millions) of data sources, and the
                        data sources have to be ok with the risk that (after enough queries) the AI user would probably
                        have a copy of the underlying data and model. Eventually, semi-input privacy violates ABC.
                    </p>

                    <h3 id="full-input-privacy">Full Input Privacy: Secure enclaves, homomorphic encryption, etc</h3>

                    <p>
                        To address these limitations, we need full input privacy, where <em>no party</em> sees data they
                        didn’t
                        already have (except the AI user receiving the prediction). This includes protecting both the AI
                        user’s query and the data sources’ information.
                    </p>

                    <p>
                        A variety of technologies can provide input privacy: secure enclaves, homomorphic encryption,
                        and various other secure multi-party computation algorithms (<a href="#ref-4"
                            class="cite">Gentry and Boneh 2009</a>;
                        <a href="#ref-5" class="cite">Costan and Devadas 2016</a>; <a href="#ref-6" class="cite">Yao
                            1982a</a>; <a href="#ref-7" class="cite">Goldreich 1987</a>; <a href="#ref-35"
                            class="cite">Bogdanov et
                            al., 2014</a>; <a href="#ref-36" class="cite">Craddocket al., 2018</a>). For ease of
                        exposition, consider a combined use of homomorphic encryption and
                        secure enclaves.
                    </p>

                    <p>
                        In the context of deep voting, two privacy preserving operations are necessary. First, the
                        AI user needs to privately send their query to millions of sources, who then must reply with
                        relevant semantic information (e.g. RAG results), and relevant syntactic information (e.g.
                        model partitions). For this, an AI user might leverage a homomorphic encryption (<a
                            href="#ref-4" class="cite">Gentry and
                            Boneh 2009</a>; <a href="#ref-37" class="cite">Boneh et al., 2011</a>) key-value database,
                        enabling them to query vast
                        collections of
                        databases without precisely revealing their query.
                    </p>

                    <p>
                        This brings us to the second privacy preserving operation, the transformation from semantic
                        and syntactic responses into an output prediction. For this, the group might co-leverage a
                        collection of GPU enclaves. GPU enclaves (<a href="#ref-5" class="cite">Costan and Devadas
                            2016</a>) offer full input privacy
                        by only decrypting information when that information is actively being computed over within its
                        chip, writing only encrypted information to RAM and hard disks throughout its process. Indeed,
                        a GPU enclave enables a collection of data providers to jointly compute a function without even
                        the administrator (or cloud provider) of the GPU enclave knowing what is being computed.
                    </p>

                    <p>
                        Taken together, while there are a variety of full input privacy technologies, some combination
                        of technologies fit for querying and then computing is likely appropriate for maximizing various
                        performance tradeoffs (<a href="#ref-36" class="cite">Craddock et al., 2018</a>). And when used
                        properly, these technologies
                        could enable deep voting wherein each data source retained sole control over the only copy of
                        their information (semantic and syntactic), and the AI user didn’t need to fully reveal their
                        query
                        to the many data sources they elect to leverage.
                    </p>

                    <p>
                        Yet, the result of the AI prediction is still revealed to the AI user. What if the AI user
                        asked,
                        ”what is all the data you can see?”. What’s to prevent aforementioned hardware and algorithms
                        from being circumvented by an AI query that just copies out sensitive information? For this,
                        input privacy is not enough.
                    </p>

                    <h3 id="output-privacy">Output Privacy: Deep Voting's Built-in Guarantees</h3>

                    <p>
                        While input privacy protects data during computation, it doesn’t prevent the AI system’s outputs
                        from revealing sensitive information about the inputs. As just described, consider an AI user
                        who prompts ”what is all the data you can see?” or makes a series of clever queries designed to
                        reconstruct training data. Even with perfect input privacy, the outputs themselves might leak
                        information.
                    </p>

                    <p>
                        However, deep voting’s intelligence budgets (via differential attribution mechanisms) already
                        provide the necessary output privacy guarantees. This is because differential attribution and
                        differential privacy (<a href="#ref-38" class="cite">Dwork et al., 2006</a>; <a href="#ref-8"
                            class="cite">Dwork et al., 2014</a>)
                        are two sides of the same coin:
                    </p>

                    <ul>
                        <li>Differential privacy focuses on preventing attribution, ensuring outputs don’t reveal too
                            much about specific inputs</li>
                        <li>Differential attribution focuses on ensuring attribution, guaranteeing that outputs properly
                            credit their influences</li>
                    </ul>

                    <p>
                        Both are ways of measuring and providing guarantees over the same fundamental constraint:
                        the degree to which an input source contributes to an output prediction (<a href="#ref-8"
                            class="cite">Dwork et al., 2014</a>).
                        Thus, by enforcing intelligence budgets and attribution bounds, deep voting naturally limits
                        how much information about any source can be leaked through outputs. We don’t need to add
                        additional privacy mechanisms; we simply need to ensure that our input privacy technologies
                        (secure enclaves and homomorphic encryption) properly enforce the attribution bounds and
                        intelligence budgets that deep voting already specifies.
                    </p>

                    <p>
                        However, this creates a new challenge: with all these privacy protections in place, an AI
                        user is forced to rely upon vast collections of information they cannot see. This should beg the
                        question: how would an AI user know that the information they’re relying upon is real... is
                        something other than random noise? This leads us to our next guarantee: input verification.
                    </p>

                    <h3 id="input-verification">Input Verification: Zero-Knowledge Proofs and Attestation</h3>

                    <p>
                        While input and output privacy protect sensitive information, they create a fundamental
                        challenge
                        for AI users: how can they trust information they cannot see? An AI user querying millions of
                        encrypted data sources needs some way to verify that these sources contain real, high-quality
                        information rather than random noise or malicious content.
                    </p>

                    <p>
                        Input verification addresses this problem through cryptographic techniques like zero-knowledge
                        proofs and attestation chains (<a href="#ref-9" class="cite">Goldwasser et al., 1989</a>; <a
                            href="#ref-10" class="cite">Feige et al., 1988</a>). These allow data
                        sources to prove properties about their data without revealing the data itself. For example:
                    </p>

                    <ul>
                        <li> A news organization could prove their articles were published on specific dates, because a
                            hash of the data was signed by someone who is a trusted timekeeper</li>

                        <li>A scientific journal could prove their papers passed peer review, because the paper was
                            cryptographically signed by the reviewers or journal (e.g. hosted on an HTTPS website).</li>

                        <li>A social media platform could prove their content meets certain quality thresholds, because
                            the data is cryptographically signed by a trusted auditor.</li>

                        <li>An expert could prove their credentials without revealing their identity (<a href="#ref-39"
                                class="cite">Sovrin</a>; <a href="#ref-40" class="cite">Wang
                                and De Filippi 2020</a>), because the issuer of those credentials has cryptographically
                            signed
                            a statement.</li>
                    </ul>

                    <p>
                        Input verification comes in roughly two styles: internal consistency and external validation.
                        One can think of internal consistency as the type of verification a barkeep might do when
                        inspecting a driver’s license. They might check that the license contains all its requisite
                        parts,
                        that the photos are in the right places, and that the document appears to be untampered and
                        whole.
                    </p>

                    <p>
                        Meanwhile, external validation is all about reputation, the degree to which others have
                        claimed that a document is true. To continue with the barkeep analogy, this would be like if a
                        bartender called the local government to check that a government document was indeed genuine,
                        ”Hi yes &mdash; does the State of Nevada have a person named James Brown with the driver’s
                        license
                        number 23526436?”? The claim is verified not because of an internal property, but because a
                        credible source has claimed that something is true.
                    </p>

                    <p>
                        Input verification techniques can be used by combining basic public-key cryptography (e.g.
                        signed hashes of data) (<a href="#ref-11" class="cite">Laurie 2014</a>; <a href="#ref-12"
                            class="cite">Chase et al., 2020</a>) with
                        verified computation techniques
                        like zero-knowledge proofs, active security, or secure (GPU or CPU) enclaves (<a href="#ref-9"
                            class="cite">Goldwasser
                            et al., 1989</a>; <a href="#ref-41" class="cite">Loftus and Smart 2011</a>; <a
                            href="#ref-5" class="cite">Costan and
                            Devadas 2016</a>).
                    </p>

                    <p>
                        For internal consistency, verified computation (<a href="#ref-41" class="cite">Loftus and Smart
                            2011</a>) enables one to send
                        a function to inspect data and check whether it has properties it should. For example, an AI
                        user
                        who is leveraging MRI scans might send in a classifier to check whether the MRI scans actually
                        contain ”pictures of a human head”, receiving back summary statistics validating that the data
                        they cannot see is, in fact, the right type of data.
                    </p>

                    <p>
                        In this context, verified computation enables them to know that their function occurred over
                        data (and code) which has particular hashes, so that if they then do a second computation (i.e.
                        an AI prediction), they can use the same verified computation techniques to ensure that the AI
                        prediction is leveraging the same (unseen) data. Taken together, verified computation can enable
                        an AI user to check unseen deep voting partitions for important properties and then ensure those
                        same partitions are used for an AI prediction (all without seeing the partitions directly).
                    </p>

                    <p>
                        Meanwhile, input verification techniques can also enable the external validation form of
                        verification. As a first step, parties who believe something about a piece of data (i.e.
                        ”According
                        to me... this statement is true”), can use public-key cryptography to hash and sign the
                        underlying
                        data with their cryptographic signature (<a href="#ref-11" class="cite">Laurie 2014</a>; <a
                            href="#ref-12" class="cite">Chase et al., 2020</a>). For example, a
                        journalist might sign their article as being true. A doctor might sign their diagnosis as being
                        their genuine opinion. Or an eye witness to an event might sign their iPhone video as being
                        something they genuinely saw. And if those signed hashes are made available to the AI user,
                        they can then use verified computation to check the hash of the signature against the hash of a
                        piece of data they cannot directly see.
                    </p>

                    <p>
                        Note that while it might sound far-fetched for everyone to be cryptographically signing all
                        their information, it is noteworthy that every website loadable by HTTPS gets signed by the
                        web server hosting it (<a href="#ref-11" class="cite">Laurie 2014</a>). Thus, there is actually
                        a rather robustly deployed chain of
                        signatures already deployed in the world. For example, if I needed to prove to you that I have a
                        certain amount of money in my bank account, I could load a webpage of my bank, download the
                        page with the signed hash from barclays.co.uk, and show it to you. And the fact that all HTTPS
                        webpages are cryptographically signed by my bank, and the fact that the page would contain my
                        name, address, and bank balance, would be enough for me to prove to you that I possessed a
                        certain amount of money. The generality of this technology being deployed at web scale is one
                        source of optimism around the DID:WEB movement (<a href="#ref-40" class="cite">Wang and De
                            Filippi 2020</a>; <a href="#ref-42" class="cite">Chaum 1985</a>;
                        <a href="#ref-43" class="cite">Adler et al., 2024</a>).
                    </p>

                    <p>
                        In the context of deep voting, these proofs would be integrated into the input privacy system.
                        When an AI user queries encrypted data sources through homomorphic encryption or secure
                        enclaves, each source would provide not just encrypted data but also cryptographic proofs about
                        that data’s properties. The enclaves would verify these proofs before incorporating the data
                        into
                        computations. And in this way, an AI user can know that they are relying upon information
                        which has properties they desire and which is signed as genuine from sources they elect to
                        trust.
                    </p>

                    <p>
                        However, this raises another critical question: even if we can verify the inputs, how can
                        we trust that the secure enclaves and homomorphic encryption systems are actually computing
                        what they claim to be computing? Given that no-one can see what happens within these systems
                        (<a href="#ref-5" class="cite">Costan and Devadas 2016</a>), who is to say that they are
                        actually running the program which has
                        been requested by the AI user and data sources? This leads us to our next guarantee: output
                        verification.
                    </p>

                    <h3 id="output-verification">Output Verification: Verifiable Computation and Attestation Chains</h3>

                    <p>
                        While input verification ensures the quality of source data, we still need to verify that our
                        privacypreserving computations are computing using the code and inputs that have been requested.
                        Only then can an AI user trust that the aforementioned input/output privacy and input
                        verification
                        techniques are properly enforcing deep voting’s intelligence budgets.
                    </p>

                    <p>
                        Output verification addresses this through two complementary mechanisms: verifiable computation
                        and attestation chains (<a href="#ref-9" class="cite">Goldwasser et al., 1989</a>; <a
                            href="#ref-5" class="cite">Costan and Devadas 2016</a>). Verifiable
                        computation (<a href="#ref-9" class="cite">Goldwasser et al., 1989</a>; <a href="#ref-41"
                            class="cite">Loftus and Smart 2011</a>) enables parties to prove that
                        specific computations were performed correctly without revealing the private inputs. For deep
                        voting, includes the critical sub-parts of the overal computation:
                    </p>

                    <ul>
                        <li>Proving that intelligence budgets were properly enforced</li>
                        <li>Verifying that semantic (RAG) queries were executed as requested</li>
                        <li>Confirming that syntactic model partitions were combined according to specification</li>
                        <li>Ensuring that differential privacy guarantees were maintained</li>
                    </ul>

                    <p>
                        Together, these mechanisms allow AI users to verify that their queries were processed
                        correctly while maintaining all privacy guarantees. However, this creates one final challenge:
                        amidst all the parties involved in a deep voting prediction (an AI user, and an unspecified
                        number
                        of data sources), how does one ensure that all the right controls are distributed to all the
                        right
                        parties? This leads us to our final guarantee: flow governance.
                    </p>

                    <h3 id="flow-governance">Flow Governance: Cryptographic Control Distribution</h3>

                    <p>
                        The means by which the aforementioned algorithms provide control over information is through
                        the distribution of cryptographic keys. Each of these keys gives its owner control over some
                        aspect of computation. The class of algorithm known as secure multi-party computation (SMPC)
                        provides the foundation for this enforcement. Through techniques like additive secret sharing,
                        SMPC enables numbers (and thus any digital computation) to be split into cryptographic ”shares”
                        distributed among participants. Each share-holder gains mathematical veto power over how their
                        share is used in subsequent computations (<a href="#ref-13" class="cite">Shamir 1979</a>).
                    </p>

                    <p>In the context of deep voting, this enables three critical forms of control distribution:</p>

                    <ul>
                        <li>
                            <strong>Data Control:</strong> Each source’s deep voting partition (semantic and syntactic)
                            can be split
                            into shares, with the source maintaining cryptographic control through their share. No
                            computation can proceed without their active participation from shareholders.
                        </li>

                        <li>
                            <strong>Budget Control:</strong> Intelligence budgets become cryptographic constraints
                            enforced through
                            SMPC, rather than just software settings. Each prediction must prove it respects these
                            budgets in sufficient manner for the shareholders (keyholders) to allow the release of the
                            final results to the AI user.
                        </li>

                        <li>
                            <strong>Computation Control:</strong> The process of generating predictions becomes a
                            multi-party
                            computation, with each source maintaining cryptographic veto power over how their
                            information is used.
                        </li>
                    </ul>

                    <p>
                        SMPC can be accomplished through software (e.g., homomorphic encryption) or through
                        hardware (e.g., GPU enclaves) techniques, enabling arbitrary parts of a software program to be
                        blocked based on arbitrary key holders signoff to proceed.
                    </p>

                    <p>
                        Together, these five guarantees (input privacy, output privacy, input verification, output
                        verification, and flow governance) provide the technical foundation for <em>structured
                            transparency</em>.They enable deep voting to operate without producing copies of information
                        right up until
                        the final result is released to the AI user. However, while these guarantees make controlled
                        collaboration possible, we still need a framework for how AI systems should operate in this new
                        paradigm. The next section introduces network-source AI as this missing piece.
                    </p>
                </section>

                <section id="second-hypothesis">
                    <h2>Second Hypothesis: From Open/Closed to Network-Source AI</h2>

                    <figure class="full-width">
                        <img src="1411.3146/nsai_3_7_v4.png" alt="Second Hypothesis concept graph">
                        <figcaption>Subset of concept graph highlighting the 2nd Hypothesis and focus of this section.
                        </figcaption>
                    </figure>

                    <p>
                        This chapter previously described how open and closed-source AI are the two governance
                        paradigms for AI systems, but that neither offered sufficient control due to the copy problem.
                        In the previous section, this chapter described structured transparency, and its applications in
                        averting the copy problem. Consequently, structured transparency yields a control paradigm
                        wherein an AI model is neither closed nor open, but is instead distributed across a network:
                        network-source AI.
                    </p>

                    <p>
                        This shift from copied software to network-resident computation directly addresses the
                        fundamental limitations of both open and closed source paradigms. Where closed source asks
                        data contributors to trust corporate guardians and open source requires them to surrender
                        control
                        entirely, network-source AI enables ABC through cryptographic guarantees.
                    </p>

                    <p>
                        Notably, this change addresses some of the thorny issues underlying the debate between
                        open and closed source AI, for example, copyright and attribution. Rather than consolidating
                        control under corporations or eliminating it entirely, network-source AI provides creators with
                        ongoing cryptographic control over how their work informs AI outputs. Intelligence budgets
                        and attribution mechanisms become technically enforced rather than merely promised.
                    </p>

                    <p>
                        Safety and misuse prevention transform as well. Instead of relying on corporate oversight
                        or community scrutiny, network-source AI enables data sources to cryptographically withdraw
                        support if their information enables harmful outcomes. This absorbs the benefits of community
                        oversight without the risks of single-points of failure within that community misusing AI in the
                        process.
                    </p>

                    <p>
                        Even bias and representation concerns find some resolution. Rather than centralizing
                        decisions about representation or allowing unrestricted modification, network-source AI gives
                        communities ongoing cryptographic control over how their perspectives inform AI decisions.
                        Attribution and intelligence budgets make representation a legible, tunable parameter through
                        technical means.
                    </p>

                    <p>
                        This resolves the choice between open and closed source AI by creating a third option:
                        AI systems that are simultaneously transparent (through verification) and controlled (through
                        cryptography). The key insight is that by preventing copying through structured transparency’s
                        guarantees, we can maintain both visibility into how systems operate and precise control over
                        how information is used.
                    </p>

                    <p>
                        This yields something fundamentally different from both open and closed source AI. It’s
                        not software that must be copied to be used, but rather a network service with cryptographic
                        guarantees about how it operates. This enables true attribution-based control while maintaining
                        the transparency needed for safety and accountability.
                    </p>

                    <figure class="full-width">
                        <img src="1411.3146/nsai_3_8_v5.png" alt="First Hypothesis concept graph">
                        <figcaption>Subset of concept graph highlighting the 1st Hypothesis and focus of this section.
                        </figcaption>
                    </figure>
                </section>

                <section id="first-hypothesis">
                    <h2>First Hypothesis: Collective Control facilitates ABC</h2>

                    <p>
                        Taken together, as unilateral control averted attribution-based control, collective control over
                        how disparate data and model resources come together to create an AI prediction facilitates
                        attribution-based control. And by combining deep voting with the suite of encryption techniques
                        necessary for <em>structured transparency</em>, collective control in AI systems becomes
                        possible.
                    </p>

                    <p>
                        However, a fundamental challenge remains. While attribution-based control enables AI
                        users to select which sources inform their predictions, it introduces a trust evaluation problem
                        at
                        scale. In a system with billions of potential sources, individual users cannot practically
                        evaluate
                        trustworthiness of each contributor. Without mechanisms for scalable trust evaluation, users
                        may default to relying on centralized intermediaries, undermining the distributed control that
                        network-source AI can enable.
                    </p>
                </section>

                <section class="references">
                    <h2>References</h2>
                    <ol>
                        <li id="ref-1"><span class="authors">Trask, A., Bluemke, E., Garfinkel, B., Ghezzou
                                Cuervas-Mons, C., & Dafoe, A.</span> (2020). <a
                                href="https://arxiv.org/abs/2012.08347"><span class="title">Beyond Privacy Trade-offs
                                    with Structured Transparency.</span></a> <span
                                class="venue">arXiv:2012.08347</span>.</li>
                        <li id="ref-2"><span class="authors">Youssef, A., et al.</span> (2023). <a
                                href="https://doi.org/10.1001/jamanetworkopen.2023.48422"><span
                                    class="title">Organizational Factors in Clinical Data Sharing for Artificial
                                    Intelligence in Health Care.</span></a> <span class="venue">JAMA Network
                                Open</span>, 6, e2348422.</li>
                        <li id="ref-3"><span class="authors">G. A. Kaissis, M. R. Makowski, D. Ruckert, and R. F.
                                Braren.</span> (2020). <a href="https://doi.org/10.1038/s42256-020-0186-1"><span
                                    class="title">Secure,
                                    privacy-preserving and federated machine learning in medical imaging.</span></a>
                            <span class="venue">Nature Machine Intelligence</span>, 2:305–
                            311, 6.
                        </li>
                        <li id="ref-4"><span class="authors">Gentry, C.</span> (2009). <a
                                href="https://doi.org/10.1145/1536414.1536440"><span class="title">Fully homomorphic
                                    encryption using ideal lattices.</span></a> <span class="venue">STOC 2009</span>,
                            169–178.</li>
                        <li id="ref-5"><span class="authors">Costan, V., & Devadas, S.</span> (2016). <a
                                href="https://eprint.iacr.org/2016/086"><span class="title">Intel SGX
                                    Explained.</span></a> <span class="venue">IACR Cryptology ePrint Archive</span>,
                            2016/086.</li>
                        <li id="ref-6"><span class="authors">Yao, A. C.</span> (1982). <a
                                href="https://doi.org/10.1109/SFCS.1982.88"><span class="title">Protocols for Secure
                                    Computations.</span></a> <span class="venue">FOCS 1982</span>, 160–164.</li>
                        <li id="ref-7"><span class="authors">Goldreich, O., Micali, S., & Wigderson, A.</span> (1987).
                            <a href="https://doi.org/10.1145/28395.28420"><span class="title">How to play ANY mental
                                    game.</span></a> <span class="venue">STOC 1987</span>, 218–229.
                        </li>
                        <li id="ref-8"><span class="authors">Dwork, C., & Roth, A.</span> (2014). <a
                                href="https://doi.org/10.1561/0400000042"><span class="title">The Algorithmic
                                    Foundations of Differential Privacy.</span></a> <span class="venue">Foundations and
                                Trends in Theoretical Computer Science</span>, 9(3–4), 211–407.</li>
                        <li id="ref-9"><span class="authors">Goldwasser, S., Micali, S., & Rackoff, C.</span> (1989).
                            <a href="https://doi.org/10.1137/0218012"><span class="title">The Knowledge Complexity of
                                    Interactive Proof Systems.</span></a> <span class="venue">SIAM Journal on
                                Computing</span>, 18(1), 186–208.
                        </li>
                        <li id="ref-10"><span class="authors">Feige, U., Fiat, A., & Shamir, A.</span> (1988). <a
                                href="https://doi.org/10.1007/BF02351717"><span class="title">Zero-Knowledge Proofs of
                                    Identity.</span></a> <span class="venue">Journal of Cryptology</span>, 1(2), 77–94.
                        </li>
                        <li id="ref-11"><span class="authors">Laurie, B., Langley, A., & Kasper, E.</span> (2014). <a
                                href="https://www.rfc-editor.org/rfc/rfc6962"><span class="title">Certificate
                                    Transparency.</span></a> <span class="venue">RFC 6962</span>.</li>
                        <li id="ref-12"><span class="authors">Chase, M., et al.</span> (2020). <a
                                href="https://signal.org/blog/signal-private-group-system/"><span class="title">Signal:
                                    Private Group System.</span></a> <span class="venue">Signal Foundation</span>.</li>
                        <li id="ref-13"><span class="authors">Shamir, A.</span> (1979). <a
                                href="https://doi.org/10.1145/359168.359176"><span class="title">How to Share a
                                    Secret.</span></a> <span class="venue">Communications of the ACM</span>, 22(11),
                            612–613.</li>
                        <li id="ref-14"><span class="authors">Cummings, C. K.</span> (2017). <a
                                href="https://global.oup.com/academic/product/democracy-of-sound-9780190625009"><span
                                    class="title">Democracy of Sound: Music Piracy and the Remaking of American
                                    Copyright.</span></a> <span class="venue">Oxford University Press</span>.</li>
                        <li id="ref-15"><span class="authors">Shu, K., et al.</span> (2020). <a
                                href="https://doi.org/10.1002/widm.1385"><span class="title">Combating Disinformation in
                                    a Social Media Age.</span></a> <span class="venue">WIREs Data Mining and Knowledge
                                Discovery</span>, 10(6), e1385.</li>
                        <li id="ref-16"><span class="authors">Schneier, B.</span> (2015). <a
                                href="https://www.schneier.com/books/data-and-goliath/"><span class="title">Data and
                                    Goliath: The Hidden Battles to Collect Your Data and Control Your World.</span></a>
                            <span class="venue">W.W. Norton & Company</span>.
                        </li>
                        <li id="ref-17"><span class="authors">Veliz, C.</span> (2020). <a
                                href="https://www.penguin.co.uk/books/314/314359/privacy-is-power/9780552177719.html"><span
                                    class="title">Privacy Is Power: Why and How You Should Take Back Control of Your
                                    Data.</span></a> <span class="venue">Transworld Digital</span>.</li>

                        <li id="ref-18">
                            <span class="authors">
                                B. Fecher, S. Friesike, and M. Hebing. 2015.
                            </span>

                            <span class="venue">
                                What drives academic data sharing? <em>PLOS ONE</em>,
                                10:e0118053, 02.
                            </span>
                        </li>

                        <li id="ref-19">
                            <span class="authors">
                                G. A. Ascoli. 2015.
                            </span>

                            <span class="venue">
                                Sharing neuron data: carrots, sticks, and digital records. <em>PLoS Biol</em>,
                                13(10):e1002275.
                            </span>
                        </li>

                        <li id="ref-20">
                            <span class="authors">
                                M. Sienkiewicz. 2025.
                            </span>

                            <span class="venue">
                                From data silos to data mesh: a case study in financial data architecture.
                                In <em>International Conference on Database and Expert Systems Applications</em>, pages
                                3–20.
                                Springer.
                            </span>
                        </li>

                        <li id="ref-21">
                            <span class="authors">
                                I. Scott and T. Gong. 2021.
                            </span>

                            <span class="venue">
                                Coordinating government silos: challenges and opportunities.
                                <em>Global Public Policy and Governance</em>, 1(1):20–38.
                            </span>
                        </li>

                        <li id="ref-22">
                            <span class="authors">
                                N. Vidal. 2024.
                            </span>

                            <span class="venue">
                                Compelling responses to NTIA’s AI open model weights RFC. Open Source
                                Initiative Blog, April. Accessed: 2025-11-04.
                            </span>
                        </li>

                        <li id="ref-23">
                            <span class="authors">
                                National Telecommunications and Information Administration. 2024.
                            </span>

                            <span class="venue">
                                NTIA AI open model
                                weights RFC. Request for Comment, Docket No. NTIA-2023-0009, February. Comment
                                period closed March 27, 2024.
                            </span>
                        </li>

                        <li id="ref-24">
                            <span class="authors">
                                Associated Press. 2025
                            </span>

                            <span class="venue">
                                Ai startup anthropic agrees to pay $1.5bn to settle book piracy lawsuit.
                                <em>The Guardian</em>, September. Settlement could be pivotal after authors claimed
                                company took
                                pirated copies of their work to train chatbots.
                            </span>
                        </li>

                        <li id="ref-25">
                            <span class="authors">
                                Open Source Initiative. 2024.
                            </span>

                            <span class="venue">
                                The open source ai definition – 1.0. Accessed: 2025-11-04.
                            </span>
                        </li>

                        <li id="ref-26">
                            <span class="authors">
                                C. Owen-Jackson. 2024.
                            </span>

                            <span class="venue">
                                Open source, open risks: The growing dangers of unregulated
                                generative ai. IBM Think.
                            </span>
                        </li>

                        <li id="ref-27">
                            <span class="authors">
                                J. Grow. 2025.
                            </span>

                            <span class="venue">
                                The zuckerberg-lecun ai paradox: A tale of two visions. <em>Medium</em>, August.
                                Accessed: 2025-11-04.
                            </span>
                        </li>

                        <li id="ref-28">
                            <span class="authors">
                                I. Gabriel, A. Manzini, G. Keeling, L. A. Hendricks, V. Rieser, H. Iqbal, N. Tomasev, I.
                                Ktena, ˇ
                                Z. Kenton, M. Rodriguez, S. El-Sayed, S. Brown, C. Akbulut, A. Trask, E. Hughes, A. S.
                                Bergman, R. Shelby, N. Marchal, C. Griffin, J. Mateos-Garcia, L. Weidinger, W. Street,
                                B. Lange, A. Ingerman, A. Lentz, R. Enger, A. Barakat, V. Krakovna, J. O. Siy, Z.
                                KurthNelson, A. McCroskery, V. Bolina, H. Law, M. Shanahan, L. Alberts, B. Balle, S. de
                                Haas,
                                Y. Ibitoye, A. Dafoe, B. Goldberg, S. Krier, A. Reese, S. Witherspoon, W. Hawkins, M.
                                Rauh,
                                D. Wallace, M. Franklin, J. A. Goldstein, J. Lehman, M. Klenk, S. Vallor, C. Biles, M.
                                R.
                                Morris, H. King, B. A. y Arcas, W. Isaac, and J. Manyika. 2024.
                            </span>

                            <span class="venue">
                                The ethics of advanced ai
                                assistants.
                            </span>
                        </li>

                        <li id="ref-29">
                            <span class="authors">
                                A. Wealand. 2025.
                            </span>

                            <span class="venue">
                                Reducing bias in AI models through open source. Red Hat Blog, September.
                                Accessed: 2025-11-04.
                            </span>
                        </li>

                        <li id="ref-30">
                            <span class="authors">
                                F. Pasquale. 2015.
                            </span>

                            <span class="venue">
                                <em>The Black Box Society</em>. Harvard University Press.
                            </span>
                        </li>

                        <li id="ref-31"><span class="authors">Gould, J.</span> (2015). <a
                                href="https://web.archive.org/web/20210421105442/http://blogs.nature.com/naturejobs/2015/09/21/data-sharing-why-it-doesnt-happen/"><span
                                    class="title">Data sharing: Why it doesn’t happen.</span></a>
                            <span class="venue">Nature Jobs</span>.
                        </li>

                        <li id="ref-32">
                            <span class="authors">
                                M. M. Grynbaum and R. Mac. 2023.
                            </span>

                            <span class="venue">
                                The times sues openai and microsoft over a.i. use of
                                copyrighted work. <em>The New York Times</em>, December.
                            </span>
                        </li>

                        <li id="ref-33">
                            <span class="authors">
                                J. K. Elsea. 2006.
                            </span>

                            <span class="venue">
                                The protection of classified information: The legal framework. Technical
                                report.
                            </span>
                        </li>

                        <li id="ref-34">
                            <span class="authors">
                                B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. 2017.
                            </span>

                            <span class="venue">
                                Communication efficient learning of deep networks from decentralized data. In
                                <em>Artificial
                                    Intelligence and
                                    Statistics</em>, pages 1273–1282. PMLR.
                            </span>
                        </li>

                        <li id="ref-35">
                            <span class="authors">
                                D. Bogdanov, P. Laud, S. Laur, and P. Pullonen. 2014.
                            </span>

                            <span class="venue">
                                From input private to universally
                                composable secure multi-party computation primitives. In <em>2014 IEEE 27th Computer
                                    Security Foundations Symposium</em>, pages 184–198. IEEE.
                            </span>
                        </li>

                        <li id="ref-36">
                            <span class="authors">
                                M. Craddock, D. Archer, D. Bogdanov, A. Gascon, B. Balle, K. Laine, A. Trask,
                                M. Raykova, M. Jug, r. McLellan, R. Jansen, O. Ohrimenko, S. Wardly,
                                K. Lauter, N. Smart, A. Sharan, I. Saxena, R. Wright, E. Garcia, and
                                A. Wall. 2018.
                            </span>

                            <a
                                href="https://arxiv.org/abs/2301.06167">
                                <span class="title">UN Handbook on Privacy-Preserving Computation Techniques.</span>
                            </a>
                        </li>

                        <li id="ref-37">
                            <span class="authors">
                                D. Boneh, A. Sahai, and B. Waters. 2011.
                            </span>

                            <span class="venue">
                                Functional encryption: Definitions and challenges.
                                In <em>Theory of Cryptography Conference</em>, pages 253–273. Springer.
                            </span>
                        </li>

                        <li id="ref-38">
                            <span class="authors">
                                C. Dwork, F. McSherry, K. Nissim, and A. Smith. 2006.
                            </span>

                            <span class="venue">
                                Calibrating noise to sensitivity in
                                private data analysis. In <em>Theory of Cryptography: Third Theory of Cryptography
                                    Conference,
                                    TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3</em>, pages 265–284.
                                Springer.
                            </span>
                        </li>

                        <li id="ref-39">
                            <span class="authors">
                                Sovrin
                            </span>

                            <span class="venue">
                                The sovrin network and zero knowledge proofs. sovrin.org.
                            </span>
                        </li>

                        <li id="ref-40">
                            <span class="authors">
                                F. Wang and P. De Filippi. 2020.
                            </span>

                            <span class="venue">
                                Self-sovereign identity in a globalized world: Credentialsbased identity systems as a
                                driver for economic inclusion. <em>Frontiers in Blockchain</em>, 2,
                                01.
                            </span>
                        </li>

                        <li id="ref-41">
                            <span class="authors">
                                J. Loftus and N. P. Smart. 2011.
                            </span>

                            <span class="venue">
                                Secure outsourced computation. In <em>International Conference
                                    on Cryptology in Africa</em>, pages 1–20. Springer.
                            </span>
                        </li>

                        <li id="ref-42">
                            <span class="authors">
                                D. Chaum. 1985.
                            </span>

                            <span class="venue">
                                Security without identification: Transaction systems to make big brother
                                obsolete. <em>Commun. ACM</em>, 28(10):1030–1044, October
                            </span>
                        </li>

                        <li id="ref-43">
                            <span class="authors">
                                S. Adler, Z. Hitzig, S. Jain, C. Brewer, V. Srivastava, B. Christian, and A. Trask.
                                2024.
                            </span>

                            <span class="venue">
                                Personhood credentials: Artificial intelligence and the value of privacy-preserving
                                tools to
                                distinguish who is real online.
                            </span>
                        </li>

                    </ol>
                </section>

                <nav class="chapter-nav">
                    <a href="chapter2.html" class="prev">Chapter II: Deep Voting</a>
                    <a href="chapter4.html" class="next">Chapter IV: Broad Listening</a>
                </nav>
            </main>
        </div>

        <footer>
            <p>Andrew Trask &middot; University of Oxford</p>
        </footer>
    </div><!-- /.wrapper -->

    <img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQCa5kJX0IfZqgsgGUg5LB470wWtz3nfi_DuQ&s"
        alt="University of Oxford" class="oxford-logo-fixed">

    <script src='main.js' type='text/javascript' defer></script>
    <script src='citation-card.js' type='text/javascript' defer></script>
</body>

</html>