\documentclass[12pt]{ociamthesis}
\pdfoutput=1

\usepackage{etex}
\usepackage{times}

\usepackage{booktabs}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{stmaryrd}
\SetSymbolFont{stmry}{bold}{U}{stmry}{m}{n}

\usepackage{tcolorbox}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{array}
\usepackage[outercaption]{sidecap}
\usepackage{dcolumn}

\usepackage{url}    
\usepackage{ccg}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{xspace}
\usepackage{color}

\bibliographystyle{acl}

\usepackage{amssymb}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}  % checkmark
\newcommand{\xmark}{\ding{55}}  % X mark

\usepackage{tikz}
\usepackage{pgfplots}
% \pgfplotsset{compat=1.9}
\usepackage{tikz-qtree}
\usetikzlibrary{arrows,positioning,calc,backgrounds,shapes,fit,mindmap,shadows}
\usetikzlibrary{matrix,decorations.pathreplacing}
\usetikzlibrary{fadings}

% Essential math packages
\usepackage{amsmath}    % Advanced math formatting
\usepackage{amssymb}    % Additional math symbols
\usepackage{amsthm}     % For theorem environments
\usepackage{mathtools}  % Enhanced math tools

% Theorem environment setup
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{remark}[definition]{Remark}

% Optional but recommended packages
\usepackage{microtype}  % Better typography
\usepackage{enumitem}   % Better list formatting
\usepackage{geometry}   % Page margins
\geometry{
a4paper,
margin=1in
}

\usepackage{thesis}

\makeatletter
\newcommand{\@BIBLABEL}{\@emptybiblabel}
\newcommand{\@emptybiblabel}[1]{}
\makeatother
\usepackage{hyperref}

\hypersetup{
colorlinks=true,
linkcolor=black,
citecolor=black,
filecolor=black,
urlcolor=black,
}

\usepackage[sort]{cite}
\renewcommand{\citepunct}{; }  % Added space after semicolon
\makeatletter
% Change brackets to parentheses and fix spacing
\def\@cite#1#2{({#1\if@tempswa, #2\fi})}
\makeatother

\DeclareMathOperator*{\argmax}{arg\,max}

% \title{Oops, Wrong Way: Centralizing AI to Lose to China}
% \title{The Backwards AI Race: To Win Against China is Not To Imitate China}
% \title{The Backwards AI Race: To Win Against China is Probably Not To Imitate China}
\title{Attribution-Based Control in AI Systems}

\author{Andrew Trask}
\college{St. Hugh's College}

\degree{Doctor of Philosophy}
\degreedate{(submission date TBD)}

\begin{document}

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{2}

\maketitle

% \include{0_acknowledgements}
% \begin{acknowledgements}

% \end{acknowledgements}


% \include{0_abstract}
% \begin{abstract}
\begin{abstract}
Many of AI's risks in areas like privacy, value alignment, copyright, concentration of power, and hallucinations can be reduced to the problem of attribution-based control (ABC) in AI systems, which itself can be reduced to the overuse of addition, copying, and branching within gradient descent. This thesis synthesizes a handful of recently proposed techniques in deep learning, cryptography, and distributed systems, revealing a viable path to reduce addition, copying, and branching; provide ABC; and address many of AI's primary risks while accelerating its benefits by unlocking 6+ orders of magnitude more data, compute, and associated AI capability.
\end{abstract}

\begin{romanpages}
\tableofcontents
\listoffigures
\listoftables
\end{romanpages}

\chapter{Introduction}


\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{broad_listening_v4.png}
\caption{Traditional open/closed-source AI systems (left) centralize data collection and decision-making, while ABC-enabled AI systems (right) enable direct communication between those with data and those seeking insights. }
% In ABC-enabled systems, attribution and control flow with the information, enabling verification of sources and protection of contributor interests.
\label{fig:broad_listening}
\end{figure}

While AI systems learn from data, AI users cannot know or control which data points inform which predictions because AI lacks \textit{attribution-based control} (ABC). Consequently, AI draws upon sources whose exact integrity cannot be verified. And when AI draws upon incomplete or problematic sources, it struggles with hallucinations, disinformation, value alignment, and a greater collective action problem: while an AI model in a democratic nation is as capable as the amount of data and compute a company can collect, an AI model in an authoritarian nation may become as capable as the amount of data and compute an entire nation can collect. Consequently, to win the AI race, democratic nations must either embrace the centralization of data and compute (i.e., centralize AI powered decision making across their society) or relinquish AI advantage to nations who do.

This thesis describes how these problems stem from a single technical source: AI's over-reliance on copying, addition, and branching operations during training and inference. By synthesizing recent breakthroughs in cryptography, deep learning, and distributed systems, this thesis proposes a new method for AI with attribution-based control. When fully bloomed, this breakthrough can enable each AI user to control which data sources drive an AI's predictions, and enable an AI's data sources to control which AI users they wish to support, transforming AI from a centralized tool that produces intelligence to a communication tool that connects people. 

Taken together, this thesis describes an ongoing technical shift, driven by the hunt for 6-orders of magnitude more data and compute, and driven by value alignment to powerful market and geo-political forces of data owners and AI users, which is transforming AI from a tool of central intelligence into something radically different: a communication tool for \textit{broad listening}. 

\section{The Problem of Attribution-Based Control (ABC)} 

\vspace{10pt}

\begin{definition}[Attribution-Based Control]
\label{def:abc}
An AI system provides attribution-based control (ABC) when it enables a bidirectional relationship between two parties:
\begin{itemize}
    \item \textbf{Data Sources}: control which AI outputs they support with AI capability, and calibrate the degree to which they offer that support.
    \item \textbf{AI Users}: control which data sources they rely upon for AI capability, and calibrate the degree to which they rely on each source.
\end{itemize}
They negotiate which sources are leveraged and how much capability to create.
\end{definition}

% \vspace{10pt}

At the present moment, AI systems do not comprehensively enable AI users to know or control which data sources inform which AI outputs (or vice versa) which is to say they do not offer attribution-based control (ABC). While RAG and similar algorithms might appear to offer inference-level control, AI systems fail to provide formal guarantees that data provided to the input of a model is truly being leveraged to create the output. Taken together, with respect to data provided during pre-training, fine-tuning, or inference, AI systems do not offer ABC.

Consequently, AI is conceived of (by people in the world) as a system that gains intelligence by harvesting information about the world, as opposed to a communication technology which enables billions of people with information to aid billions seeking insights. In the same way, an AI model is conceived of as the asset which produces intelligence, instead of merely as a temporary cache of information making its way from sources to users. If AI were a telephone, it would be one without buttons, and its users would believe they are talking to the phone itself, as opposed to talking with people \textit{through the phone}.

This design decision triggers a cascade of consequences, consequences which are felt uniquely at each level of society: individual, institutional, societal, and geo-political. And as a result, each level of society is incubating deep desires for something more than today's AI has to offer. The next four subsections survey this increasing demand for ABC in AI by surveying the felt consequences of the lack of ABC in AI.
\vspace{5pt}
\\
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{1411.3146/abc_2_v5.png}
\caption{Insufficient ABC underpins problems at several levels.}
\label{fig:broad_listening}
\end{figure}
\vspace{-20pt}
\\
\subsection{Individual Consequences}

The lack of ABC presents AI users with a basic authenticity problem, discussed publicly using phrases like hallucination, deepfake, and disinformation \cite{dziri_origin_2022, vaccari_deepfakes_2020}. When an AI generates a response, users often cannot verify its sources; unlike a Google search where one can examine original documents \cite{zuccon_hallucination_attribution, schmidt2014google}. Consider a medical student using AI to research rare diseases. They cannot tell whether an AI's detailed symptom description comes from high-quality sources (e.g., peer-reviewed journals) or less qualified sources \cite{GRAVEL2023226, bhattacharyya2023high}. Meanwhile, the researchers who authored medical papers feeding the AI lose control over how their work is used, dis-incentivizing them to share information \cite{health_sharing_incentives}. This mutual blindness creates the opportunity for hallucination and disinformation.
\\
\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{1411.3146/individual_consequences_v3.png}
\caption{Traditional open/closed-source AI systems (left) mathematically delete information linking source and prediction. Once this information is gone, neither adding more data (RLHF), looking for source information (influence functions), or providing oversight over AI systems addresses the core issue. Instead, ABC-enabled AI systems (right) enable direct communication between those with data and those seeking insights. In ABC-enabled AI, attribution and control flow with the information, enabling verification of sources and protection of contributor interests.}
\label{fig:broad_listening}
\end{figure}
\\
\textbf{Hallucination:} To see ABC's contribution to the problem of hallucination, consider the mechanics of an AI hallucination. In some cases, an AI model makes a prediction regarding a subject either not properly covered in its training data or prompt, or not properly recalled during inference \cite{yu-etal-2024-mechanistic}. But since an AI model's predictions are necessarily based on data, the AI will attempt to use insights from less related documents\footnote{By "less related documents", I mean that since AI models only output information based on data, that if they cannot find data relevant to the current prompt, they will still output results based on \textit{some kind of data}. In the limit, an AI model will leverage its general corpus to create a response which is grammatical, even if no training data document is relevant to the current query. The plausibility of such outputs despite their lack of grounding is the root behind their characterization as \textit{hallucination}.} to generate a plausible-sounding response: a hallucination. Yet, because of a lack of ABC, the AI user has little ability to spot an AI model sourcing from such unqualified documents, or to make adjustments to ensure the AI's prediction is properly sourced. The AI user, for example, has no ability to say, "I queried this AI model about Java development, but I notice it's relying upon data sources about Indonesian coffee cultivation instead of programming language documentation." Instead, the AI user merely sees the output and wonders, "Is this AI generated statement true or is it merely grammatical?".
\\
\\
\textbf{Disinformation:} To see ABC's contribution to the problem of disinformation, consider the mechanics of a misleading prediction. In some cases, an AI model makes a prediction by combining concepts which ought-not be combined if they are to yield something truthful. An AI model queried to create a fake picture of a celebrity performing a controversial act, for example, might recall (pre-training) data about the likeness of a celebrity with data about unrelated people performing that controversial act, yielding an insidious form of disinformation: a deepfake. 

In scientific literature, journalism, or law, a reader might easily spot such disinformation by consulting a written work's bibliography and considering whether original sources are relevant, reputable, and being synthesized properly into a novel insight. Not so with AI. Because of a lack of ABC, the end user viewing synthetic media has no source bibliography (and thus no ability to spot an AI prediction sourcing from such unqualified pre-training/RLHF/etc. data points, or to make adjustments to ensure the AI's prediction ignores/leverages data points appropriately \footnote{As before, a reader might object and say that RAG or other context window information might inform the user of this problem, and thus provide ABC. However, an AI model need not actually use any of its inputs. Consequently, AI users have no ability to know whether an AI model used information fed into the input, nor do they have the ability to know which non-input information the AI model relies upon for a prediction.}). The AI user, for example, has no ability to say, "I queried this AI model for a picture of London, so why is it also sourcing from Hilton Hotels promotional materials instead of only photos of the English capital?". Consequently, an AI prediction can contain misleading information whose message is compelling but whose deceitful source is imperceptible to the viewer.
\\
\\
\textbf{Proposed Remedies:} Within the context of LLMs, recent work has attempted to address this through a myriad of methods such as, data cleaning, fine-tuning more data into AI models (e.g., RLHF, InstructGPT), direct oversight of deployed AI (e.g., HITL, auditing), measures of AI confidence (i.e., self checking), improving prompts (e.g., chain of thought, RAG), source attribution methods to reverse engineer the source-prediction relationship (i.e.,  influence functions), and others \cite{RLHF, roozbahani2025review, ahmad2023creatingtrustworthyllmsdealing, manakul2023selfcheckgptzeroresourceblackboxhallucination}. 

These efforts, however, face two fundamental limitations which prevent them from solving the issue. First, it has been shown that LLMs will always hallucinate because they cannot learn all computable functions between a computable LLM and a computable ground truth function \cite{xu2024hallucinationinevitableinnatelimitation} (a proof which focuses on LLMs but requires no specific tie to language or transformers). And second, since AI will always hallucinate to some extent, the question is whether or not detection is possible and addressable. Since AIs only know how to output true facts based on the data they have consumed, this concerns measuring and controlling whether an AI model is sourcing from appropriate experts for each of its statements. That is, detecting disinformation, hallucinations, and other authenticity issues is not \textit{first} a problem of intelligence, it's \textit{first} a problem of attribution (and then perhaps a problem of intelligence).

However, as this thesis will soon show, when neural networks combine information through addition operations during training, they irreversibly compress the relationship between sources and predictions. No amount of post-training intervention (e.g. fine-tuning, auditing, influence functions, etc.) can fully restore these lost connections, solve the lack of ABC, and empower AI users to better avoid and address AI hallucinations, disinformation, or deepfakes. Yet, if attribution was solved, while it would still be possible to compel an AI to produce false information, it may become difficult (if not impossible) to do so without also revealing the inappropriate sources used to generate such false statements \footnote{This would be because AI models do not generate predictions out of thin air. They are data-trained machines, and thus must pull information from sources in some respect. Consequently, if such sources were visible to the final AI user, they would have a significant leg up (or definitive ability) to detect disinformation and hallucinations.}. Altogether, AI needs a bibliography.

\subsection{Institutional Consequences}

\\
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{1411.3146/abc_3_v8.png}
\caption{Insufficient ABC underpins problems for data-owning institutions.}
\label{fig:broad_listening}
\end{figure}
\\

The lack of ABC creates a basic incentive problem at the institutional level. Data-owning institutions can either share data to create AI, or they can decline and maintain control over their data, a dilemma discussed publicly using words like copyright, privacy, transparency, and misuse \cite{st, Steptoe}. Leading cancer centers, for example, struggle to train AI to detect breast cancer using more than 3-4 million mammograms (the largest and most diverse dataset known to this author being \cite{jeong2022emorybreastimagingdataset}). Yet hundreds of millions of mammograms are produced annually worldwide \cite{global_cancer_screening, us_cancer_screening}. 

This gap exists because medical facilities face an impossible decision: either withhold their data to protect patient privacy and maintain control (i.e. protect privacy, IP, security, legal risk, etc.), or share it and lose all ability to control how it's used \cite{st}. In 2023, multiple major medical centers explicitly cited this dilemma when declining to participate in AI research \cite{privacy_blocks_medical_sharing}. Yet their complaint is not unique to medical information, it exists for any data owner who has some incentive to care about when and how their data is used \cite{data_sharing_doesnt_happen}.

The consequences of unmotivated data owners are staggering to consider. Despite AI drawing the attention of the world, even the most famous AI models have not inspired most of the world's data or compute owners to collectively act to train it. As described in chapter 2, 6 orders of magnitude more data, and 6 orders of magnitude more compute productivity remain untapped, such that even the most powerful AI models are trained on less than a millionth of the possible resources available. 
\\
\\
\textbf{Fighting Against Data and Compute for AI — Copyright, Siloing, and Rate-limiting:} The resistance against AI training is widespread and highly vocal. It can be found regarding websites as big as Reddit, the New York Times, and Twitter, and as small as individual bloggers who are fighting against onerous scrapers \cite{grybauskas2023twitter, obrien2025reddit, samuelson2023generative, grynbaum2023times}. Resistance can be found across media and entertainment, but also in the siloing of the world's structured data \cite{patel2019bridging}. This resistance has cultivated a profound contradiction (unpacked in Chapter 2): the AI community feels there is a lack of data and compute (enough for NVIDIA chips to be in short supply and Ilya Sutzskever to announce "peak data") while around 1,000,000x more of each remains untapped \cite{robison2024openai}. 
\newpage
\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{1411.3146/institutional_consequences.png}
\caption{Traditional open/closed-source AI systems (left) copy data to AI providers, giving them unilateral control over the resulting model and its predictions. Instead, ABC-enabled AI systems (right) enable direct communication between those with data and those seeking insights. In ABC-enabled AI, attribution and control flow with the information, enabling data sources to retain control over which predictions they seek to support.}
\label{fig:broad_listening}
\end{figure}

Recent work has suggested that privacy-enhancing technologies (PETs) can solve this collective action problem, suggesting technologies like federated learning, secure multi-party computation, and synthetic data \cite{fl, Rieke_2020, smpc_ml, synthetic_data_privacy}. These efforts, however, face a fundamental limitation: they may enable data owners to retain control over the only copy of their information, but each of these technologies alone do not enable data owners to obtain fine-grained control over which AI predictions they wish to support. Consequently, none provide attribution-based control.

Federated learning and synthetic data, for example, avoid the need to share raw data, enabling them to retain control over that data, but they don't enable data providers to \textit{collectively} control how an AI model they create is used because they do not provide a user-specific (i.e. attribution-based) control mechanism \cite{mcmahan2017communication, rubin1993statistical}. Secure multi-party computation schemes (secure enclaves, SPDZ, etc.) might allow for joint control over an AI model, but they do not inherently provide the metadata or control necessary for data providers to add or withdraw their support for specific AI predictions (i.e. attribution) \cite{yao1982protocols}. To use SMPC for ABC, one would need to re-train AI models whenever a part of an AI model's pre-training data needs to be removed (such as for compliance with \textit{right to be forgotten} laws \cite{lobo2023right}).

That is to say, unless AI models are continuously retrained, the SMPC problem is one of decision bundling (analogous to economic bundling), such that an entire group would need to decide whether to leverage an entire model... or not. Taken together, while some specific privacy concerns have been addressed by PETs, the broader incentive issues averting collective action have not. PETs offer collective production of AI models through encryption, but PETs do not offer fine-grained control by each contributing data source (i.e. attribution-based control)... because AI models don't reveal source attribution per prediction.

Yet, if attribution was solved in AI systems (in a more efficient manner than model retraining), it could be combined with techniques like FL or SMPC to provide ABC. Thus, the complementary challenge is attribution (i.e., the ability to enable each data participant to efficiently elect to support or not support specific AI predictions) without needing to relinquish or economically bundle control within AI models. This attribution task is also referred to as \textit{machine unlearning}, which at the time of writing remains an open problem in the literature \cite{nguyen2024surveymachineunlearning}. 
\newpage
In chapter 2, this thesis reveals existing, overlooked remedies to attribution/unlearning, which enables chapter three to combine those ingredients with SMPC and related privacy enhancing technologies. Together, chapters 2 and 3 reveal existing, overlooked techniques for both attribution and control, revealing the AI community's viable (albeit presently overlooked) path to ABC, and a means to reverse incentives presently blocking 6+ orders of magnitude more data and compute productivity.

\subsection{Societal Consequences}

\\
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{1411.3146/abc_4_v4.png}
\vspace{5px}
\caption{Insufficient ABC underpins representation dilemmas for societies seeking to solve bias and value alignment problems.}
\label{fig:broad_listening}
\end{figure}
\\

The lack of ABC creates a governance crisis unprecedented in scale at the societal level. This crisis manifests publicly as concerns about AI safety, value alignment, and algorithmic bias \cite{amodei2016concreteproblemsaisafety, gabriel2020artificial, ntoutsi2020bias}. Crisis arises because AI systems can derive their capabilities from millions of contributors yet remain controlled by whomever possesses a copy of an AI. This control structure sparks concern that either the AI or its owner will use the AI's intelligence in ways that conflict with society's values or interests \cite{gabriel2024ethicsadvancedaiassistants}. An LLM trained on chemistry research papers, for example, could be fine-tuned to generate instructions for bioweapons, with no way for the training data creators to prevent this misuse \cite{ai_bioweapon}.

Recent work has proposed various oversight mechanisms and safety guidelines in response \cite{industryAustraliaAI2024, ecGermanyAI2024, ecFranceAI2024, ecWhitePaperAI2020, ieeeEthicsAI2021, futureOfLifeAIPrinciples2024, accessNowTorontoDeclaration2023, aiNowAlgorithmicAccountability2024, montrealDeclaration2024, weforumAIGovernance2023, harvardPrincipledAI2020, turingAIethics2019, googleAIPrinciples2024, microsoftAIPrinciples2024, ibmAIPrinciples2018}. However, these efforts face a limitation history has shown before: attempting to solve the problem of serving many by empowering few \cite{riesen2023imagine, pope2023centralizing, crispweed2024alignment, summerfield2025impact}. When individuals, small groups, or tech companies train AI models on global internet data, decisions about content filtering and model behavior end up being made by AI models or small teams (at most thousands attempting to represent billions). AI governance therefore becomes an exercise in altruism, operating under the hope that AI models, IT staff, or institutions controlling them will forego selfish incentives to serve humanity's broader interests \cite{welle2025aligning}. These hopes may be disappointed: despite grand claims, leading AI labs may not be successfully evading their selfish incentives \cite{Samuel_2024, Hu_Cai_2024, Morales_2024}.

\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{1411.3146/societal_challenges.png}
\caption{A fictional illustration of an AI model trained on data from people across the US Pacific coast, and used by people in the eastern half of the United States. However, irrespective of where the training data comes from or the predictions go to, a handful of people at the AI organization retains centralized control over the AI model's capability and use. This calls into question how/whether the AI or its creators will forego selfish incentives and use the AI's intelligence to serve society broadly.}
\label{fig:broad_listening}
\end{figure}

Attempted solutions to these problems remain insufficient because they don't address the underlying representative control problem. This problem stems from a fundamental decoupling: AI capability and AI alignment operate independently. Due to scaling laws, an AI model becomes more capable when its owner can centralize more data and compute \cite{scaling_laws_2020, hoffmann2022trainingcomputeoptimallargelanguage}. Yet its alignment with human values depends on the AI model and/or its supervisors choosing to forego selfish incentives, two forces that are not naturally correlated \cite{crispweed2024alignment}. This decoupling creates a dangerous asymmetry: unlike democratic systems, where a leader's power ideally scales with public support, AI systems concentrate power without requiring \textit{ongoing public consent}. This asymmetry intensifies as AI becomes more capable, threatening democratic values through greater autonomy, adoption, and complexity.

This represents not merely a policy problem, but an architectural one rooted in how AI systems process information \cite{pope2023centralizing}. As this thesis shows, when neural networks irreversibly: 1) combine data through addition operations and 2) decouple models from source data through copy operations, and 3) route the selection of trusted sources and users through ultra-high branching intermediaries, they sever the connection between data contributors and AI users, shifting control over an AI model to the model itself and/or its owner. This architectural design means no amount of after-the-fact regulation can restore the lost capability for continuous, granular, public consent over how AI systems use society's knowledge (consider, for example, music piracy) \cite{st}. The model remains controlled by whomever has a copy, and whomever has a copy (person or AI) may or may not elect to forego selfish incentives and serve the interests of society broadly. Society therefore faces a growing, collective need for representative control over AI systems, motivating a technical need for ABC within AI systems.

Yet, even if such AI-creating institutions could be made align-able to society, AI systems possess an even deeper flaw. An AI system's behavior is determined by its training data (first by pre-training data, then overridden in specific ways through RLHF/RLVF training updates). However, centralized institutions are gathering training data at a scale far greater than their ability to read and vet for data poisoning attacks. Consequently, as corporations, governments, and billions around the world consider relying upon AI systems (most especailly agents) for information and decision making, they open themselves up to an uncomfortable fact: anyone could publish a few hundreds pages to the internet and change the behavior of their AI system in the future, likely in a way which is undetectable by AI companies. 

As an early example of this, in 2011 the Huffington Post broke a story about the \textit{Hathaway Effect}, that when famous actress Anne Hathaway experiences a significant career moment, similarly-named Berkshire Hathaway stock rises as a result of algorithmic agents failing to disambiguate in online sentiment analysis \cite{mirvish2011hathaway}. The setup is a direct comparison to LLMs. These stock agents crawl the web looking for content, aggregating sentiment about companies (and their products, services, management teams, etc.), and translate these indicators into stock signals used for training at major financial institutions. However, while the internet contains useful signal for trading stocks, it also presents itself as a wide-open funnel for noise. And through sheer chance, Anne Hathaway's last name provides the accidental noise some early AI agents can fall for.

And while this somewhat humorous societal phenomena is perhaps harmless, it is a very real example of a more disturbing societal risk for modern AI agents. In a very real sense, as web-trained AI agents make their way into government agencies, job application portals, healthcare systems, financial models, security/surveillance systems, customer service portals, credit rating models, they are all making their decision making process available for anyone in the world to edit, simply by uploading a webpage with the right type of data poisoning content, and making that webpage available to the right web crawlers. Consequently, even if centralized AI companies were altruistic, the data they rely upon for intelligence need not be. The simultaneous problems of poisonous data and non-altruistic intermediaries provides technical motivation which goes beyond mere representative control; to solve these problems, society needs representative control which can be source-weighted for veracity in a specific context, attribution-based control in AI systems.

\subsection{Geopolitical Consequences}

\\
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{1411.3146/abc_5_v5.png}
\caption{Insufficient ABC underpins dilemmas for democratic societies seeking to maintain their values while winning the AI race against authoritarian nations.}
\label{fig:broad_listening}
\end{figure}
\\

The lack of ABC creates tension between AI capability and democratic values at the geopolitical level. AI systems become more powerful with more data and compute, as described by so-called "Scaling Laws" \cite{scaling_laws_2020, hoffmann2022trainingcomputeoptimallargelanguage} and implied by the "Bitter Lesson" \cite{sutton2019bitter}. But without ABC, acquiring data (and compute) means acquiring a copy of it within a company, and acquiring a copy means centralizing unilateral control...  over the data, over the compute, and over the AI systems this centralization creates.

This dilemma constrains democratic nations to two normative paths. Corporate AI caps AI capability at whatever data and compute companies can acquire (e.g. Google, Meta, X, etc.). National AI caps AI capability at whatever data and compute nations can centralize (e.g., China). Democratic nations face this tension most clearly in situations like the US-China AI competition: they must either compromise their principles by centralizing data and compute, or cede AI leadership to autocratic nations who face no such constraints.

Some policymakers have responded to this crisis not by addressing the underlying ABC problem, but by proposing a "Manhattan Project" for AI \cite{Tong_Martina_2024}. These proposals suggest competing with China's centralization by out-centralizing them... meeting authoritarian AI capability by building a larger, even more centralized AI capability domestically.

\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{1411.3146/geopolitical_consequences.png}
\caption{A fictional illustration of a traditional AI model trained within organizations across the US (left) and trained across all organizations in China (right). It is meant to illustrate the difference between resources acquired at the corporation level and at the nation level.}
\label{fig:broad_listening}
\end{figure}

Such an approach would face multiple challenges. To win, it would require overlooking democratic principles (e.g., privacy, personal property, freedom of choice, etc.) to centralize AI-powered decision making in a single, maximally-powerful, state-sponsored AI. It would still face uncertainty about exceeding China's AI capability; while the US may have a head-start on compute and talent availability, China has a 4x larger population from which to draw data.

Yet this dilemma only exists because of the lack of ABC. With ABC, vast swaths of data providers could collectively create individual AI predictions. And with ABC, AI users could request which data sources they wish to use for their specific AI predictions. ABC would subvert the need to centralize data, compute, and talent by creating a third AI paradigm beyond corporate AI or national AI... AI as powerful as the amount of data, compute, and talent across the global free market. For now, however, emerging ABC remains largely unrecognized, while policymakers consider the centralization of data and AI-powered decision making at an unprecedented scale.

\subsection{Centralization Consequences} 

\\
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{1411.3146/abc_6_v3.png}
\caption{Insufficient ABC creates advantages for centralized power.}
\label{fig:broad_listening}
\end{figure}
\\

At its core, AI intermediates a flow of information from billions of people (i.e. training data sources) to billions of people (i.e. AI users). However, the lack of ABC in AI systems obfuscates this underlying communication paradigm and presents AI as a tool of central decision making. Yes, AI systems promise to process humanity's knowledge at unprecedented scale. Yet their very architecture forces central AI creators to first accumulate that knowledge (even for "open-source" AI), then guide how society accesses and uses it. Rather than enabling direct communication between those who have knowledge and those who seek it, AI systems become intermediaries that extract knowledge from many to serve the interests of few (albeit by guiding the decisions of many). It reminds one of the rise of the mainframe computer in the 1950s and 60s, which obfuscated computing's forthcoming dominant use as a communication technology (i.e. the internet), presenting computing instead as a centralized coordination apparatus.

In AI, this architectural obfuscation creates stark dilemmas. Users must trust central authorities about what sources inform AI outputs. Data owners must choose between sharing knowledge and maintaining control over how it's used, starving AI of all but 0.0001\% of the world's data. Society must accept governance by tiny groups over systems trained on global knowledge. And democracies must either embrace centralized control over data, compute, and AI-powered decision-making or cede AI leadership to nations who do. Each of these dilemmas stems from the same technical limitation: AI systems which fail to enable AI users and data sources to control when and how AI is used...  ceding that power to central AI creators instead.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{1411.3146/centralization_vs_communication.png}
\caption{(left) An illustration of AI as an intermediary between those who have information and those who seek insights. (right) An illustration of AI as a communication tool.}
\label{fig:broad_listening}
\end{figure}

Yet these challenges also reveal an opportunity. By reformulating how AI systems process information (shifting from operations that break ABC to ones that preserve it) society is actively overcoming the perception of AI as a tool of central intelligence, revealing it to be a communication paradigm of great magnitude. And as described above, this change will not just solve technical problems; it could realign AI development with democratic values. The path to this transformation begins with the mathematical operations which preserve or destroy ABC in AI.

\section{Underlying Causes and Contributing Factors}


\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{1411.3146/abc_7_v7.png}
\caption{ABC is underpinned by the addition, branching, and copy problems}
\label{fig:broad_listening}
\end{figure}

\subsection{Cause: Attribution and The Addition Problem}

When a person concatenates two numbers together, they preserve the integrity of each contributing piece of information. However, when a person adds two numbers together, they produce a result which loses knowledge of the original values, and thus also loses the ability to precisely remove one of those values later. Consider the following example:

\begin{align*}
\text{Concatenation preserves} & \\
\text{sources and enables removal:} & \\
\text{"1"} \oplus \text{"6"} & = \text{"16"} \\
\text{"2"} \oplus \text{"5"} & = \text{"25"} \text{ (can identify and remove either "2" or "5")}\\
\\
\text{Addition erases them:} & \\
1 + 6 & = 7 \\
2 + 5 & = 7 \text{ (cannot identify and then remove either input)}
\end{align*}

In the example above, receiving the number ``7'' provides no information on what numbers were used to create it, while receiving the number ``16'' preserves the numbers used to create it. 

In the same way, when a deep learning AI model is trained, each datapoint is translated into a "gradient update" which is \textit{added into the previous weights of the model}, losing the ability to know which aspects of each weight were derived from which datapoints and thus the ability to later remove specific datapoints' influence \cite{Goodfellow-et-al-2016}. Thus, despite recent work, when an AI model makes a prediction, no-one can verify which source datapoints are informing that prediction (i.e. influence functions), nor can they cleanly remove problematic datapoints' influence (i.e. "unlearning") without retraining the entire model. Despite attempts, influence function based unlearning remains an unsolved challenge \cite{nguyen2024surveymachineunlearning}, blocking an ABC solution...  because addition fundamentally destroys information which cannot then be recovered. Addition blocks attribution, which blocks attribution-based control in AI systems.



\subsection{Cause: Control and The Copy Problem}
When a person makes a copy of their information and gives it to another, they lose the ability to enforce how that information might be used \cite{st}. And upon this core flaw, even if AI attribution was solved (e.g. unlearning and influence functions were solved), certain dominoes would still fall and avert ABC within AI systems.

To begin, consider that when training data providers give a copy of their training data to an AI organization, they lose the ability to control how that data might be used,  thereby losing any enforceable control over how the resulting model might be used \footnote{or how it might be value aligned}. Similarly, if one gives an AI model to someone else, the former loses the ability to control how that model would be used. Taken together, because of the overuse of copying, AI is unilaterally controlled by whomever has a copy of the trained model. At a minimum, this always includes the organization responsible for its training (i.e. "closed source AI"). In the maximum, this includes any person who downloads a model from the internet (i.e., "open source AI"). 

While many debate the merits of so-called "closed source" or "open source" AI models, the copy problem underpins a constructive criticism of both: both are systems of unilateral control because of the over-use of copying. And the over-use of copying doesn't just create systems which \textit{can} be unilaterally controlled, it creates systems which \textit{must be unilaterally controlled}... systems which explicitly prohibit collective control. Taken together, addition blocks attribution, copying blocks collective control,and together these foundational flaws block attribution-based control in AI systems.

\subsection{Cause: Bi-directional Delegation and The Branching Problem}

When a person needs to synthesize information from a small number of sources, they can often leverage their existing trust networks (people, brands, and various groups or institutions they are familiar with) to obtain high-veracity information they trust. Similarly, when a person needs to broadcast information to a small number of recipients, they can often evaluate each recipient's trustworthiness before sharing, and in-so-doing protect their privacy, further their interests, and broadly avert what they might consider to be information mis-use. However, when a person needs to synthesize information from billions of sources or broadcast to billions of recipients, they necessarily cannot evaluate each party individually \cite{dunbar1993coevolution}; they are forced to delegate evaluation to intermediaries who scale trust evaluation on their behalf. And upon this core constraint, even if AI attribution was solved (e.g., unlearning and influence functions were solved) and collective control was solved (e.g. the copy problem), certain dominoes would still fall and avert ABC within AI systems.

To begin, consider that when sources contribute training data to high-branching AI systems (where billions of sources branch into a single model), they lose the ability to evaluate which of billions of users will leverage their contribution... thereby losing any enforceable control over whether those users are trustworthy or align with their values. Similarly, when users query high-branching AI systems (where a single model branches out to billions of users), they lose the ability to evaluate which of billions of sources inform their predictions... thereby losing any enforceable control over whether those sources are reliable, independent, or free from conflicts of interest. Taken together, because of the overuse of high-branching aggregation, AI concentrates source and user selection at whichever entities facilitate high-branching operations: the platforms connecting billions of sources to billions of users \cite{2f928592-a19d-38f4-91e4-45f12ea471a0, burt2003social}.

While platforms may attempt to perform this selection faithfully, the branching problem underpins a structural limitation: high-branching operations mathematically necessitate centralized selection authority at information bottlenecks \cite{freeman1977set}. And the overuse of high-branching aggregation doesn't just create systems which \textit{can} have centralized selection, it creates systems which \textit{must} have centralized selection, systems which explicitly prohibit distributed bilateral authority over source and user selection. Taken together, addition blocks attribution, copying blocks collective control, and high-branching blocks distributed selection. Tkaen together, addition, branching, and copying avert attribution-based control in AI systems.

\subsection{Contributing Factors: Consensus and the Natural Problem}

Yet technical barriers alone may not explain ABC's absence. Perhaps surprisingly, this thesis will describe how the technical ingredients for an ABC solution already operate at scale in specific AI contexts. This disconnect between possibility and adoption demands explanation. 

Consider the incentives: democratic nations seek to compete with authoritarian AI programs, data owners want to maximize revenue, and societies aim to enhance AI capabilities while ensuring those capabilities benefit society broadly. ABC could advance all these goals, and, as this thesis surveys, local incentives have inspired local solutions to emerge and become tested at scale. Yet ABC remains conspicuously absent from mainstream AI development. Why?

While a complete sociological analysis is beyond the technical scope of this thesis, examining current AI development reveals a striking pattern: researchers consistently draw inspiration from a specific conception of biological cognition \cite{Goodfellow-et-al-2016, russell1995modern, lecun2015deep}, cognition that provides no attribution-based control over its predictions. For example, humans do not ask permission from their parents, friends, co-workers, or evolutionary ancestors every time they seek to use a concept taught to them by these external parties (i.e. every time they open their mouth or move their body... all actions learned at some point from another party). For those who see AI development as fundamentally about imitating this meme of biological cognition, ABC represents a deviation from nature's blueprint. \footnote{However, there are other conceptions of natural intellignece which are compatible with this vision, such as collective or networked intelligence, although they have somewhat fallen out of favor since the peak of theories like cybernetics. It remains to be seen whether the popular meme of intelligence will expand to a more networked form.}. 

Moreover, ABC may threaten established business models. Large AI companies have built multi-billion (soon, multi-trillion?) dollar valuations in part on their ability to use training data without compensating providers \cite{berger2025ai}, and to bend the behavior of AI models in a way that suits them. For example, the targeted advertising industry, projected to reach over a trillion dollars by 2030 \cite{statista2025advertising}, relies heavily on AI systems that obscure rather than expose the sources influencing their recommendations \cite{bernays1928propaganda}, and enable the AI owner to sell influence over the model to the highest bidder (e.g. ads in newsfeed algorithms) \cite{adikari2015real}. An AI deployment with true ABC might disrupt both of these profitable arrangements, arrangements which are the primary business models of several of the largest AI developers.

This relationship to ABC reflects a deeper tension in AI development: the choice between transparency and mystique. In the case that ABC provides precise, controllable, and attributable intelligence, many influential voices may prefer the allure and profitability of black-box systems that more closely mirror a specific conception of biological cognition. Yet, this dichotomy does not just shape technical decisions, but who retains power over AI, and the future of AI's role in society. As readers proceed through this thesis, they might consider: what if this preference for black-box AI systems stems not from technical necessity, from safety maximization, or from economic/political incentive, but from deeply held assumptions about what artificial intelligence should be? For now, the thesis will overlook such concerns, returning to them in closing notes.

\section{Thesis Outline}

\\
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{1411.3146/abc_8_v4.png}
\caption{The addition, branching, and copy problems addressed in chapters 2, 3, and 4 through the use of concatenation, encryption, and recursion to provide attribution-based control.}
\label{fig:broad_listening}
\end{figure}
\\

Across three chapters, this thesis progressively constructs a new paradigm for AI development and deployment by describing how breakthroughs in deep learning, distributed systems, and cryptography might: avert the addition, branching, and copy problems, provide a means for AI systems with attribution-based control, and better align AI with free and democratic society.

\subsection{Chapter 2: Addition — From Deep Learning to Deep Voting} 

The thesis begins at the mathematical center of modern AI, artificial neural networks. Chapter 2 reveals how reliance on addition operations in deep learning averts attribution and promotes centralized control into AI systems.

\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{1411.3146/deep_learning_to_deep_voting_v2.png}
\caption{Traditional open/closed-source deep learning systems (left) pool all data into a deep learning model (i.e. by adding weight updates) which is later used for predictions, while deep voting systems (right) learn source-specific model parameters which remain partitioned until prediction time, at which point they can be rapidly synthesized to make a prediction. In such rapid synthesis, attribution metadata flows (concatenated) with predictions, enabling source verification. Darker lines indicate information flow being used for training or prediction. Lighter lines indicate data sources which could have participated in the prediction, but were excluded.}
\label{fig:broad_listening}
\end{figure}

Chapter 2 surveys recent developments in sparse deep learning architectures and cryptographic protocols that displace some addition operations with concatenation operations. The chapter describes how these architectural modifications preserve mappings between training examples and their influence on specific predictions, a property that standard neural networks lose through parameter addition and dense inference. The chapter formalizes these architectures as \textit{deep voting}: ensemble methods where model components trained on disjoint data subsets (and which remain in a concatenated state after training) only merge through addition during inference in a more linear fashion, and perform that merge in a way which is bounded by \textit{intelligence budgets}. In this way, it describes how recent work in deep learning sparsity and cryptography can be combined to control source-prediction influence in deep learning systems. 

The chapter then documents empirical results demonstrating that deep voting architectures achieve comparable accuracy to standard deep learning while maintaining per-source attribution. This property enables two capabilities unavailable in addition-based architectures: users can specify weights over training sources after training (dynamic source selection), and sources can withdraw their contributions from specific predictions (selective unlearning).  These results establish that AI \textit{attribution} (maintaining verifiable mappings from predictions back to contributing training sources) is architecturally feasible at scale, addressing a necessary requirement for attribution-based control in AI systems.

\newpage
\subsection{Chapter 3: Copying — From Deep Voting to Network-source AI}

While Chapter 2 synthesizes techniques for AI with attribution, it surfaces a deeper challenge: how can we enable selective sharing of information without losing control through copying? Chapter 3 surveys recent cryptographic breakthroughs and reframes them in an end-to-end framework: \textit{structured transparency}, describing how combinations of cryptographic algorithms can allow data owners to collectively enable some uses of their data without enabling other uses.

\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{1411.3146/deep_voting_to_network_source_ai_v2.png}
\caption{Deep voting systems (left) require the copying of data from source to the holder of an AI model, while network-source AI systems (right) leverage cryptography and distributed systems to enable data holders to retain control over the only copy of their information...  and the predictive capability that information lends to an AI system. In network-source AI, AI users directly query vast numbers of data owners requesting predictive capability for a single prediction, learning only the output of their prediction in the process. Darker lines indicate sources being used for a prediction.}
\label{fig:broad_listening}
\end{figure}

When applied to the attribution-preserving networks from Chapter 2, structured transparency enables a new kind of AI system, one where predictive power flows freely through society without requiring centralized data collection. Data owners can contribute to collective intelligence while maintaining strict control over which predictions their information is used to support, seemingly fulfilling the technical requirements for attribution-based control (ABC) in AI systems.

\subsection{Chapter 4: Branching — From Network-Src AI to Broad Listening}

Chapters 2 and 3 provide the technical foundation for ABC in AI, enabling data sources to calibrate who they wish to imbue with predictive power, and enabling AI users to calibrate who they trust to provide them with value-aligned predictive power. However, given the billions of people in the world, a new problem arises: how can data sources or AI users know who to trust?

\subsubsection{The Scale Problem}

ABC requires distributed authority over billions of potential sources, but humans can sustain high-trust relationships with approximately 150 entities (i.e., Dunbar's Number). This creates a gap of seven orders of magnitude between what ABC requires ($\sim10^9$ sources) and what individuals can evaluate ($\sim10^2$ relationships). Delegation becomes architecturally necessary, but which delegation architecture preserves ABC's requirement for distributed authority?
\vspace{10px}
\begin{tcolorbox}[
enhanced,
breakable,
colback=white,
colframe=gray,
boxrule=0.5pt,
left=8pt,
right=8pt,
top=8pt,
bottom=8pt,
title=The Dunbar Gap]
Consider your close friends (people you'd call if you needed help moving or advise about a major life decision). Most people sustain perhaps a dozen such relationships. Expand to include colleagues you genuinely know (not just recognize), neighbors you actually talk to, family you stay in touch with, (etc.). Dunbar suggests this number caps around 150.
\\
\\
Now consider what ABC requires: evaluating which of billions of potential sources to trust, which queries to allow, or which contexts are appropriate. The gap isn't close, its seven orders of magnitude off. It's not like needing to manage 200 relationships when you can handle 150. It's needing to manage \textit{billions} when you can handle \textit{hundreds}.
\\
\\
Asking individuals to directly evaluate trust at ABC's required scale is like asking someone to maintain close friendships with every person in China (not theoretically impossible, just absurdly incompatible with how human relationships actually work).
\end{tcolorbox}

\subsubsection{1st Criterion: Maximizing Breadth}

To address this scale problem while preserving ABC's distributed authority, Chapter 4 identifies two criteria that any delegation architecture must satisfy. The first is the \textit{breadth criterion}. Truth-finding requires aggregating from many independent sources because coordinated deception becomes logistically harder when information must be consistent across many independently-weighted witnesses. However, filtering through centralized bottlenecks reduces this advantage. If all sources must pass through a single institution, coordination difficulty decreases since fewer parties must align their story to coordinate a compelling deception.

\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{1411.3146/breadth_assumption_v2.png}
\caption{Coordinating deception becomes harder as the number of independent sources increases, but only when those sources can be independently weighted without passing through centralized bottlenecks. Aggregating from few sources (left) reduces coordination difficulty, while aggregating from many independent sources (right) makes coordination harder to achieve undetected. Darker lines indicate sources being used for a prediction.}
\label{fig:breadth_criterion}
\end{figure}

\subsubsection{2nd Criterion: Maximizing Depth}

The second criterion is the \textit{depth criterion}. Trust evaluation requires sustained relationships that enable: observing behavior over repeated interactions, understanding context to verify independence, experiencing consequences through social closure, and maintaining accountability through ongoing reciprocity. This cannot be achieved through weak ties or at web-scale per evaluator; trust assessment requires strong ties maintained within bounded relationship capacity.

\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{1411.3146/depth_assumption_v2.png}
\caption{Trust evaluation requires sustained relationships that enable observation over time, verification of independence, and reputational consequences through social closure. Aggregating from untrusted relationships (left) reduces information authenticity relative to aggregating from trusted relationships (right), because relationships create mutual stakes... co-dependence on truthful information exchange now and in the future. Darker lines indicate sources being used for a prediction. Line sparsity indicates trust level.}
\label{fig:depth_criterion}
\end{figure}

\subsubsection{The Conflict Between Breadth/Depth and Dunbar's Number}
These criteria are in direct conflict. The breadth criterion would be maximized by aggregating from billions of sources to make coordination difficult, while the depth criterion would be maximized by only trusting parties with whom one has spent great deals of time developing a high-trust relationship (approximately 150 sustained relationships according to Dunbar). 

One might surmise that the only possible solution to this conflict is delegation, to maintain 150 strong relationships with parties who possess the scale to create relationships with billions of entities in the world: web-scale institutions. However, such a high-branching social graph (high branching in two places, as such institutions accept information from billions of sources and provide services to billions of recipients) violates the definition of attribution-based control.

When individuals connect to institutions and institutions connect to billions of sources, all information flow passes through institutional nodes that occupy structural holes... positions controlling information flow between otherwise disconnected parties. This bottleneck structure violates the depth criterion through multiple mechanisms: monitoring costs prevent institutional evaluation of independence across billions of source pairs, relationships between institutions themselves and their sources become weak ties at scale (e.g., problems within Trust and Safety in online platforms, Sybil attacks, and Spam) as fixed institutional resources divide across billions of sources, social closure disappears due to negligible network overlap between sources at billion scale, and institutions necessarily control source selection from their bottleneck positions. The architectural bottleneck thus concentrates authority over source selection at institutional nodes, violating ABC's requirement for distributed authority regardless of institutional intentions or policies. Taken together, high-branching social institutions are unfit to offer ABC in AI systems.

\subsubsection{The Solution: Low-Branching Recursive Delegation}

\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{1411.3146/scale_assumption.png}
\caption{High-branching delegation (left) connects directly to many sources but creates bottlenecks and exceeds evaluation capacity. Low-branching recursion (right) achieves exponential reach through depth while preserving evaluation capacity at each hop. Each person maintains $\sim$50 verifiable connections; over 6 hops this reaches $50^6 = 15.6$ billion sources. Darker lines indicate sources being used for a prediction. The architecture leverages small-world network properties where people are connected through short paths.}
\label{fig:scale_solution}
\end{figure}

Chapter 4 proposes low-branching recursive delegation as an alternative architecture that satisfies both criteria by achieving exponential reach, propagating trust recursively over $H$ hops. For example, if $H=6$ (e.g. six degrees of separation), this approach would achieve exponential reach of $50^6 = 15.6$ billion sources.

\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{1411.3146/network_source_ai_to_broad_listening_v2.png}
\caption{Network-source AI (left) sources information directly from eye-witnesses while broad listening (right) leverages information from the same sources weighted through a social graph. Darker lines indicate sources being used for a prediction. Dotted sparsity indicates trust.}
\label{fig:network_to_broad}
\end{figure}

This pattern has precedent in systems achieving billion-scale reach with distributed authority: PageRank (pages link to $\sim$10-100 pages, authority distributes to authors), academic citations (papers cite $\sim$10-50 sources, authority distributes to researchers), PGP web of trust (users sign $\sim$10-50 keys, authority distributes to users), and Wikipedia (articles cite $\sim$10-50 sources, authority distributes to editors).

\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{1411.3146/deep_learning_to_broad_listening_v2.png}
\caption{Traditional AI (left) compresses all sources into a model, obfuscating source influence and control while broad listening (right) leverages information from relevant sources, weighted through a social graph. Darker lines indicate sources being used for a prediction.}
\label{fig:traditional_to_broad}
\end{figure}

This massive path redundancy eliminates bottlenecks. No single party controls access to sources, and users can route around failing or captured intermediaries through alternative verified paths. The architecture simultaneously satisfies the depth criterion by constraining each participant to approximately 50 connections ($k = 50 < 150$), preserving capacity for strong ties with sustained evaluation, verification of independence across manageable pairs ($\binom{50}{2} = 1{,}225$ pairs), and social closure through network overlap. 

Because verification occurs at each hop rather than concentrating at institutional nodes, the quality of trust evaluation remains high throughout the recursive chain. Each person evaluates their 50 connections through sustained relationships, and those connections evaluate their 50 through sustained relationships, creating verified paths from users to billions of ultimate sources.
% \vspace{10px}
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{1411.3146/abc_final_v4.png}
\caption{Solutions in addition, branching, and copy problems propagate to societal problems.}
\label{fig:abc_final}
\end{figure}
Chapter 4 formalizes this architecture for ABC by integrating recursive trust propagation with intelligence budgets from Chapter 2 and privacy-preserving structured transparency from Chapter 3. The result: each person can synthesize information from billions of sources, weighted through local trust relationships, and verified at each hop, transforming network-source AI's raw listening capacity into trust-weighted "word-of-mouth at global scale." Taken together, they provide a viable path towards attribution-based control in AI systems.



\subsection{Chapter 5: Conclusion — AI as a Communication Tool}

% or perhaps "from broadcasting to broadlistening"

Chapter 5 returns to examine how the technical developments in Chapters 2-4 address the cascading consequences of missing ABC laid out in this introduction. It demonstrates how concatenation-based networks solve the addition problem by preserving source attribution, how encrypted computation solves the copy problem by enabling sharing without loss of control, and how recursion solves the branching problem by enabling a digital word-of-mouth at scale through AI systems.

The chapter then analyzes how these capabilities fundamentally transform the nature of artificial intelligence. When AI relies on addition, copying, and branching it necessarily becomes a tool of central control, forcing us to trust whoever possesses a copy of the model. But as AI comes to preserve ABC, it enables something radically different: a communication tool that connects those who have knowledge with those who seek insights. Beyond a technical shift, ABC actively re-frames AI from a system that produces intelligence through centralization into a system that enables intelligence to flow freely through society.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.85\textwidth]{1411.3146/broad_listening_global.png}
\caption{A fictional illustration of the premise of attribution-based control enabling \textit{broad listening} predictions which traverse the entire free market's data and compute — as opposed to merely the data and compute which can be centralized by a single organization or nation.}
\label{fig:broad_listening}
\end{figure}

\newpage 
The chapter concludes by arguing that democratic societies face an architectural choice that will determine the future of AI: continue building systems that mathematically require central control, or embrace AI architectures that enable broad listening. It demonstrates why broad listening uniquely addresses the individual problems of hallucination and disinformation through source verification, the institutional problems of data sharing through aligned contributor incentives, the societal problems of governance through continuous public consent, and the geopolitical challenge of competing with authoritarian AI by harnessing the collective resources of the free market. Taken together, the chapter argues that ABC isn't just technically possible, it's essential for transforming AI from a technology that advantages central control into one that advantages democratic values. The future of AI need not be a race to centralize. Through broad listening, it can become a race to connect through AI systems with attribution-based control.


\chapter{From Deep Learning to Deep Voting}

\\
\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{1411.3146/abc_ch2_2_v8.png}
\caption{The lack of ABC creates problems for data-owning institutions, problems which avert their sharing of data and compute for AI training, problems which are underpinned — at their core — by the overuse of addition within deep learning systems.}
\label{fig:broad_listening}
\end{figure}
\\

\section{Chapter Summary}

Estimates below suggest that models are trained using less than 1/1,000,000th of the world's data and AI compute productivity. Consequently, following AI's scaling laws \cite{scaling_laws_2020, hoffmann2022trainingcomputeoptimallargelanguage}, AI models possess capabilities which are insignificant compared to what existing data, compute, and algorithms could create. 

Yet, if AI models (and their capabilities) are the lifeblood of the AI industry, why are data and compute so underutilized? Why is AI capability so constrained? This chapter unpacks the cause of such a drastic resource under-utilization. It begins by linking resource utilization to attribution-based control (ABC). It then breaks attribution-based control into problems with attribution and control, which are themselves underpinned by deep learning's core philosophy of mixing dense features. This mixing is only problematic because of a specific technical choice: the use of addition to update model weights, which erases provenance information during gradient descent.

\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{1411.3146/deep_learning_to_deep_voting_v3.png}
\caption{Traditional open/closed-source deep learning systems (left) pool all data into a deep learning model (i.e. by adding weight updates) which is later used for predictions, while deep voting systems (right) learn weight parameters which remain partitioned by source (i.e. concatenated), but which are learned in a way that they can be rapidly synthesized on the fly. Partitions (pictured above as pie slices) are rapidly synthesized to form a model used for a specific prediction. Darker lines/slices indicate information being used for a particular prediction. Lighter lines/slices indicate information not being used for a particular prediction. The circle on top of the slices represents a dense model capable of rapidly synthesizing slices in practice. The combination of concatenated weights and concatenated metadata with inference predictions (which tracks the synthesis and subsequent use of slices for an AI prediction) enables ABC.}
\label{fig:broad_listening}
\end{figure}

The chapter then explores alternatives to addition during the training process, revealing a fundamental trade off between three factors: AI capability (driven by unrestricted feature learning), attribution (tracking where features came from), and computational complexity (tracking the path of feature mixing). It proposes a key innovation, a re-purposing of differential privacy for attribution: \textit{differential attribution}, using the natural boundaries of training documents to identify which concepts must mix freely and vice versa, thereby pushing this Pareto frontier by providing a data-driven approach to balance addition and concatenation.

Building on this insight, the chapter develops a specific form of concatenation to replace addition in key sections of the deep learning training process. This transformation — from deep learning to \textbf{deep voting} — cascades upward through the aformentioned hierarchy of problems, reducing the need for dense feature mixing across data sources, enabling attribution-based control, and unlocking a viable path towards another 6+ orders of magnitude of training data and compute productivity. Taken together, the chapter reveals how a seemingly technical choice (the use of addition) creates far-reaching consequences for AI systems, and how careful, data-driven use of concatenation may dramatically expand AI's access to computational and data resources.

\section{The Symptom: Data/Compute Underutilization}
\label{symptom}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{1411.3146/abc_ch2_2_v9.png}
\caption{The lack of ABC creates problems for data-owning institutions, problems which avert their sharing of data and compute for AI training, problems which are underpinned by the overuse of addition within deep learning systems.}
\label{fig:broad_listening}
\end{figure}
\\

As of NeurIPS 2024, leading AI researchers have reported that available compute and data reserves are approaching saturation, creating constraints on both computational resources and pre-training scale \cite{robison2024openai, gpu_shortage}. However, this assessment overlooks approximately six orders of magnitude of underutilized compute productivity and siloed data. Rather than absolute scarcity, the industry faces structural problems of data and compute access and productivity.

\subsection{6+ OOM: Underutilized Training Compute Productivity}
\label{compute_prod}

The AI industry's computational requirements have driven significant economic and geopolitical consequences, including NVIDIA's rise to become the world's most valuable company, U.S. export restrictions on AI chips to China, and intense competition for latest-generation hardware among startups, enterprises, and major technology firms \cite{Kaye2025NvidiaFirstCompany, kachwala2024nvidia, howley2023nvidia}. However, recent evidence suggests that current AI training and inference processes utilize less than 0.0002\% of available compute productivity, indicating that perceived compute scarcity may reflect inefficiency rather than absolute resource limits. To evaluate this claim, this section estimates computational waste in two key activities: inference (forward propagation) and learning (backpropagation and gradient descent).

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{abc_ch2_2_1_v8.png}
\caption{A tree of sub-problems regarding inefficient training compute productivity.}
\label{fig:broad_listening}
\end{figure}

\subsubsection{2-3 OOM: Inefficient AI inference}

Due to its role in commercial deployments, analysts estimate that AI firms spend billions of dollars annually on inference \cite{epoch2025openaicomputespend}. However, while these costs might appear to reflect fundamental requirements for achieving high performance, a growing body of empirical work indicates substantial inefficiency in current inference practices.

\vspace{0.4cm}

\begin{tcolorbox}[
enhanced,
breakable,
colback=white,
colframe=gray,
boxrule=0.5pt,
left=8pt,
right=8pt,
top=8pt,
bottom=8pt,
title=A Library Analogy]
Consider a library. When someone asks a librarian about the rules of chess, the librarian doesn't subsequently read \textit{every book in the library} to find the answer. Instead, they use the catalog system to find a relevant bookshelf, the titles of books on that shelf to find the relevant book, and the table of contents of that book to find the relevant section. This practice stands in stark contrast to how AI systems process information. To make an AI prediction with a model like GPT-3, AI users forward propagate \textit{through the entire model and all of its knowledge} (i.e., read every book in the library). And in the case of large language models, they don't just do this once per answer, they do this \textit{for every token they predict}. This is analogous to a librarian reading every book in the library every time they utter a word. Given how implausible it is that any prediction requires \textit{the entirety of an AI models knowledge}, AI's full, dense inference is a staggering inefficiency.
\end{tcolorbox}

AI models store information within their weights. General-purpose models (e.g., Gemini, ChatGPT, Claude, Llama) encode substantial portions of their training corpora (often representing significant fractions of publicly available internet data) in these parameters. However, when models like GPT-3 generate predictions, they forward propagate through every non-embedding parameter in the network, regardless of query relevance. This constitutes a form of exhaustive computation wherein all stored knowledge is activated for each inference, analogous to searching an entire corpus rather than querying relevant subsets.

From an information-theoretic perspective, this practice is inefficient; the relevant question concerns the magnitude of this inefficiency. A comprehensive answer would require empirically measuring the maximum percentage of model weights that can be excluded from inference without degrading accuracy. While such systematic measurement remains incomplete, existing work provides lower bounds on potential efficiency gains.

DeepMind's RETRO achieves comparable performance to GPT-3 while using 1/25th of the parameters through retrieval from a large-scale vector database \cite{borgeaud2022improving}. Similarly, Meta's ATLAS demonstrates that models can be reduced to 1/50th their original size while maintaining or exceeding baseline performance through database-augmented inference \cite{izacard2023atlas}.

We adopt RETRO/ATLAS-style parameter efficiency as a conservative lower bound on current compute waste, noting that these approaches have not been widely adopted in either the sparsity literature \cite{lederer2024statistical} or frontier AI deployments, nor have comparable efficiency gains been demonstrated through alternative methods (cf. the persistent redundancy problem in Mixture of Experts models \cite{dai2024deepseekmoe}). These results suggest that at least 96-98\% of parameters activated during dense inference are unnecessary for individual queries.

This estimate is likely conservative, as it implies that 2-4\% of a model's knowledge base is relevant to any individual query.\footnote{While this fraction seems implausibly large for most queries, systematic measurement of query-specific parameter relevance remains limited in the literature. We therefore retain this conservative 2-4\% estimate.} However, parameter overuse during forward propagation represents only one source of computational waste. A second form of inefficiency arises in how models store and access information.
\vspace{0.25cm}
\begin{tcolorbox}[
enhanced,
breakable,
colback=white,
colframe=gray,
boxrule=0.5pt,
left=8pt,
right=8pt,
top=8pt,
bottom=8pt,
title=A Library Analogy]
Consider a library once again. When someone asks a librarian about the rules of chess, and the librarian goes to fetch a particular book, the librarian doesn't bring back \textit{every copy of the book in the library}. And, as unintuitive as this might seem, the librarian also doesn't bring back \textit{empty books from random assortments of shelves}. Instead, they use the catalog system to find a relevant bookshelf, the titles of books on that shelf to find the relevant book, and then they select a single book for the library's customer.
\\
\\
This practice stands in stark contrast to how AI systems process information. To make an AI prediction within a model like GPT-3, AI users don't merely forward propagate \textit{through the entire model and all of its knowledge} (i.e., read every book in the library), AI users must forward propagate through some mixture of \textit{multiple copies of the same information} (i.e. multiple copies of the same book) as well as \textit{empty vector space} (i.e. empty books) in order to create an output.
\end{tcolorbox}
\vspace{0.25cm}
\newpage
Recent work demonstrates that current architectures contain redundant and underutilized parameters. Guo et al. achieve 5-10x reduction in parameter count through lossless compression while maintaining accuracy: "Notably, we distill CIFAR-10 and CIFAR-100 to 1/5 and Tiny ImageNet to 1/10 of their original sizes without any performance loss on ConvNet, offering the first lossless method of dataset distillation" \cite{guo2023towards}. This compression has been demonstrated across multiple standard architectures, as shown in Table \ref{tab: main comparison}.

\begin{table*}[h]
\newcommand{\chart}{\includegraphics[align=c,height=2ex]{figures/chart_down.png}}
% \setemojifont{}
\centering

\footnotesize
\renewcommand{\arraystretch}{1.1}
\resizebox{1\linewidth}{!}{
\begin{tabular}{c|ccccc|cccc|ccc}
\toprule
Dataset      & \multicolumn{5}{c|}{CIFAR-10} & \multicolumn{4}{c|}{CIFAR-100} & \multicolumn{3}{c}{Tiny ImageNet}       \\
IPC          & 1     & 10  & 50 & 500 & 1000 & 1      & 10    & 50    & 100   & 1        & 10      & 50      \\
Ratio        & 0.02  & 0.2 & 1  & 10  & 20   & 0.2    & 2     & 10    & 20    & 0.2      & 2       & 10      \\
\midrule
Random       & 15.4±0.3 & 31.0±0.5 & 50.6±0.3 & 73.2±0.3 & 78.4±0.2 & 4.2±0.3 & 14.6±0.5 & 33.4±0.4 & 42.8±0.3 & 1.4±0.1 & 5.0±0.2 & 15.0±0.4 \\
DC           & 28.3±0.5 & 44.9±0.5 & 53.9±0.5 & 72.1±0.4 & 76.6±0.3 & 12.8±0.3 & 25.2±0.3 & - & - & - & - & - \\
DM           & 26.0±0.8 & 48.9±0.6 & 63.0±0.4 & 75.1±0.3 & 78.8±0.1 & 11.4±0.3 & 29.7±0.3 & 43.6±0.4 & - & 3.9±0.2 & 12.9±0.4 & 24.1±0.3 \\
DSA          & 28.8±0.7 & 52.1±0.5 & 60.6±0.5 & 73.6±0.3 & 78.7±0.3 & 13.9±0.3 & 32.3±0.3 & 42.8±0.4 & - & - & - & - \\
CAFE         & 30.3±1.1 & 46.3±0.6 & 55.5±0.6 & - & - & 12.9±0.3 & 27.8±0.3 & 37.9±0.3 & - & - & - & - \\
\hspace{2mm}KIP$^{1}$          & 49.9±0.2 & 62.7±0.3 & 68.6±0.2 & - & - & 15.7±0.2 & 28.3±0.1 & - & - & - & - & - \\
\hspace{2mm}FRePo$^{1}$          & 46.8±0.7 & 65.5±0.4 & 71.7±0.2 & - & - & 28.7±0.1 & 42.5±0.2 & 44.3±0.2 & - & 15.4±0.3 & 25.4±0.2 & - \\
\hspace{2mm}RCIG$^{1}$          & \underline{53.9±1.0} & \underline{69.1±0.4} & 73.5±0.3 & - & - & \underline{39.3±0.4} & 44.1±0.4 & 46.7±0.3 & - & \underline{25.6±0.3} & 29.4±0.2 & - \\
\hspace{2mm}MTT$^{2}$           & 46.2±0.8 & 65.4±0.7 & 71.6±0.2 & \chart{} & \chart{} & 24.3±0.3 & 39.7±0.4 & 47.7±0.2 & 49.2±0.4 & 8.8±0.3 & 23.2±0.2 & 28.0±0.3 \\
\hspace{2mm}TESLA$^{2}$         & \textbf{48.5±0.8} & 66.4±0.8 & 72.6±0.7 & \multicolumn{1}{c}{\chart{}} & \chart{} & 24.8±0.4 & 41.7±0.3 & 47.9±0.3 & 49.2±0.4 & - & - & - \\
\hspace{3mm}FTD$^{2,3}$          & 46.0±0.4 & 65.3±0.4 & 73.2±0.2 & \chart{} & \chart{} & 24.4±0.4 & 42.5±0.2 & 48.5±0.3 & 49.7±0.4 & 10.5±0.2 & 23.4±0.3 & 28.2±0.4 \\
\textbf{DATM} (Ours)        & 46.9±0.5 & \textbf{66.8±0.2} & \textbf{76.1±0.3} & \textbf{83.5±0.2} & \cellcolor[HTML]{DAE8FC}\textbf{85.5±0.4} & \textbf{27.9±0.2} & \textbf{47.2±0.4} & \textbf{55.0±0.2} &\cellcolor[HTML]{DAE8FC}\textbf{57.5±0.2} & \textbf{17.1±0.3} & \textbf{31.1±0.3} & \cellcolor[HTML]{DAE8FC}\textbf{39.7±0.3} \\ \midrule
Full Dataset & \multicolumn{5}{c|}{84.8±0.1} & \multicolumn{4}{c|}{56.2±0.3}  & \multicolumn{3}{c}{37.6±0.4} \\ \bottomrule
\end{tabular}
}
\vspace{-1mm}
\caption{Table and caption from \cite{guo2023towards} (see paper for additional detailed descriptions). "Comparison with previous dataset distillation methods on CIFAR-10, CIFAR-100 and Tiny ImageNet... \colorbox[HTML]{DAE8FC}{Highlighted} results indicate we achieve lossless distillation." - \cite{guo2023towards}
}
\
\label{tab: main comparison}
\end{table*}
\vspace{-0.5cm}

Because they account for information waste in different ways, these inefficiencies compound multiplicatively: irrelevant parameters (25-50x+) and redundant parameters (5-10x+) suggest a 125-500x+ lower-bound to inference inefficiency. And while these are estimates, both bounds come from working implementations that maintain model performance, using techniques which are not widely popular, suggesting this waste is common in frontier AI systems, and that this waste stems from architectural choices rather than fundamental limitations.

\subsubsection{6 OOM: Underutilized and Inefficient Compute in AI Learning}

AI firms famously spend immense amounts of money training their AI models, a point which features heavily in their marketing \cite{meta2024llama, brown2020language, wiggers2024openai}. However, despite widespread hype around AI training spend, a theoretical inefficiency is backed by an increasingly large body of empirical observations, suggesting that the compute requirements for training AI models are largely misunderstood. To introduce the theory, consider an analogy.

\vspace{0.5cm}
\begin{tcolorbox}[
enhanced,
breakable,
colback=white,
colframe=gray,
boxrule=0.5pt,
left=8pt,
right=8pt,
top=8pt,
bottom=8pt,
title=A Library Analogy]
As before, consider a library. When a library adds or removes a significant number of books to/from their collection, they don't \textit{rebuild the entire building and re-print all of their books from scratch}, they simply add/remove books, shelves, or rooms. 
These practices stand in stark contrast to how AI systems process information. To add or remove a significant portion of knowledge from a deep learning system, AI researchers \textit{retrain them from scratch} (i.e., tear down the entire library, burn all the books, rebuild the library, and re-print all the books from scratch). Despite being a widespread, even ubiquitous practice within AI research, this practice is staggeringly inefficient. It re-characterizes the claims of compute scarcity in an entirely new light. It is like a librarian who repeatedly tears down their library, and re-prints all their books — lamenting insufficient bricks, paper, or ink.
\end{tcolorbox}
% \vspace{0.5cm}
AI models store information within their weights. To acquire this information, models are trained on large corpora at substantial computational cost (e.g., significant portions of the public internet) \cite{EpochAIModels2025}. However, when models require substantial updates (either incorporating new information or removing outdated content) current practice involves retraining from scratch \cite{Goodfellow-et-al-2016}. This approach discards all previously computed parameters and repeats the entire training process, even when the majority of learned representations remain valid.

From an information-theoretic perspective, this practice is inefficient; the question concerns its magnitude. Comprehensive quantification would require detailed documentation of compute allocation within leading AI firms, information that is not publicly available. However, public disclosures and industry analysis provide sufficient data to establish lower bounds on this inefficiency.

Analysis of the largest AI firms reveals that pre-training their most capable models consumes less than 1\% of quarterly compute budgets (see Table \ref{tab:model-compute-summary}; methodology detailed in Appendices I and II). Yet these same firms continue expanding computational infrastructure to support larger models \cite{mehta2024zuckerberg, openai2024learning, sevilla2024training}, suggesting that remaining compute capacity is allocated to other training activities rather than final model production.

\begin{table}[htbp]
\caption{The far right columns present the estimated compute usage by large AI models relative to three baselines: the estimated compute usage of all public models from that company (Model / Public Models (\%)), the estimated annual FLOP capacity for a year running at PEAK (Model / Peak Annual(\%)), and the former multiplied by 100x. For full details on this table and sources of data which feed it, please see Appendix II.}
\label{tab:model-compute-summary}
\tiny
\begin{tabular}{lllrrrrrr}
\toprule
Model & Organization & Lab/Cloud & Train & Parent Org Peak & Model/Public & Model/Peak & Model/Peak \\
& & & FLOPs & Annual FLOPs & Models (\%) & Annual (\%) & w/100x (\%) \\
\midrule
Gemini 1.0 Ultra & Google DeepMind & Google DeepMind & $5.00 \times 10^{25}$ & $3.87 \times 10^{28}$ & 45.65 & 0.129 & 12.93 \\
Claude 3.5 Sonnet & Anthropic & Anthropic/Amazon & $4.98 \times 10^{25}$ & $2.27 \times 10^{28}$ & 69.74 & 0.220 & 21.96 \\
GPT-4o & OpenAI & Microsoft/OpenAI & $3.81 \times 10^{25}$ & $4.35 \times 10^{28}$ & 53.36 & 0.088 & 8.75 \\
Llama 3.1-405B & Meta AI & Meta AI & $3.80 \times 10^{25}$ & $5.65 \times 10^{28}$ & 66.32 & 0.067 & 6.72 \\
GPT-4 & OpenAI & Microsoft/OpenAI & $2.10 \times 10^{25}$ & $4.35 \times 10^{28}$ & 29.41 & 0.048 & 4.82 \\
Gemini 1.0 Pro & Google DeepMind & Google DeepMind & $1.83 \times 10^{25}$ & $3.87 \times 10^{28}$ & 16.71 & 0.047 & 4.73 \\
Claude 3 Opus & Anthropic & Anthropic/Amazon & $1.64 \times 10^{25}$ & $2.27 \times 10^{28}$ & 22.97 & 0.072 & 7.23 \\
Gemini 1.5 Pro & Google DeepMind & Google DeepMind & $1.58 \times 10^{25}$ & $3.87 \times 10^{28}$ & 14.43 & 0.041 & 4.09 \\
Llama 3-70B & Meta AI & Meta AI & $7.86 \times 10^{24}$ & $5.65 \times 10^{28}$ & 13.72 & 0.014 & 1.39 \\
GPT-4o mini & OpenAI & Microsoft/OpenAI & $7.36 \times 10^{24}$ & $4.35 \times 10^{28}$ & 10.31 & 0.017 & 1.69 \\
PaLM 2 & Google & Google DeepMind & $7.34 \times 10^{24}$ & $3.87 \times 10^{28}$ & 6.70 & 0.019 & 1.90 \\
Llama 3.3 & Meta AI & Meta AI & $6.86 \times 10^{24}$ & $5.65 \times 10^{28}$ & 11.98 & 0.012 & 1.21 \\
Amazon Nova Pro & Amazon & Anthropic/Amazon & $6.00 \times 10^{24}$ & $2.27 \times 10^{28}$ & 8.40 & 0.026 & 2.65 \\
Amazon Titan & Amazon & Anthropic/Amazon & $4.80 \times 10^{24}$ & $2.27 \times 10^{28}$ & 6.72 & 0.021 & 2.12 \\
Claude 2 & Anthropic & Anthropic/Amazon & $3.87 \times 10^{24}$ & $2.27 \times 10^{28}$ & 5.41 & 0.017 & 1.70 \\
Minerva (540B) & Google & Google DeepMind & $2.74 \times 10^{24}$ & $3.87 \times 10^{28}$ & 2.50 & 0.007 & 0.71 \\
GPT-3.5 (text-davinci-003) & OpenAI & Microsoft/OpenAI & $2.58 \times 10^{24}$ & $4.35 \times 10^{28}$ & 3.61 & 0.006 & 0.59 \\
U-PaLM (540B) & Google & Google DeepMind & $2.53 \times 10^{24}$ & $3.87 \times 10^{28}$ & 2.31 & 0.007 & 0.65 \\
PaLM (540B) & Google Research & Google DeepMind & $2.53 \times 10^{24}$ & $3.87 \times 10^{28}$ & 2.31 & 0.007 & 0.65 \\
Flan-PaLM 540B & Google & Google DeepMind & $2.50 \times 10^{24}$ & $3.87 \times 10^{28}$ & 2.28 & 0.006 & 0.65 \\
FLAN 137B & Google Research & Google DeepMind & $2.05 \times 10^{24}$ & $3.87 \times 10^{28}$ & 1.87 & 0.005 & 0.53 \\
Meta Movie Gen Video & Meta AI & Meta AI & $1.65 \times 10^{24}$ & $5.65 \times 10^{28}$ & 2.88 & 0.003 & 0.29 \\
Megatron-Turing NLG 530B & Microsoft,NVIDIA & Microsoft/OpenAI & $1.17 \times 10^{24}$ & $4.35 \times 10^{28}$ & 1.64 & 0.003 & 0.27 \\
Llama 2-70B & Meta AI & Meta AI & $8.10 \times 10^{23}$ & $5.65 \times 10^{28}$ & 1.41 & 0.001 & 0.14 \\
% Gopher (280B) & DeepMind & Google DeepMind & $6.31 \times 10^{23}$ & $3.87 \times 10^{28}$ & 0.58 & 0.002 & 0.16 \\
% Chinchilla & DeepMind & Google DeepMind & $5.76 \times 10^{23}$ & $3.87 \times 10^{28}$ & 0.53 & 0.001 & 0.15 \\
% \% LLaMA-65B & Meta AI & Meta AI & $5.50 \times 10^{23}$ & $5.65 \times 10^{28}$ & 0.96 & 0.001 & 0.10 \\
% \% OPT-175B & Meta AI & Meta AI & $4.30 \times 10^{23}$ & $5.65 \times 10^{28}$ & 0.75 & 0.001 & 0.08 \\
% \% BlenderBot 3 & McGill University,Meta AI,Mila & Meta AI & $4.30 \times 10^{23}$ & $5.65 \times 10^{28}$ & 0.75 & 0.001 & 0.08 \\
% \% Parti & Google Research & Google DeepMind & $3.96 \times 10^{23}$ & $3.87 \times 10^{28}$ & 0.36 & 0.001 & 0.10 \\
% \% FunSearch & Google DeepMind & Google DeepMind & $3.87 \times 10^{23}$ & $3.87 \times 10^{28}$ & 0.35 & 0.001 & 0.10 \\
\bottomrule
\end{tabular}
\end{table}

The contrast between frontier models consuming a small fraction of quarterly compute budgets and ongoing infrastructure expansion suggests that leading AI firms train numerous experimental models beyond their final deployments. This interpretation aligns with widely documented practices in frontier AI laboratories. Major labs employ hundreds to thousands of researchers who routinely train models during development. Standard optimization procedures, such as hyperparameter sweeps, involve training single model architectures tens to hundreds of times to identify optimal configurations \cite{wandb2025sweeps_tutorial}.
\newpage
Taken together, producing a final frontier model requires training hundreds to thousands of intermediate models during the R\&D process. While hyperparameter optimization is essential to model development, current approaches necessarily involve complete retraining for each configuration. This contrasts with modular systems where components can be incrementally optimized without discarding the entire structure. Recent work on targeted model modification (e.g., LLM surgery \cite{veldanda2024llm}) suggests alternatives to full retraining, but such techniques have not been widely adopted in frontier model development, necessitating substantial computational expenditure during optimization.

Consider the magnitude of this inefficiency. If frontier labs aim to produce general-purpose models, then computational resources allocated to intermediate experimental models represent overhead that does not directly contribute to final model capability. Based on Table \ref{tab:model-compute-summary}, approximately 98.53\% of annual training budgets are allocated to models other than the final deployment (calculated as $100\% - (0.22\% / 15\%)$, where 15\% represents the estimated fraction of total compute dedicated to training \cite{bratt2025inference}). Under the objective of producing a general-purpose model, this implies that the vast majority of training compute is allocated to parameters that are ultimately discarded during the optimization process.

This annual estimate may understate total inefficiency, as it assumes frontier labs must train at least one model from scratch annually. If model parameters could be efficiently reused across generations (as RETRO/ATLAS demonstrate through their retrieval databases, where up to 98\% of knowledge can be transferred between model versions \cite{borgeaud2022improving, izacard2023atlas}) the efficiency gap would be larger. However, retraining overhead represents only one source of computational waste. A second source arises from how models process information during training. To observe the theory behind this phenomena, consider the following analogy.

\vspace{0.5cm}
\begin{tcolorbox}[
enhanced,
breakable,
colback=white,
colframe=gray,
boxrule=0.5pt,
left=8pt,
right=8pt,
top=8pt,
bottom=8pt,
title=A Library Analogy]
Once again, consider a library. When someone submits a new book to a library, the librarian doesn't subsequently read \textit{every book in the library} to figure out where to store it on the shelves. Instead, they use the catalog system to find a relevant bookshelf, the decimal system to locate the right placement on the shelf, and (perhaps) the title of the book to find its alphabetical placement on that shelf. 
\\
\\
This practice stands in stark contrast to how AI systems are trained. To add \textit{a single training example} into a model like GPT-3, AI users forward propagate \textit{through the entire model and all of its knowledge} (i.e., read every book in the library). And to train an entire model like GPT-3 on modern training corpuses (i.e. trillions of tokens), it repeats this process \textit{trillions of times}. It is like a librarian who repeatedly reads every book to figure out where a book should be placed, and then complains about not having enough librarian assistants (i.e. GPU compute threads) to accomplish the task.
\end{tcolorbox}
\vspace{0.5cm}

The question concerns the magnitude of this inefficiency. The RETRO and ATLAS results previously discussed demonstrate that models can achieve comparable performance while being 25-50x smaller in parameter count. This parameter reduction translates directly to reduced training costs: fewer parameters require proportionally fewer FLOPs during both forward and backward propagation. The compression results further indicate that models can be trained with 5-10x fewer parameters without performance degradation, compounding the potential efficiency gains.

Yet, these three sources of training inefficiency (retraining overhead, dense forward propagation, and parameter redundancy) do not exhaust the potential efficiency gains. A fourth source arises from how models organize information during the training process itself.

\vspace{0.5cm}
\begin{tcolorbox}[
enhanced,
breakable,
colback=white,
colframe=gray,
boxrule=0.5pt,
left=8pt,
right=8pt,
top=8pt,
bottom=8pt,
title=A Library Analogy]
Consider a brand new library. When a librarian goes to stock their library, they do not necessarily store \textit{every book in the universe} in their library. Instead, libraries participate as a part of a library \textit{network}. In this way, a nation-wide (or even global) community of libraries each store a cache of books, and when one user asks their local library for a book they do not have, that library will call in that book from another library. Through this process, even tiny, rural libraries are (in a way) making a massive, global collection of knowledge available to their local community. Some might even say that a small, local library makes all of humanity's knowledge available to their local community, even if their local collection is small.
\\
\\
This practice stands in stark contrast to how AI systems are trained. Firms around the world are scraping the internet (or downloading web scrapes) and training their own models from scratch \textit{largely on the same information scraped from the internet}. In these practices, they are encoding the same information redundantly across many organizations instead of building upon the existing knowledge already encoded into neural weights by other parties. That is to say, AI companies repeat each others' work to a great degree.
\end{tcolorbox}
\vspace{0.5cm}

AI models store information within their weights. General-purpose models encode substantial portions of publicly available internet data through training on large, overlapping corpora. Multiple organizations train models on similar or identical datasets, often drawn from common sources such as web scrapes and public repositories. This results in redundant encoding of the same information across independently trained models.

\begin{table}[htbp]
\caption{Estimated Total Worldwide AI Computing Capacity (Q4 2024).}
\label{tab:total-capacity-first}
\small
\begin{tabular}{lrrl}
\toprule
Category & Computing Power & Share & Source/Calculation \\
& (FLOP/s) & (\%) & \\
\midrule
\multicolumn{4}{l}{\textit{Cloud/AI Providers}} \\
Meta & $1.79 \times 10^{21}$ & 5.57 & From Table \ref{tab:cloud-deployments} Total Q4 2024 \\
Microsoft/OpenAI & $1.38 \times 10^{21}$ & 4.29 & From Table \ref{tab:cloud-deployments} Total Q4 2024 \\
Google/DeepMind & $1.23 \times 10^{21}$ & 3.81 & From Table \ref{tab:cloud-deployments} Total Q4 2024 \\
Amazon/Anthropic & $7.19 \times 10^{20}$ & 2.23 & From Table \ref{tab:cloud-deployments} Total Q4 2024 \\
\addlinespace
\multicolumn{4}{l}{\textit{Consumer Computing}} \\
Smartphones & $7.48 \times 10^{21}$ & 23.23 & Sum of Active iPhones/Androids from Table \ref{tab:new-capacity} \\
PC CPUs/GPUs & $2.23 \times 10^{21}$ & 6.92 & Sum of PC CPUs and GPUs from Table \ref{tab:new-capacity} \\
Game Consoles & $8.64 \times 10^{20}$ & 2.68 & From Table \ref{tab:new-capacity} \\
\addlinespace
Other Cloud/Pre-2023 & $1.65 \times 10^{22}$ & 51.28 & (see appendix for details) \\
\midrule
Total & $3.22 \times 10^{22}$ & 100.00 & Sum of all rows above \\
\bottomrule
\end{tabular}
\end{table}
\newpage

From an information-theoretic perspective, this redundancy is inefficient; the question is: how much? Comprehensive measurement would require documenting the overlap in training data and model capabilities across organizations, information that is not systematically available.

However, industry analysis provides order-of-magnitude estimates of aggregate underutilization. The largest AI firm controls less than 5.57\% of global computing capacity (Table \ref{tab:total-capacity-first}). If training resources could be pooled across organizations (analogous to libraries participating in a global network rather than maintaining independent collections) available compute would increase by a factor of approximately $100/5.57 \approx 17.95$x relative to any single firm's capacity. This represents the theoretical gain from distributed training architectures that enable collaborative model development across organizational boundaries.

Yet, even these four training inefficiencies may not fully describe the inefficiency present in modern AI. To observe the theory behind this next problem, consider the following analogy.

\begin{tcolorbox}[
enhanced,
breakable,
colback=white,
colframe=gray,
boxrule=0.5pt,
left=8pt,
right=8pt,
top=8pt,
bottom=8pt,
title=A Library Analogy]
Consider a brand new library which doesn't even have any books in it yet. Let's say the librarian is in a hurry, and so they take the first book, pick one of the empty shelves, set the book on that shelf, and then run to get the second book. Then, looking at the second book, they ask, "is this similar to the first book... or different". And if it's similar to the first book, they put it closer to the first book on the shelves, and if it's different from the first book, they put it farther away. This librarian repeats this process over and over until they encounter a problem. After 10,000 books (out of the millions they have to load), one of the bookshelves is full. So, because the shelf they need is full, they run to the next shelf and \textit{empty it... throwing books onto the floor} to make space for their new book, which needs to be in this location. But now, they need to re-stock the books they just threw on the floor! They then pick up the books from the floor and attempt to find them all new places in the library, accidentally filling up shelves in the process. They then repeat this process many, many times... stocking and re-stocking all the books until a sensible organization emerges.
\\
\\
Or consider another librarian, who is opening a new library. But before they begin stocking books, they set up a Dewey Decimal System. They label each book in the system, count the number of books in each category, and plan their shelf capacity appropriately. Then, they take each book and load it into its appropriate shelf in a single pass. This second technique stands in stark contrast to how AI systems are trained. To add \textit{the first training example} into an untrained model like GPT-3, AI users forward propagate \textit{through the entire model and all of its knowledge} and store that information in random locations throughout the model. Then, as more training examples pile into the model, the model experiences \textit{catastrophic forgetting} \cite{doi:10.1073/pnas.1611835114} as collisions occur. And to train an entire model like GPT-3 on modern training corpuses (i.e. trillions of tokens), it repeats this process \textit{trillions of times}.
\end{tcolorbox}

Quantifying the computational cost of catastrophic forgetting remains challenging due to limited empirical work on information segmentation during training. RETRO and ATLAS provide partial evidence, as does work on curriculum learning and knowledge distillation. Kemker et al. observe that avoiding catastrophic forgetting requires approximately 40x larger model capacity \cite{kemker2018measuring}, though this estimate is not especially recent. Multiple sources of training inefficiency compound: full model retraining during updates, dense forward propagation through all parameters, parameter redundancy from insufficient compression, and capacity overhead to prevent catastrophic forgetting during sequential training.

These inefficiencies compound multiplicatively in terms of FLOPs. Parameter reuse across model generations could increase compute productivity by a factor of $100/(100 - 98.53\%) \approx 68$x. RETRO/ATLAS-demonstrated parameter efficiency provides 25-50x gains. Lossless compression techniques offer an additional 5-10x reduction. Catastrophic forgetting avoidance inflates model sizes by approximately 40x. Combined multiplicatively, these factors suggest potential efficiency gains ranging from $(68 \times 25 \times 5 \times 17.95) \approx 153{,}000$x to $(68 \times 50 \times 10 \times 17.95 \times 40) \approx 24{,}400{,}000$x, representing approximately 5-7 orders of magnitude of potential compute productivity improvement.

\begin{table}[htbp]
\centering
\caption{Summary of Major AI System Inefficiencies}
\label{tab:inefficiencies}
\begin{tabular}{lrrr}
\toprule
Inefficiency Type & Range & Evidence \\
\midrule
\multicolumn{3}{l}{\textit{Inference Inefficiencies:}} \\
Full Forward Propagation & 25-50x+ & RETRO/ATLAS \\
Parameter Redundancy & 5-10x+ & Compression \\
Catastrophic Forgetting & 40x & Size Heuristic \\
\addlinespace[0.5em]
\multicolumn{3}{l}{\textit{Training Inefficiencies:}} \\
Re-training from Scratch & 68x+ & Industry Analysis \\
Full Forward Propagation & 25-50x+ & RETRO/ATLAS \\
Parameter Redundancy & 5-10x+ & Compression \\
Siloed Compute & (est.) 17.95x & Global Compute \\
Catastrophic Forgetting & 40x & Size Heuristic \\
\addlinespace[0.5em]
\multicolumn{3}{l}{\textit{Combined Effects:}} \\
Inference Total & 5000-20,000x+ & Multiplicative \\
Training Total & 6,103,000-24,412,000x+ & Multiplicative \\
\bottomrule
\end{tabular}
\end{table}

Taken together, these estimates suggest that current inference practices exhibit inefficiency factors of approximately $5{,}000$-$20{,}000$x, while training practices exhibit inefficiency factors of approximately $150{,}000$-$24{,}000{,}000$x. These bounds support the conclusion that at least six orders of magnitude of compute productivity remains unexploited in current AI systems.

Yet even these estimates contain within them one especially conservative estimate, the 25-50x inefficiency from dense forward propagation. While RETRO/ATLAS provides the only concrete lower bound on this inefficiency, the true sparsity opportunity is almost certainly significantly more. To return to the library analogy, what percentage of the world's collective library is needed to answer a particular question? If one believes that 2-4 percent of all human knowledge is needed for every question, then perhaps the RETRO/ATLAS estimate is accurate. 

While systematic measurement is lacking, the assumption that any query requires more than one-millionth of a model's knowledge base ($>10^{-6}$) appears conservative, suggesting these efficiency estimates may understate potential gains by an additional 4 orders of magnitude (although this is merely conjecture... future empirical work is needed).

\begin{tcolorbox}[
enhanced,
breakable,
colback=white,
colframe=gray,
boxrule=0.5pt,
left=8pt,
right=8pt,
top=8pt,
bottom=8pt,
title=A Full Picture of Compute Waste: The Library Analogy]
Consider first how an AI system would operate as a librarian. When someone asks about the rules of chess, this librarian doesn't merely consult the games section. Instead, they read \textit{every single book in the library}. Not just once, they do this for \textit{every single query}. When this AI librarian needs to add a new book to their collection, they don't simply locate an appropriate shelf using a catalog system. Instead (and this characterizes a fundamental inefficiency in current AI systems) they first read \textit{every book in the library}, then displace existing books onto the floor to make space, then must \textit{re-read everything} to determine where to relocate those displaced books. This process repeats, sometimes \textit{trillions of times}, until the library reaches a new equilibrium. Worse, if a book needs to be decisively removed, the entire library must be burned to the ground, all of the books burned, and a new library constructed from scratch, repeating the entire aformentioned process over again.
\\
\\
Furthermore, this AI librarian doesn't participate in an efficient network of libraries. Instead, they insist on maintaining their own complete copy of \textit{every book in existence}, greatly amplifying the challenge of the aformentioned processes. When other AI librarians open new libraries, they too download and store the same vast collection, redundantly encoding identical information in countless separate locations (but also re-paying the cost of learning how to organize their libraries). And when these AI librarians need to modify their collections (to add or remove significant knowledge) they don't merely reorganize their existing structure. Instead, they \textit{retrain from scratch}... also equivalent to burning their entire library to the ground, demolishing every book, and rebuilding the complete collection from the ground up (repaying all of the aformentioned costs).
\\
\\
Now consider how human librarians process information. When someone inquires about chess, they navigate directly to the games section, select a relevant text, and locate the rules. When adding a new book, they utilize the Dewey Decimal system to identify the appropriate shelf and place it there. The process is direct, efficient, and purposeful. Moreover, human librarians don't attempt to store every book in existence in their local library. Instead, they participate in an interconnected system of libraries, each maintaining their own cache of books. When a patron requests a book not locally available, the librarian simply requests it from another library in the network. Through this elegant system, even the smallest rural library can provide access to humanity's collective knowledge.
\end{tcolorbox}

The contrast between these approaches illuminates a critical insight about current AI systems. They operate with a level of inefficiency that we've somehow normalized within the field. In essence, what we've built is a global network of millions of librarians who must read their entire library just to fetch a single book, read it again to add a new book, and then read it countless more times to relocate all the books they displaced in the process. And when they're not doing this trillions of times over, they're burning their buildings to the ground, destroying their entire collections, and starting over from scratch. And as the world's AI compute costs approach the level of a small nation, this practice, despite being ubiquitous within AI research, represents perhaps the greatest inefficiency in the history of information processing. And via this chapter, this thesis will describe existing innovations which could alleviate much of this inefficiency.

\subsection{6+ OOM: Siloed Data}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{abc_ch2_2_2_v5.png}
\caption{A tree of sub-problems regarding data siloing.}
\label{fig:broad_listening}
\end{figure}

Following growing rumors across the AI research community that data is becoming a major bottleneck, OpenAI's former chief scientist, Ilya Sutskever, announced during his test of time award speech at NeurIPS 2024 that data for training AI has peaked, "We've achieved peak data and there'll be no more" \cite{robison2024openai}. However, while this may be true for the AI industry, and Ilya (being recent Chief Scientist at OpenAI) is perhaps one of the best people in the world to know, Ilya's statement does not reflect the reality of what data exists in the world.
\vspace{3px}
\begin{tcolorbox}[
enhanced,
breakable,
colback=white,
colframe=gray,
boxrule=0.5pt,
left=8pt,
right=8pt,
top=8pt,
bottom=8pt,
title=A Library Analogy]
Consider a world where libraries could only acquire books through anonymous donations left on their doorstep. No matter how many valuable books exist in private collections, university archives, or government repositories, libraries would be limited to what people voluntarily abandon. In such a world, librarians might reasonably conclude they're "running out of books", even while surrounded by vast, inaccessible collections within surrounding businesses and homes.
\\
\\
This mirrors the current state of AI training. When frontier models like GPT-4 (trained on 6.5 trillion tokens), and Qwen2.5-72B (18 trillion tokens), LLama 4 (30 trillion tokens), \cite{EpochNotableModels2024} report hitting data limits, they're really hitting access limits. They're not running out of data, they're running out of data they can freely collect.
\end{tcolorbox}

\subsubsection{2-4 Orders of Magnitude: Text Humans Create By Hand}

Dataset sizes for frontier AI models range from publicly disclosed values to industry estimates. GPT-4 was trained on approximately 6.5 trillion tokens, while Alibaba's Qwen2.5-72B used 18 trillion tokens. The largest reported text dataset, used for Meta's Llama 4, contains 30 trillion tokens \cite{EpochAIModels2025}. Using RedPajama as a reference \cite{together2023redpajama}, each trillion tokens requires less than 6TB of storage, implying that the largest known training dataset (Llama 4) occupies less than 180TB.

% Table 1: Public Digital Content 
\begin{table}[htbp]
\centering
\begin{tabular}{lrrr}
\hline
\textbf{Category \& Source} & \textbf{Words (T)} & \textbf{Tokens (T)} & \textbf{Rel. Size*} \\
\hline
\multicolumn{4}{l}{\textbf{Web Data}} \\
FineWeb & 11 & 15 & 1.0 \\
Non-English Common Crawl (high quality) & 13.5 & 18 & 1.0 \\
All high quality web text & 45--120 & 60--160 & 4.0--11.0 \\
\hline
\multicolumn{4}{l}{\textbf{Code}} \\
Public code & -- & 0.78 & 0.05 \\
Private Code & -- & 20 & 1.3 \\
\hline
\multicolumn{4}{l}{\textbf{Academic Publications and Patents}} \\
Academic articles & 0.8 & 1 & 0.07 \\
Patents & 0.15 & 0.2 & 0.01 \\
\hline
\multicolumn{4}{l}{\textbf{Books}} \\
Google Books & 3.6 & 4.8 & 0.3 \\
Anna's Archive & 2.8 & 3.9 & 0.25 \\
Every unique book & 16 & 21 & 1.4 \\
\hline
\multicolumn{4}{l}{\textbf{Court Documents}} \\
US federal court documents & 2 & 2.7 & 0.2 \\
\hline
\multicolumn{4}{l}{\small *Relative size using Llama 3 = 1 as reference} \\
\end{tabular}
\caption{Estimated Volume of Public Digital Text Content \cite{educating_silicon_2024}}
\label{tab:public-text}
\end{table}

% Table 2: Social Media, Audio Content and Communication
\begin{table}[htbp]
\centering
\begin{tabular}{lrrr}
\hline
\textbf{Category \& Source} & \textbf{Words (T)} & \textbf{Tokens (T)} & \textbf{Rel. Size*} \\
\hline
\multicolumn{4}{l}{\textbf{Social Media}} \\
Twitter / X & 8 & 11 & 0.7 \\
Weibo & 29 & 38 & 2.5 \\
Facebook & 105 & 140 & 10.0 \\
\hline
\multicolumn{4}{l}{\textbf{Publicly Available Audio (Transcribed)}} \\
YouTube & 5.2 & 7 & 0.5 \\
TikTok & 3.7 & 4.9 & 0.3 \\
All podcasts & 0.56 & 0.75 & 0.05 \\
Television archives & 0.05 & 0.07 & 0.001 \\
Radio archives & 0.5 & 0.6 & 0.04 \\
\hline
\multicolumn{4}{l}{\textbf{Private Data}} \\
All stored instant messages & 500 & 650 & 45.0 \\
All stored email & 900 & 1200 & 80.0 \\
\hline
\multicolumn{4}{l}{\textbf{Total Human Communication}} \\
Daily & 115 & 150 & 10 \\
Since 1800 & 3,000,000 & 4,000,000 & 10^5 \\
All time & 6,000,000 & 8,000,000 & 10^5 \\
\hline
\multicolumn{4}{l}{\small *Relative size using Llama 3 = 1 as reference} \\
\end{tabular}
\caption{Estimated Volume of Text Data \cite{educating_silicon_2024}}
\label{tab:social-communication}
\end{table}

The scale of untapped data is staggering. As shown in Tables \ref{tab:public-text} and \ref{tab:social-communication}, stored email and instant messages alone contain over 1,850 trillion tokens, approximately 60 times the largest known training dataset \cite{educating_silicon_2024}. Daily human communication generates approximately 150 trillion tokens, accumulating to roughly 55 quadrillion tokens annually (approximately 1,750 times the scale of frontier training sets).

\subsubsection{6+ Orders of Magnitude: Multi-media data broadly}

Yet even this vast sea of text represents merely a drop in the ocean of total digital data. While frontier AI models train on curated web scrapes such as Common Crawl (454 TB as of December 2023) \cite{wikipedia_commoncrawl_2024}, the Internet Archive's Wayback Machine alone stores approximately 100 petabytes \cite{internet_archive_donation}. Meanwhile, global digital data is projected to reach 180 zettabytes by 2025 \cite{mider_osint_2024, taylor2024data}, six orders of magnitude larger than The Internet Archive and nine orders of magnitude larger than the largest known training datasets.

\begin{tcolorbox}[
enhanced,
breakable,
colback=white,
colframe=gray,
boxrule=0.5pt,
left=8pt,
right=8pt,
top=8pt,
bottom=8pt,
title=A Library Analogy]
Consider a national library system. While a single library might proudly maintain millions of books, this represents only a tiny fraction of all written human knowledge. Beyond its walls lie vast corporate archives, government repositories, university collections, and personal libraries. To get a sense of scale — consider the size of a library's physical building, and compare that to the size of the rest of the physical buildings in a city — each with books and letterboxes and filing cabinets containing all manner of correspondence and record. Each holds unique and valuable information, yet remains inaccessible to the library system not because of physical constraints, but because of attribution and control concerns.
\\
\\
Similarly, when AI companies claim to have "reached peak data," they're really saying they've exhausted what they can freely obtain (often without permission or attribution). The actual digital data of the world — in private databases, corporate systems, government archives, and personal devices — remains largely untapped, representing over six orders of magnitude more information than current AI systems can access.
\end{tcolorbox}
\vspace{0.1cm}

The magnitude of this disparity is difficult to comprehend. While the largest known AI dataset (to the awareness of this researcher) is roughly 180 TB of information, and may be derived from a dataset as big as common crawl (450 TBs of information), or if we were very, very conservative, might be as big as the full history of the entire publicly indexable internet (and other data the Internet Archive keeps) 100 PBs, even this number is \textit{6+ orders of magnitude smaller} than the amount of digital data in the world. Taken together, even under highly conservative estimates, it is very likely that AI has not yet trained on even one millionth of the amount of data that humanity has digitized. And beyond what humanity has digitized lies the vast amounts of information which is not yet encoded into a computer. 

Consider the 150 trillion tokens humans create every day, the zettabytes-worth of yet-to-be-videoed information going on across the planet and all of its inhabitants which (despite helping living creatures get smarter day-in-and-day-out) is completely inaccessible to systems which only read digital information. This striking disparity between used and available data raises a crucial question: why, in an era of unprecedented digital abundance, do AI systems train on such a microscopic fraction of all knowledge? The answer lies not in easily observable symptoms like data availability, but in fundamental problems underpinned by insufficient ABC.

\subsection{The Search for Root Causes (Three "Whys")}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{abc_ch2_2_3_v6.png}
\caption{The cascade of causes between the addition problem and ABC.}
\label{fig:broad_listening}
\end{figure}

The previous section revealed a paradox: despite widespread beliefs of data and compute scarcity, AI systems access less than one millionth of digital resources, and an untold micro-fraction of the world's information broadly. This under-utilization raises a critical question: if more data and compute directly improves AI capabilities through scaling laws, why do AI systems use such a tiny fraction of what's available? The answer lies in a cascade of technical and institutional barriers, each revealing a deeper "why" that must be understood:

\begin{itemize}
\item First Why: Attribution-based Control
% Data sources fight against AI training because they cannot control how their data is used

\item Second Why: Deep Learning's Feature Mixing Precludes Partitioning
% These inefficiencies stem from how neural networks fundamentally process information

\item Third Why (Root Cause): Addition of Source-Separable Concepts
% These inefficiencies stem from how neural networks fundamentally process information
\end{itemize}

As we follow this chain of questions, we'll see how each answer reveals a deeper technical challenge. More importantly, we'll discover how recent breakthroughs in cryptography, deep learning, and distributed systems have already created solutions to these challenges (solutions which remain largely unrecognized by the AI community).

\newpage

\section{First Why: Attribution-based Control}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{1411.3146/abc_ch2_3_0_v5.png}
\caption{Subset of conceptual graph regarding Section 2.3}
\label{fig:broad_listening}
\end{figure}

The previous section revealed significant inefficiencies in the training of AI systems: 6+ orders of magnitude in underutilized data and compute. While there may be multiple contributing factors to these constraints, this thesis and chapter examines one particular root cause: AI's inability to provide attribution-based control (ABC). An AI model possesses attribution-based control when two properties are true: data sources control which AI predictions they support, AI users control which data sources they rely upon for an AI prediction. Following this definition (Definition~\ref{def:abc}), ABC implies certain architectural properties as novel requirements:

\begin{itemize}
\item \textbf{Source-Partitionable Representations:} Knowledge within an AI system is partition-able by source, otherwise sources lose control when their information is mixed, requiring:
\begin{itemize}
    \item \textbf{Source-Partitionable Inference:} Partitions are independently usable at inference, otherwise users can't select specific sources and sources can't participate irrespective of the decisions of other sources
    \item \textbf{Source-Partitionable Training:} Partitions are independently trainable, otherwise sources can't update their contributions without requiring other sources to do so.
\end{itemize}
\item \textbf{Rapid Partition Synthesis:} Partitions are rapidly synthesize-able during inference, otherwise collective insights which are only learnable via information from multiple sources cannot be realized in production AI systems.
\end{itemize}

Frontier AI systems lack these properties. Yet, if these properties were achieved, attribution would be achieved and aforementioned problems regarding data and compute productivity would be impacted. Let's examine each in detail, linking each problem to attribution-based control.

\subsection{ABC and Compute Productivity (6+ OOM)}

ABC would address compute productivity issues along two dimensions: access and learning structure. Regarding structure, successful ABC would necessarily provide a means to structure the learning and representation process, reducing re-training, forward propagation, redundancy, and catastrophic forgetting. Regarding access, ABC would provide a means to overcome incentive issues presently siloing the world's compute resources. Let us consider these claims in the context of inference and learning. 

\subsubsection{2-3 OOM: Inefficient AI inference}

\vspace{0.5cm}
\begin{tcolorbox}[
enhanced,
breakable,
colback=white,
colframe=gray,
boxrule=0.5pt,
left=8pt,
right=8pt,
top=8pt,
bottom=8pt,
title=A Library Analogy]
Consider a library wherein a librarian reads every book in the library whenever they need to answer a question, including reading multiple copies of the same book and pretending to read an empty book whenever shelves contain empty sections. 
\\
\\
A solution to attribution-based control would necessarily reconfigure the library to fetch information based on its source (book or author). While such a solution might be challenging, if it successfully delivered capable predictions, it would necessarily do so while skipping an enormous amount of wasted computation. This is because ABC is not actually about selecting the sources one desires (normal full forward propagation already does this), its actually about \textit{ignoring all the books you don't want}. ABC requires making an AI inference while skipping an enormous amount of wasted computation, because (to return to the analogy) it would involve training the librarian on how to skip reading the entire library when making a prediction... and only fetch information from specific, relevant sources (i.e. books).
\\
\\
Thus, while ABC's definition might not appear to require an increase in computational efficiency, its definition directly requires that a vast amount of information not be included in the computational process, reducing the need to compute over that information.
\end{tcolorbox}
\vspace{0.5cm}

Recall that current AI models must activate vast numbers of parameters for every prediction because they struggle to pre-identify which parameters are storing relevant information to the present query; they struggle with sparse forward propagation. While various approaches to sparse AI exist, successful ABC would necessarily enable one particularly compelling form of sparsity: source-based sparsity (i.e. source-based partitioning). Consequently, if an AI model could forward predict based on a relevant subset (i.e., the top 10) of many (i.e. billions) of sources, then it is very likely to also greatly reduce the computational complexity involved in forward propagating, because it would be forward propagating through vastly less information.

RETRO and ATLAS demonstrate the minimum scale of such a breakthrough. By maintaining source-based partitions through their database architecture, they achieve equal performance while activating only 2-4\% of the parameters of similarly performant dense models \cite{borgeaud2022improving, izacard2023atlas}.

Similarly, recall how current models process some combination of redundant copies of knowledge and empty regions of parameter space during inference. While there are many approaches to reducing these forms of waste (e.g., distillation, compression, etc.), successful ABC would necessarily enable one particularly compelling way to avoid copies or empty space: source-based partitioning. If an AI model could forward predict based on a relevant subset (i.e., the top 10) of many (i.e., billions) of sources, then tuning the number of sources being relied upon could also tune the redundancy being used in forward propagation. Meanwhile, ensuring that the partitions being leveraged for forward propagation were relevant to the query would combat the risk of forward propagating empty space \footnote{Additionally, if successful ABC reduced the number of parameters being used as a result of these other techniques, then it would also increase the number of samples being applied to each set of dense parameters — perhaps reducing the opportunity for empty vector space (more on this later)}.

RETRO and ATLAS demonstrate these principles through their database architecture, showing how source-based organization can naturally eliminate redundant processing and avoid computation on irrelevant parameters while maintaining model capability. They also demonstrate the ability for such partitioning to increase the number of samples being used against a dense (i.e. non-partitioned) section of the network, reducing empty space. Yet as significant as these inference inefficiencies are, they pale in comparison to the waste in how AI systems learn.

\subsubsection{6+ OOM: Underutilized and Inefficient Compute in AI Learning}

Recall that current AI models must retrain entirely from scratch when updating their knowledge, because of problems such as catastrophic forgetting \cite{kemker2018measuring}. While various approaches to incremental training exist, successful ABC would necessitate one particular solution: source-partitioned retraining. Consequently, if an AI model can train source-separated subsections of its weights, it can re-train them as well, and it is very likely to also greatly reduce the computational complexity involved in the re-training process, because it would be re-training vastly fewer parameters at a time. RETRO/ATLAS demonstrate one such approach, wherein re-training can be done with the computational complexity involved in adding or removing vector-embeddings from a database.

Similarly, recall how current training processes waste compute in multiple ways: through redundant copies of the same knowledge, through activation of irrelevant parameters, and through inefficient parameter density. While various approaches to training efficiency exist, successful ABC would necessarily enable compelling means to overcome these problems (as already described in the previous section describing ABC's impacts on inference).

Additionally, these ABC opportunities further compound when we consider how compute is distributed across organizations. Consider our library analogy once more: libraries don't each maintain copies of every book ever written; they form networks to share resources efficiently. In contrast, frontier AI models are created by a host of organizations around the world, each re-paying the cost of creating an AI model (most of which are trained on largely the same data).

Successful ABC may activate an economic incentive addressing this waste \footnote{This hypothesis has been somewhat validated by OpenMined in early pilots of ABC-enabled AI systems with publishers, but this research is as-of-yet unfinished}. At the present moment, frontier AI models are economic bundles, marketed under a story which itself is an economic bundle: artificial general intelligence. Because of this, if one AI company re-trains the capabilities of another company (e.g. spending \$200M on compute) and then adds a bit more to it (e.g., another \$10M in data and compute), an end user must then choose between that full economic bundle and another full economic bundle. This creates a situation wherein companies effectively have to pay the minimum amount to catch up to the leading position and then extend some beyond it. 

\vspace{0.5cm}
\begin{tcolorbox}[
enhanced,
breakable,
colback=white,
colframe=gray,
boxrule=0.5pt,
left=8pt,
right=8pt,
top=8pt,
bottom=8pt,
title=A Library Analogy]
Consider a new library which doesn't yet have any books. To load the library in the style of AI, a librarian would first build tens-of-thousands of libraries and print copies of books into each one (i.e. signifying both the redundant training of models by many companies and the hyperparameter sweeps occurring within each one). Then, within each library, a librarian would first load all the shelves (of which there are a fixed number) books containing random strings of letters (i.e., initialize a model randomly). Then, the librarian would select the first book to load into the library, pretend to read every word in every one of the random books, and after that was done, select which book to replace with the book being loaded into the library, casting the book being replaced onto the floor. The process would repeat until all of the books had been loaded... with a catch. Each time the book being thrown on the floor wasn't a random book (but was a \textit{real} book) that real book then needs to go through the process again itself. And in the end, if the library wasn't big enough to contain all the books, all the libraries would be destroyed, all the copies of books burned, and everyone would re-build bigger libraries to hold the vast and growing collection of books.
\\
\\
A solution to attribution-based control would necessarily reconfigure the library to load information based on its source (book or author). While such a solution might be challenging, if it successfully delivered capable predictions, it would necessarily do so while skipping an enormous amount of wasted computation. Instead of each library creating a collection big enough to hold the world, the vast collection of the world's books could be divided among the libraries (perhaps with some mild redundancy). Each library could then organize their books by the dewey decimal system, measuring how big each section in their library needs to be in order to hold the sections they are meant to store. After these measurements were completed, the building could be constructed, the books loaded in their proper places, and the job could be completed. 
\\
\\
And in this way, the librarian could avoid storing all the world's information in their own library, re-building libraries from scratch, reading all the books in the library over and over, loading in many copies of the same book, loading in empty or random books, and re-loading books which no longer fit on the shelves they're loading. Taken together, while ABC's definition might not appear to require an increase in computational efficiency, its definition directly requires that a vast amount of information not be processed during iterative steps in the training process, reducing the need to compute over that information. In some cases, ABC implies the elimination of iterative processes altogether.
\end{tcolorbox}
\vspace{0.5cm}

However, successful ABC would modularize the initial capability, such that some percentage of the original \$200M could be inherited from previous models and then extended with new capabilities. Given the immense costs involved, such a modularization breakthrough would constitute an enormous economic pressure. If firms pedantically chose to re-pay the cost to train their own from scratch, recreating 90\% of the modules which already exist in the market, they would incur very high costs which must correspond with very high prices to recoup those costs. Inversely, a startup which came along and inherited the 90\% produced by others, paying only for their specialization, would (all else equal) be able to charge lower prices, and win in the market. 

\newpage
From a compute efficiency standpoint, the prospect of cross-market weight-reuse translates directly into the sharing of compute costs for training AI systems. Industry analysis reveals the potential impact: no single AI provider controls more than 5.57\% of global compute capacity. Thus, since source-based partitioning could unlock this siloed compute by enabling controlled sharing of specialized knowledge, it could increase effective compute by 17.95x (100/5.57) or more because organizations would waste fewer resources re-computing features which are already commoditized in the market. Taken together, economic unbundling would plausibly drive specialization and more efficient use of compute resources in the market.

While various approaches to these inefficiencies exist, solving ABC would necessarily enable one comprehensive solution path. Note that this is not saying that ABC is the solution, merely that ABC is difficult because it would involve solving these other difficult challenges... because ABC requires that sources maintain control over their contributions while enabling rapid synthesis. Taken together, any solution to ABC solution must provide:
\begin{itemize}
\item Selective retraining instead of full rebuilding (68x+ improvement)
\item Efficient computation during training (25-50x+ improvement)
\item Reduced parameter redundancy through source-based organization (5-10x+ improvement)
\item Specialization with controlled sharing (17.95x+ improvement)
\item Organization averting catastrophic forgetting (17.95x+ improvement)
\end{itemize}

Industry analysis and empirical results suggest the combined impact could be dramatic. When these improvements compound multiplicatively, they point to potential training efficiency gains of 6+ orders of magnitude. Yet these numbers, as striking as they are, point to something more fundamental: our failure to maintain attribution in AI systems coincides with a broader acceptance of wasteful practices as inevitable. More than a technical issue, the inefficiency of AI training is a symptom of how we've structured AI computation. While other solutions may exist, attribution-based control offers one path to reimagining how AI systems learn and compute, potentially unlocking orders of magnitude more efficiency in the process.

\subsection{How Failing ABC Siloes Data (6 OOM)}

Recall that current AI models can only train on data they can access, which is dominated by data available to the public over the internet. Consequently, AI models almost certainly train on less than 1/1,000,000th of the digitized information in the world because they cannot access the other 99.9999\%, which remains hidden amongst the world's 360 million companies, 8+ billion citizens, etc. \cite{bogwasi2025business}.

While various approaches to data access exist, successful ABC would necessarily enable one compelling solution: controlled data sharing. We take as an assumption that the world's data owners have some uses in the world they would support (for which their data could be useful). We take as a second assumption that a significant portion of those sources are hidden because the incentives are not sufficient for them to support — or more crucially because the negative consequences would be too great, that their data might not just activate the uses they wish to support, but that it would also activate mis-uses (concerns regarding privacy, security, IP, competition, legal risks, etc.).
\newpage
Successful ABC would necessarily enable one particularly compelling form of data sharing. The ability for a data source to decide which AI predictions to support is (almost tautologically) the ability for a data source to enable uses while averting mis-uses. Consequently, AI empowered by ABC may activate vastly more data than is presently available. One could argue that truly successful ABC would constitute an incentive shift attracting all of the world's data to be pulled into at least some AI predictions.

The potential impact is staggering. While frontier AI models train on carefully curated web scrapes on the order of 180 TBs (and the public web plausibly less than common crawl's largest copy, 450 TBs) the world's total digital data is estimated to reach 180 zettabytes by 2025. This six-to-nine orders of magnitude difference represents all the data locked behind organizational boundaries, including some data of immense value (e.g. financial data, health data, environmental data, etc.).

RETRO and ATLAS demonstrate part of potential path forward, demonstrating a type of partial ABC at scale by training AI models which query from a database. Certain extensions (such as those suggested in this thesis) could take this further to enable full attribution-based control, and the shifting of incentives around data sharing.

\subsection{Synthesis: Attribution as a Path Forward}

The previous sections revealed two significant inefficiencies in current AI systems: 6+ orders of magnitude waste in compute and 6+ orders of magnitude in untapped data. While there may be many approaches to addressing these inefficiencies, this thesis focuses on attribution-based control as one solution path. As we've seen, solving ABC (if such a solution exists) would necessarily enable both efficient compute through source-based partitioning and broad data access through attribution preservation.

Yet this raises a deeper question: if ABC offers such compelling benefits, why don't current AI systems maintain attribution? The answer, as we'll see in the next section, lies in how neural networks fundamentally process information. The unconstrained mixing of features during training makes it impossible to partition knowledge by source, revealing our second "why": deep learning's feature mixing precludes the very partitioning that ABC requires.

\section{Second Why: Deep Learning's Feature Mixing Precludes Partitioning}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{1411.3146/abc_ch2_4_0_v5.png}
\caption{Subset of conceptual graph highlighting the Second Why and focus of this section.}
\label{fig:broad_listening}
\end{figure}

The previous section revealed how solving attribution-based control would necessarily enable massive data and compute gains in AI systems. Yet this raises a deeper question: why do current AI systems fail to maintain attribution in the first place? The answer lies in deep learning's foundational premise: algorithms should learn everything from scratch through layers of (largely) unrestricted feature mixing on raw data \cite{Goodfellow-et-al-2016}.

This commitment to unrestricted learning manifests in how neural networks fundamentally process information. Through operations that combine and mix information at every step (from layer computations to weight updates to knowledge accumulation) neural networks create increasingly complex representations of patterns in their training data. While this flexibility enables powerful pattern recognition, it creates a fundamental problem: features become stored in a disorganized, obfuscated way within the deep learning model... a black box.
\newpage
Consider what happens when a neural network learns to recognize cats. Rather than storing clear, interpretable features like "pointy ears" or "whiskers", the network distributes this knowledge across its weights in complex, entangled patterns \cite{le2013building}. This unrestricted mixing of features makes post-hoc source-based partitioning impossible (at the present moment):

\begin{itemize}
\item Features can't be attributed to specific sources (preventing data control)
\item Knowledge can't be updated independently (requiring full retraining)
\item Computation can't be selectively activated (forcing dense inference)
\item Resources can't be efficiently shared (blocking specialization)
\end{itemize}

The research community has made extensive efforts to address these limitations. Recent work has attempted to trace predictions back to training data through influence functions, remove specific datapoints' influence through machine unlearning, and develop various attribution methods to reverse engineer the source-prediction relationship \cite{nguyen2024surveymachineunlearning}. Yet despite these attempts, both influence functions and unlearning remain unsolved challenges in the literature. So far, the relationship between sources and predictions has been irreversibly compressed during training, and no amount of post-training intervention has successfully restored these lost connections.

The consequences are severe. New models like GPT-5 cannot inherit features from predecessors like GPT-4... they must relearn basic patterns from scratch. Even during inference, models must activate vast parameter spaces for every prediction, unable to pre-identify which features are relevant (and only forward propagate those features). These inefficiencies aren't mere implementation details that clever algorithms might solve. They stem from something more fundamental about how deep learning processes information.

Yet this raises an even deeper question: why does feature mixing obfuscate attribution-based control? The answer, as we'll see in the next section, traces back to deep learning's most basic mathematical operation and how it fundamentally precludes the partitioning that ABC requires.

\vspace{0.5cm}
\begin{tcolorbox}[
enhanced,
breakable,
colback=white,
colframe=gray,
boxrule=0.5pt,
left=8pt,
right=8pt,
top=8pt,
bottom=8pt,
title=A Library Analogy]
Consider a library wherein all of the books have had their covers removed, their table of contents erased, and their chapters torn out and shuffled amongst all the books. Consequently, when someone wants to answer a specific question, they have to read through the entire library searching for relevant information for their query.
\\
\\
Deep learning stores information in a similar way, with so-called \textit{distributed representations} spreading concepts across many neurons... each of which is unlabeled (i.e. "hidden"). Far from an accident, this form of learning is at the center of deep learning's core philosophy, the unrestricted learning of dense, hidden features.
\end{tcolorbox}
% \vspace{0.5cm}

\section{Third Why (Root Cause): Addition of Source-Separable Concepts}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{1411.3146/abc_ch2_5_0_v5.png}
\caption{Subset of conceptual graph highlighting the Third Why and the focus of this section.}
\label{fig:broad_listening}
\end{figure}

The previous section revealed how deep learning's feature mixing precludes the partitioning required for attribution-based control. Yet this raises our final "why": what makes this mixing fundamentally irreversible? The answer lies in deep learning's most basic mathematical operation: addition.

Addition might seem like an implementation detail, but it fundamentally prevents recovering source information. When values combine through addition, the result does not uniquely determine its inputs—multiple distinct source combinations produce identical outputs:

\begin{definition}[Non-Injectivity of Addition]\label{def:addition-problem}
Addition is not injective: for any sum $y$, there exist infinitely many distinct pairs $(x_1, x_2)$ and $(x'_1, x'_2)$ such that $x_1 + x_2 = y = x'_1 + x'_2$ where $(x_1, x_2) \neq (x'_1, x'_2)$.
\end{definition}

This non-injectivity means that observing the sum provides no information about which specific sources contributed. Consider the contrast with concatenation:

\begin{align*}
\text{Concatenation preserves sources:} & \\
\text{"1"} \oplus \text{"6"} & = \text{"16"} \\
\text{"2"} \oplus \text{"5"} & = \text{"25"} \\
\text{(distinct inputs } & \rightarrow \text{ distinct outputs)} \\
\\
\text{Addition erases them:} & \\
1 + 6 & = 7 \\
2 + 5 & = 7 \\
\text{(distinct inputs } & \rightarrow \text{ identical outputs)}
\end{align*}

When partitions are known, concatenation of numbers is injective; different inputs produce different outputs, allowing source recovery. Addition is not; the output $7$ could arise from $1+6$, $2+5$, $3+4$, $0+7$, or infinitely many other combinations. This non-injectivity is the mechanism through which deep neural networks erase attribution information: once gradients from different sources are summed into shared parameters, no function of those parameters can recover which sources contributed what information.

And neural networks use addition extensively: combining features between layers, aggregating gradients during backpropagation, and updating weights during training. Each addition irreversibly combines information, destroying the provenance of where that information came from and how it was interpreted.

A natural solution might seem possible: why not just track every operation during training ... while also doing these additions? Unfortunately, this approach fails for three fundamental reasons:

First, in models like GPT-3, through forward and backward propagation, each example's information eventually touches every weight in the network. Even tiny weight changes alter how future examples flow through the network, creating cascading effects that can amplify initially small influences (related: vanishing and exploding gradients \cite{hochreiter1998vanishing, hanin2018neural}).

Second, these influences compound exponentially and recursively, creating higher order all-to-all relationships between inputs and weights as they do. Consider the mathematics of weight updates for a weight update function $g$, weights and data at time $t$ as $w_t$ and $x_t$:
\begin{align*}
w_{t+1} &= g(w_t, x_t) \\
w_{t+2} &= g(g(w_t, x_t), x_{t+1}) \\
w_{t+3} &= g(g(g(w_t, x_t), x_{t+1}), x_{t+2})
\end{align*}

The number of potential attribution paths grows as $\Omega(w \cdot n)^t$, where $w$ is the number of weights, $n$ is the number of examples, and $t$ is the number of steps.

Third, this exponential growth makes exact attribution computationally intractable. The most capable language models frequently leverage just under 1\% of their parent organization's AI training budget (see Appendix I and II for details). Thus, tracking the full web of dependencies (every interaction, every update, every influence path) would require many orders of magnitude more compute than is available to the largest tech firms.

These three barriers (information loss through addition, exponential propagation of influences, and computational intractability) combine to create a fundamental limitation. No amount of clever engineering can fully recover what addition has destroyed. This mathematical reality explains why attempts at machine unlearning and influence functions remain fundamentally limited: they try to reconstruct what addition has already erased.
\\
\\
\textbf{The Root Problem:} The implications are significant. Without the ability to track sources through training, we cannot provide the attribution that ABC requires. Without attribution, we cannot enable the partitioned sharing and use of data and compute that could unlock orders of magnitude more AI resources. Addition itself blocks the very data and compute gains described earlier in this chapter, and holds up the many problems described in Chapter 1.

\vspace{0.5cm}
\begin{tcolorbox}[
enhanced,
breakable,
colback=white,
colframe=gray,
boxrule=0.5pt,
left=8pt,
right=8pt,
top=8pt,
bottom=8pt,
title=A Library Analogy]
Consider a library wherein all of the books have had their covers removed, their table of contents erased, and individual sentences on each page torn out into their own strips. Now imagine that each word in each strip is converted into a number "aardvark = 1", "abernathe = 2", and so forth. And then imagine that each of these strips from the whole library is shuffled around, and groups of strips are placed back in the coverless books. Yet, instead of each strip being glued in place, it is first combined with many other strips, adding their respective numbers together. Consequently, when someone wants to answer a specific question, they have to read through the entire library searching for relevant information for their query,  first by converting their query into numbers and then attempting to match it to numbers found in the books.
\\
\\
Deep learning stores information in a similar way, with so-called \textit{distributed representations} spreading concepts across many neurons... each of which is unlabeled (i.e. "hidden"). Far from an accident, this form of learning is at the center of deep learning's core philosophy, the unrestricted learning of dense, hidden features which are formed through many successive additions.
\end{tcolorbox}
\vspace{0.5cm}

\newpage
\section{Third Hypothesis (Root Solution): Concatenating Along Natural Boundaries in Data Sources Enables Attribution}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{1411.3146/abc_ch2_6_0_v6.png}
\caption{Subset of concept graph highlighting the Third Hypothesis and focus of this section.}
\label{fig:broad_listening}
\end{figure}

% TODO: somewehre in here make the point that rapid synthesis is possible in part because deep learning is additive (i.e. consider the attention mechanism as a whole... or the implications of the reverse curse... or links to ngram langauge models).

We now continue by constructing a hypothesis (the ``Third Hypothesis") which corresponds to the Third Why, which will support the Second Hypothesis addressing the Second Why and so forth. The previous sections revealed how addition in deep learning creates a fundamental barrier to attribution. Yet examining why addition fails suggests a testable hypothesis: can we significantly reduce the use of addition, perhaps swapping it with concatenation?

Deep learning's central hypothesis would suggest we can't, that features need to densely mix (using addition) in order to learn the powerful correlations and representations that give deep learning its predictive capability. However, presumably deep learning maps multiple distinct concepts into a shared feature when those two concepts are related \cite{krizhevsky2012imagenet}. For example, a deep learning model which classifies images might have features which detect ears, fur, and eyes — features which would be useful for modeling many different animals which possess these related concepts \cite{zeiler2014visualizing}. However, as these features are not laid out in advance, deep learning needs to densely mix its features in order to discover these related patterns across training datapoints. That is to say, perhaps over-zealous dense feature mixing is more about training than inference.

Yet, perhaps deep learning is over-zealous in its feature mixing, opening itself to representation power which could mix any feature with any other when in the real-world not all concepts are closely related. That is to say, not all concepts require that level of general representation power. Perhaps some concepts are actually unrelated to one another, such that some proportion of dense feature mixing in deep learning is superfluous.
\newpage
That said, some concepts are densely mixed, while others are clearly less so. Some information patterns appear ubiquitously: basic rules of grammar that structure language, logical operations that appear in reasoning, morphological patterns which make up words, edges and corners in images, etc. Elements like these are frequently reused across almost every data point, appearing in many billions of documents, images, audio, and videos. Such dense patterns suggest unrestricted mixing through addition may be appropriate for a core subset of features. Their ubiquity also makes attribution less critical; they represent shared computational tools rather than source-specific claims about the world. While perhaps not formally stated, noted researcher Andrej Karpathy recently suggested a similar concept when referring to a future LLM ``cognitive core'':
\vspace{5px}
\begin{tcolorbox}[
enhanced,
breakable,
colback=gray!5,
colframe=gray!50,
boxrule=0.5pt,
left=10pt,
right=10pt,
top=10pt,
bottom=10pt,
fontupper=\itshape]

The race for LLM ``cognitive core''—a few billion param model that maximally sacrifices encyclopedic knowledge for capability. It lives always-on and by default on every computer as the kernel of LLM personal computing. Its features are slowly crystalizing:
\begin{itemize}
\item Natively multimodal text/vision/audio at both input and output.
\item Matryoshka-style architecture allowing a dial of capability up and down at test time.
\item Reasoning, also with a dial. (system 2)
\item Aggressively tool-using.
\item On-device finetuning LoRA slots for test-time training, personalization and customization.
\item Delegates and double checks just the right parts with the oracles in the cloud if internet is available.
\end{itemize}

It doesn't know that William the Conqueror's reign ended in September 9 1087, but it vaguely recognizes the name and can look up the date. It can't recite the SHA-256 of empty string as e3b0c442..., but it can calculate it quickly should you really want it.

What LLM personal computing lacks in broad world knowledge and top tier problem-solving capability it will make up in super low interaction latency (especially as multimodal matures), direct / private access to data and state, offline continuity, sovereignty (``not your weights not your brain''). i.e. many of the same reasons we like, use and buy personal computers instead of having thin clients access a cloud via remote desktop or so.

\vspace{5pt}
\hfill --- Andrej Karpathy\footnote{\url{https://x.com/karpathy/status/1938626382248149433}}
\end{tcolorbox}
\vspace{5px}



In contrast, perhaps most information is encyclopedic and appears sparsely: specific facts about the world, domain expertise in particular fields, claims made by individual sources, etc. The capital of France, the rules of chess, statistics about pizza... each appears in distinct contexts with limited overlap. As Chomsky noted in linguistics \cite{chomsky2014aspects}, while we use common patterns to express all knowledge, the knowledge itself often remains naturally partitioned by topic, and when documents are topic specific... by source.

\newpage
Let us assume for a moment that this is true. If so, then in theory some section of a neural network could be made sparse, namely the part of the neural network which stores concepts which are largely decoupled from the rest of a neural network's knowledge (facts, domain expertise, semantic information, etc.). Perhaps this section could use less addition (and more concatenation), enabling sparsity which could drive attribution. Meanwhile, another part of the neural network might need to remain dense, storing and synthesizing concepts which are ubiquitous across a statistical distribution (logic, reasoning, syntax, etc.).

The key question: how would one go about training a neural network which successfully partitioned information into sparse and dense sections?

A key insight of this chapter is that techniques from privacy-preserving machine learning, particularly differential privacy (DP) \cite{dwork2006calibrating}, provide a principled way to measure and control which features benefit from dense mixing versus sparse representation. Differential privacy quantifies how much a model's outputs depend on any individual training example:

\begin{definition}[($\epsilon$, $\delta$)-Differential Privacy]\label{def:diff-privacy}
A randomized mechanism $\mathcal{M}: \mathcal{D} \to \mathcal{R}$ satisfies ($\epsilon$, $\delta$)-differential privacy if for all adjacent datasets $D, D' \in \mathcal{D}$ (differing in one example) and all subsets of outputs $S \subseteq \mathcal{R}$:
\begin{align*}
\Pr[\mathcal{M}(D) \in S] \leq e^\epsilon \cdot \Pr[\mathcal{M}(D') \in S] + \delta
\end{align*}
where small $\epsilon$ indicates strong privacy—outputs barely depend on any individual example.
\end{definition}

The parameter $\epsilon$ provides a quantitative measure of individual example influence on outputs. This same measure can serve three distinct control objectives:

\begin{definition}[Three Regimes of Influence Control]\label{def:influence-control}
For a mechanism $\mathcal{M}$, examples $e$, and thresholds $0 < \tau_{\text{min}} < \tau_{\text{max}}$:
\begin{itemize}
\item \textbf{Privacy (constrain influence):} Enforce $\epsilon_e < \tau_{\text{min}}$ for all examples, guaranteeing that individual examples cannot be distinguished through their influence on outputs
\item \textbf{Measurement (track influence):} Compute $\epsilon_e$ for each example, enabling quantification of which examples influence which outputs, without enforcing bounds
\item \textbf{Attribution (ensure influence):} Enforce $\epsilon_e > \tau_{\text{max}}$ for specified examples, guaranteeing that certain examples have measurable influence on outputs
\end{itemize}
\end{definition}

These three regimes serve different stakeholder needs. Data owners might seek differential privacy (ensuring their data cannot be identified in model outputs), \textit{differential measurement} (understanding their data's contribution), or \textit{differential attribution} (guaranteeing their data influences predictions). Users might seek privacy (preventing their queries from revealing information) or attribution (ensuring they can identify which sources influenced their results). The same mathematical framework (DP's $\epsilon$ parameter) enables each form of control.

Given differential privacy's use in deep learning (e.g., \cite{Abadi_2016}), this framework suggests an architectural insight: a neural network serving diverse stakeholders requires the capability to enforce different $\epsilon$ regimes for different information. Information requiring privacy (small $\epsilon$) can pass through privacy-constrained layers with dense mixing via addition. Information requiring attribution (large $\epsilon$) must route through pathways preserving source identity via sparse concatenation. Information requiring measurement sits between these extremes, with $\epsilon$ tracked but not bounded. The model's optimization pressure, when constrained to respect these different $\epsilon$ regimes, might naturally partition information accordingly. This framework suggests two testable predictions:

First, features with high source-specific attribution should cluster naturally by source, while features with low attribution should appear consistently across sources. Recent work with RETRO and ATLAS provides initial evidence for this prediction, showing how knowledge naturally separates into general computational patterns (the Transformer reading from a database of vectors, similar to Karpathy's ``cognitive core'') and source-specific information (the database of vectors).

Second, respecting these natural boundaries through architectural choices might enable more efficient computation. If most information is sparsely distributed, then forcing it through dense addition operations wastes significant compute. Models that preserve sparse patterns through concatenation while sharing dense patterns through addition should achieve better computational efficiency. Again RETRO and ATLAS provide early evidence of this ability, wherein information within their vector database is concatenated (i.e. in different rows of the database), while information in the neural network consuming from the database is densely stored within Transformer weights.

The next section builds on this hypothesis, suggesting how these natural boundaries in information usage could enable new approaches to AI development. If correct, this could resolve the tension between deep learning's powerful pattern recognition and the need for attribution-based control in AI systems.

\vspace{0.5cm}
\begin{tcolorbox}[
enhanced,
breakable,
colback=white,
colframe=gray,
boxrule=0.5pt,
left=8pt,
right=8pt,
top=8pt,
bottom=8pt,
title=A Library Analogy]
Consider a library wherein all of the books have had their covers removed, their table of contents erased, and individual sentences on each page torn out into their own strips. Now imagine that each word in each strip is converted into a number "aardvark = 1", "abernathe = 2", and so forth. And then imagine that each of these strips from the whole library is shuffled around, and groups of strips are placed back in the coverless books. Yet, instead of each strip being glued in place, it is first combined with many other strips, adding their respective numbers together. Consequently, when someone wants to answer a specific question, they have to read through the entire library searching for relevant information for their query, first by converting their query into numbers and then attempting to match it to numbers found in the books.
\\
\\
Following the analogy, differential attribution ensures that each strip of numbers remains separated (instead of added) into each other strip, and preserved within the same book as before, partitioning data in a way which might be indexed by source or topic.
\end{tcolorbox}
\vspace{0.5cm}
\newpage







\section{Second Hypothesis: From Deep Learning to Deep Voting with AI Recycling and Src-specific Intelligence Budgets} \label{intel_budget}


\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{1411.3146/abc_ch2_7_0_v5.png}
\caption{Subset of concept graph highlighting the Second Hypothesis and focus of this section.}
\label{fig:broad_listening}
\end{figure}

The previous section revealed how privacy mechanisms might naturally separate dense from sparse information patterns. Yet this theoretical insight raises a practical question: how do we actually \textit{implement} the measurement and control regimes we defined? The definitions in the previous section assumed worst-case bounds (requiring that $\epsilon$ constraints hold for \textit{all} pairs of neighboring datasets). But attribution-based control requires the opposite: not uniform bounds across all sources, but \textit{source-specific} control where each source can have different influence levels matching different stakeholder needs.

\subsection{From Worst-Case to Individual Differential Privacy}

Standard differential privacy enforces worst-case bounds across all possible pairs of neighboring datasets. Consider a dataset where most individuals' data appears in common patterns, but one individual has highly unique data. Worst-case differential privacy must constrain the entire mechanism based on that one outlier, reducing utility for everyone—even though the mechanism only ever operates on the \textit{actual} dataset, not all possible datasets. Individual differential privacy \cite{idp} provides a more nuanced approach by focusing privacy guarantees on the \textit{actual} dataset rather than all possible datasets:
\newpage
\begin{definition}[Individual Differential Privacy]\label{def:individual-dp}
Given a dataset $D$, a response mechanism $\mathcal{M}(\cdot)$ satisfies $\epsilon$-individual differential privacy ($\epsilon$-iDP) if, for any dataset $D'$ that is a neighbor of $D$ (differing in one example), and any $S \subset \text{Range}(\mathcal{M})$:
\begin{align*}
\exp(-\epsilon) \cdot \Pr[\mathcal{M}(D') \in S] &\leq \Pr[\mathcal{M}(D) \in S] \\
&\leq \exp(\epsilon) \cdot \Pr[\mathcal{M}(D') \in S]
\end{align*}
\end{definition}

The crucial difference from standard DP: $D$ refers to the \textit{actual} dataset being protected, while $D'$ ranges over $D$'s neighbors. Standard DP requires indistinguishability for \textit{any} pair of neighbors; individual DP requires indistinguishability only between the actual dataset and its neighbors. This asymmetry allows the mechanism to adjust noise based on properties of the actual dataset (such as its local sensitivity) rather than worst-case properties across all possible datasets.

This enables tighter privacy guarantees in practice. When the actual dataset has low local sensitivity (changing any individual barely affects outputs), individual DP requires minimal noise. When the actual dataset has high local sensitivity (some individuals significantly affect outputs), individual DP adds proportionate noise. Standard DP must always assume worst-case sensitivity regardless of the actual dataset's properties.

\subsection{From Individual Privacy to Individual Attribution}

Just as we extended standard differential privacy to define attribution regimes in the previous section, we can extend individual differential privacy to define individual attribution. The key insight remains the same: privacy and attribution are opposite ends of the same $\epsilon$ spectrum, now applied to the actual dataset rather than worst-case bounds.

\begin{definition}[Individual Differential Privacy vs. Attribution]\label{def:individual-privacy-attribution}
For a mechanism $\mathcal{M}$, actual dataset $D$, and threshold $\tau > 0$:
\begin{itemize}
\item \textbf{Individual Privacy:} Enforce $\epsilon < \tau$ for all neighbors $D'$ of $D$, guaranteeing that any individual's data in the dataset cannot be distinguished through its influence on outputs.
\item \textbf{Individual Measurement:} Compute $\epsilon$ for the actual dataset, enabling quantification of influence without enforcing bounds.
\item \textbf{Individual Attribution:} Enforce $\epsilon > \tau$ for the actual dataset, guaranteeing that individuals in the actual dataset have measurable influence on outputs.
\end{itemize}
\end{definition}

\subsection{From Examples to Sources: Group Differential Privacy}

But attribution-based control requires more than individual-level bounds, it requires \textit{source-level} control. Individual differential privacy protects single examples in the actual dataset, but data sources typically contribute many examples. Consider a medical AI trained on data from 100 hospitals: each hospital contributes thousands of patient records. ABC needs to measure and control influence at the hospital level, not just the individual patient level. The differential privacy literature addresses this through group differential privacy \cite{dwork2014algorithmic}, which extends privacy guarantees from individuals to groups of records:

\begin{definition}[Group Differential Privacy]\label{def:group-dp}
A randomized mechanism $\mathcal{M}: \mathcal{D} \to \mathcal{R}$ satisfies $\epsilon$-group differential privacy for groups of size $k$ if for all datasets $D, D'$ differing in at most $k$ records and all subsets of outputs $S \subseteq \mathcal{R}$:
\begin{align*}
\Pr[\mathcal{M}(D) \in S] \leq e^\epsilon \cdot \Pr[\mathcal{M}(D') \in S]
\end{align*}
\end{definition}

Standard differential privacy automatically provides group privacy through composition: if a mechanism satisfies $\epsilon$-DP for individuals, it satisfies $k\epsilon$-DP for groups of size $k$. However, this is a worst-case bound that assumes all $k$ individuals have maximal independent influence. Group differential privacy makes group protection explicit, enabling tighter analysis when group members have correlated data or when mechanisms can exploit group structure.

We can combine group differential privacy with individual differential privacy to obtain source-level control calibrated to the actual dataset. When we partition a dataset by sources $D = \bigcup_{s \in S} D_s$, we treat each source as a group and apply individual DP at the group level:

\begin{definition}[Source-Level Individual Differential Privacy]\label{def:source-idp}
Given a dataset $D$ partitioned by sources $D = \bigcup_{s \in S} D_s$, a response mechanism $\mathcal{M}(\cdot)$ satisfies $\epsilon_s$-individual differential privacy for source $s$ if, for any dataset $D'$ differing from $D$ only in source $s$'s data (i.e., $D' = D_{-s} \cup D'_s$ where $|D'_s| = |D_s|$), and any $S \subset \text{Range}(\mathcal{M})$:
\begin{align*}
\exp(-\epsilon_s) \cdot \Pr[\mathcal{M}(D') \in S] &\leq \Pr[\mathcal{M}(D) \in S] \\
&\leq \exp(\epsilon_s) \cdot \Pr[\mathcal{M}(D') \in S]
\end{align*}
\end{definition}

This combines group DP's extension to multiple records with individual DP's calibration to the actual dataset. Each source $s$ receives its own privacy parameter $\epsilon_s$ measuring influence on the actual dataset $D$, not worst-case influence across all possible datasets. A hospital contributing highly unique medical data might have large $\epsilon_s$ for the actual dataset, while a hospital contributing common patterns might have small $\epsilon_s$ (without forcing all hospitals to share worst-case bounds).

\subsection{From Source-Level Privacy to Source-Level Attribution}

Just as individual DP extends to attribution regimes (privacy, measurement, attribution), source-level individual DP extends to source-level attribution. We can quantitatively measure each source's influence on the actual dataset:

\begin{definition}[Source-Level Individual Differential Attribution]\label{def:source-attribution}
Let $\mathcal{A}$ be a randomized algorithm and let $D$ be a dataset partitioned by sources $D = \bigcup_{s \in S} D_s$. For any source $s$ and prediction $f$, the individual differential attribution of source $s$ on function $f$ is:
\begin{align*}
\text{Attribution}_\alpha(s, f) &= D_\alpha^\leftrightarrow(\mathcal{A}(D)\|\mathcal{A}(D^{-s})) \\
&= \max\{D_\alpha(\mathcal{A}(D)\|\mathcal{A}(D^{-s})), D_\alpha(\mathcal{A}(D^{-s})\|\mathcal{A}(D))\}
\end{align*}
where $D_\alpha$ is the Rényi divergence of order $\alpha$\footnote{Rényi divergence generalizes both KL divergence ($\alpha \to 1$) and max divergence ($\alpha \to \infty$), providing tractable computation with tunable sensitivity-privacy tradeoffs.}, $D^{-s} = D \setminus D_s$ represents the dataset with source $s$ removed, and $\mathcal{A}(D)$ represents the output distribution of algorithm $\mathcal{A}$ on dataset $D$.
\end{definition}

This measures each source's attribution relative to the actual dataset $D$ by quantifying how predictions change when that specific source is included versus excluded. Unlike standard group DP (which provides worst-case $k\epsilon$ bounds for all groups of size $k$), source-level individual attribution calibrates to actual dataset properties: a medical research paper might have high attribution for predictions in its domain (large divergence from $D^{-s}$) but negligible attribution for unrelated queries (small divergence). 

This enables the diverse source-level control ABC requires. Consider the medical AI trained on 100 hospitals: some hospitals can demand privacy (enforce small $\epsilon_s$ calibrated to their actual data), others can guarantee attribution (enforce large $\epsilon_s$ ensuring measurable influence), others can simply track their contribution (measure $\epsilon_s$ without bounds). Worst-case group DP cannot accommodate this diversity; it forces uniform $k\epsilon$ bounds across all groups of size $k$. Source-level individual attribution provides per-source control adjusted to the actual dataset, precisely what attribution-based control requires.

\subsection{Intelligence Budgets: Implementing Individual Source Control}

This source-level individual attribution measure enables a practical control mechanism: \textit{intelligence budgets}. Rather than simply measuring influence after the fact, we can actively control how much each source influences predictions through architectural routing decisions.

\begin{definition}[Intelligence Budgets via Forward Pass Weighting]\label{def:intelligence-budgets}
The model has two types of parameterized functions:
\begin{align*}
g_s(\cdot; \Theta_{\text{semantic}}[s]) &: \text{source-specific function for source } s \\
f(\cdot; \Theta_{\text{syntactic}}) &: \text{shared function across all sources}
\end{align*}

Information flows through two stages:
\begin{align*}
\text{Stage 1 (Semantic): } \quad s_s &= g_s(x_s; \Theta_{\text{semantic}}[s]) \\
\text{Stage 2 (Syntactic): } \quad h(x) &= f\left([x_1, \ldots, x_{|S|}, \gamma[1] \cdot s_1, \ldots, \gamma[|S|] \cdot s_{|S|}]; \Theta_{\text{syntactic}}\right)
\end{align*}
where $\gamma[s] \in [0,1]$ is a per-source scaling weight and $[\cdot]$ denotes concatenation of all inputs.

The intelligence budget $B(s)$ bounds source $s$'s influence:
\begin{align*}
\text{Attribution}_\alpha(s, h) &\leq B(s)
\end{align*}

Setting $\gamma[s] \approx 0$ enforces small $B(s)$ (privacy regime) by preventing semantic contributions. Setting $\gamma[s] \approx 1$ allows large $B(s)$ (attribution regime) by preserving semantic identity.
\end{definition}

Each $g_s$ can be an arbitrary neural network (e.g., a deep learning model) with parameters $\Theta_{\text{semantic}}[s]$ specific to source $s$. The syntactic function $f$ can also be an arbitrary neural network (e.g., a Transformer) with shared parameters $\Theta_{\text{syntactic}}$. The function $f$ receives all raw inputs and all scaled semantic outputs concatenated together, and can mix them however it wants. The parameter $\gamma[s]$ directly controls source $s$'s intelligence budget by scaling how much of its semantic output $s_s = g_s(x_s)$ contributes to the final prediction.

When $\gamma[s] = 0$, source $s$ contributes only through its raw input $x_s$ concatenated with others. The syntactic function $f$ can mix these raw inputs however it wants. Differential privacy constraints on $f$ ensure this mixing prevents source identification, enforcing the privacy regime. When $\gamma[s] = 1$, source $s$ contributes its full semantic representation $s_s$ with identity preserved through concatenation, enabling attribution tracking and enforcing the attribution regime. Intermediate values of $\gamma[s]$ enable the measurement regime.

This budgeting mechanism provides flexible, context-dependent control matching diverse stakeholder needs. A source demanding privacy sets $\gamma[s] = 0$. A source guaranteeing attribution sets $\gamma[s] = 1$. A source tracking contribution uses intermediate $\gamma[s]$ and measures resulting influence. The model simultaneously satisfies all these requirements by setting different $\gamma[s]$ values for different sources.

\subsection{From Concatenation and IDP to Deep Voting}

The deep voting framework reveals a two-dimensional spectrum in machine learning architectures by introducing a second control parameter $\lambda \in [0,1]$ that governs the overall balance between semantic and syntactic capacity:

\begin{definition}[Capacity Allocation ($\lambda$)]\label{def:capacity-allocation}
The parameter $\lambda$ determines what fraction of total model capacity is allocated to each function:
\begin{align*}
|\Theta_{\text{syntactic}}| &= \lambda \cdot |\Theta_{\text{total}}| \\
\sum_{s} |\Theta_{\text{semantic}}[s]| &= (1-\lambda) \cdot |\Theta_{\text{total}}|
\end{align*}
where $|\Theta|$ denotes parameter count.
\end{definition}


\vspace{0.5cm}
\begin{tcolorbox}[
enhanced,
breakable,
colback=white,
colframe=gray,
boxrule=0.5pt,
left=8pt,
right=8pt,
top=8pt,
bottom=8pt,
title=Deep Voting Analogy: Individual Differential Privacy via Adaptive Filtering]

Feldman and Zrnic's individual differential privacy framework \cite{feldman2020individual} provides a concrete example of intelligence budgets implemented through adaptive filtering rather than architectural routing.
\\
\\
\textbf{Architecture:} Their approach in Example 2.7 uses $\lambda = 1$ (pure syntactic processing) with all capacity in shared parameters $\Theta_{\text{syntactic}}$. No semantic section exists ($\Theta_{\text{semantic}}[s] = \emptyset$), meaning all sources contribute only through raw inputs: $h(x) = f([x_1, \ldots, x_n]; \Theta_{\text{syntactic}})$ where $f$ adds Gaussian noise for privacy.
\\
\\
\textbf{Intelligence Budgets via Filtering:} Rather than controlling $\gamma[s]$ continuously, they implement binary filtering. At each time step $t$, compute the individual privacy loss $\rho_t^{(i)} = \frac{\alpha \|\bar g_t(X_i)\|_2^2}{2\sigma^2}$ where $\bar g_t(X_i)$ is the clipped gradient. Source $i$ remains active while $\sum_{j=1}^t \rho_t^{(i)} \leq B$, then gets dropped (equivalent to setting $\gamma[i] = 0$ for all future steps).
\\
\\
\textbf{Key Insight:} The intelligence budget $B(i)$ is implicitly determined by realized gradient norms. For Lipschitz functions with coordinate sensitivity $L_i$, the bound becomes $B(i) \approx \frac{\alpha L_i^2 \|\phi(X_i)\|^2}{2\sigma^2}$. Sources with small gradients (low sensitivity) can participate longer before exceeding their budget.
\\
\\
\textbf{Contrast with Deep Voting:} Feldman & Zrnic achieve privacy (small $\epsilon$) by dropping high-influence examples entirely, routing all remaining examples through privacy-constrained shared processing. Deep voting generalizes this by: (1) introducing a semantic section ($\lambda < 1$) that preserves source identity, (2) allowing continuous control ($\gamma[s] \in [0,1]$) rather than binary drop/keep decisions, and (3) enabling attribution regime where large $\epsilon$ is desirable. Individual DP represents the special case where $\lambda = 1$, $\gamma[s] \in \{0,1\}$ (binary), and all sources demand privacy.
\\
\\
\textbf{Adaptive Composition:} The Feldman & Zrnic result on adaptive composition with data-dependent privacy parameters ($\sum_t \rho_t^{(i)} \leq B \Rightarrow (\alpha, B)$-RDP) directly parallels our intelligence budget composition: both handle the challenge that influence parameters depend on previous outputs, enabling provable bounds even under adaptive computation.

\end{tcolorbox}

With these mechanisms in place, we can return to the implications of such a system for providing attribution-based control: the potential to dramatically increase the amount of data and compute available for training AI systems. By providing clear mechanisms for measuring source influence ($\text{Attribution}_\alpha(s, h)$), bounding influence when needed ($\gamma[s] = 0$ for privacy), and guaranteeing influence when required ($\gamma[s] = 1$ for attribution), deep voting might enable the safe use of orders of magnitude more training data.
\vspace{0.5cm}

\begin{tcolorbox}[
enhanced,
breakable,
colback=white,
colframe=gray,
boxrule=0.5pt,
left=8pt,
right=8pt,
top=8pt,
bottom=8pt,
title=A Library Analogy]
Consider a library wherein all of the books have had their covers removed, their table of contents erased, and individual sentences on each page torn out into their own strips. Now imagine that each word in each strip is converted into a number "aardvark = 1", "abernathe = 2", and so forth. And then imagine that each of these strips from the whole library is shuffled around, and groups of strips are placed back in the coverless books. Yet, instead of each strip being glued in place, it is first combined with many other strips, adding their respective numbers together. Consequently, when someone wants to answer a specific question, they have to read through the entire library searching for relevant information for their query — first by converting their query into numbers and then attempting to match it to numbers found in the books.
\\
\\
Following the analogy, differential attribution ensures that each strip of numbers remains separated (instead of added) into each other strip,  preserved within the same book as before, partitioning information in a way which might be indexed by source (or topic). It further provides a staff of librarians who know how to read relevant information and synthesize them, each according to a topic that librarian happens to be familiar with. Taken together, a customer of a library can leverage one librarian to index into the appropriate shelf, identify the right book, and the right snippets of that book, and then ask a subset of the staff of librarians who are experts on that topic to properly interpret those snippets.
\end{tcolorbox}

\section{First Hypothesis: ABC and 6+ Orders of Magnitude more Data/Compute}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{1411.3146/abc_ch2_8_0_v5.png}
\caption{Subset of concept graph highlighting the First Hypothesis and focus of this section.}
\label{fig:broad_listening}
\end{figure}

The deep voting framework reveals a two-dimensional spectrum in machine learning architectures. Consider how different parameter settings affect the model's behavior:

At $(\lambda,\gamma) = (1,0)$, we find pure deep learning with maximum compression. These systems, like GPT-4, use only shared parameters with basic bounds on attribution. They achieve powerful feature learning but sacrifice attribution clarity. At $(\lambda,\gamma) = (0,1)$, we find pure partition-based learning, like federated systems, which maintain group privacy but limit cross-source learning. At $(\lambda,\gamma) = (0,0)$, we find pure source-specific learning systems like k-nearest neighbors, providing perfect attribution through direct tracking but losing the benefits of parameter sharing.

Far from being theoretical abstractions, these points represent real systems in production today, each making explicit tradeoffs between learning power (as measured by model performance), attribution clarity (as measured by influence tracking), and computational efficiency (as measured by FLOP counts).

\newpage

Current systems tend to choose a fixed point on this spectrum, making an explicit tradeoff between these competing forces. Deep voting formalizes a more continuous alternative: dynamic adjustment of these parameters (perhaps based on empirical information). When data shows high redundancy across sources (like common language patterns), higher $\lambda$ values enable efficient parameter sharing. When sources contain unique information (like proprietary research), lower values maintain clearer attribution and better reward data owners for their novelty and innovation.

The empirical evidence presented earlier in this chapter paints a stark picture of current limitations. AI systems today access less than one millionth of the world's digital data, 180 zettabytes versus roughly 180 terabytes for the largest known training sets. Even the largest AI firms utilize less than 5.57\% of available compute capacity. Meanwhile, techniques like RETRO/ATLAS have demonstrated 25-50x parameter efficiency gains while maintaining performance, recent compression research shows 5-10x reduction in parameter counts without accuracy loss, and catastrophic forgetting work suggests a 40x drop in model size is attainable.

This tension yields our central empirical question: Can deep voting meaningfully shift the Pareto frontier between learning power, attribution clarity, and computational efficiency? Success could mean unlocking orders of magnitude more training data through clear attribution mechanisms. Success would enable dramatic reductions in computational waste through dynamic parameter sharing. Perhaps most importantly, success would create a path toward safe compute sharing through reliable influence tracking; the sparse, concatenated section could be distributed across multiple computers.

The stakes are significant. If deep voting succeeds, it could unlock another 6+ orders of magnitude of training data and compute productivity. If it fails, we may remain constrained by the fundamental limitations of current architectures. The next section examines the empirical evidence, providing an early view of whether deep voting might achieve the theoretical benefits suggested by its mathematical framework.

\section{Empirical Evidence: Does the Pareto-Tradeoff Move?}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{abc_ch2_9_0_v4.png}
\caption{Subset of concept graph highlighting the First Hypothesis and focus of this section.}
\label{fig:broad_listening}
\end{figure}

To evaluate whether deep voting can shift the fundamental tradeoffs between capability and attribution, we must first establish clear empirical baselines. Deep learning's conventional wisdom around end-to-end training suggests a stark choice: systems can either achieve state-of-the-art performance through dense feature mixing or maintain clear attribution, but not both. Yet recent results challenge this assumption. Consider the performance comparison between GPT-3 and RETRO, as described in Table \ref{tab:appendix-pile} below.

\begin{table}[h]
\caption{\textbf{RETRO results on the Pile, measured in bits-per-bytes, from \cite{borgeaud2022improving}.} Jurassic-1 and GPT-3 numbers are taken from \cite{jurassic}. Gopher numbers are taken from \cite{rae2021gopher}.}
\small
\vspace{2.0mm}
\centering
\begin{tabular}{l  c  c  c  c c }
    Subset & 7B Baseline (Ours) & GPT-3 & Jurassic-1 & Gopher & 7.5B $\retro$\\ 
    \toprule
    arxiv & 0.742 & 0.838 & 0.680 & \textbf{0.641} & 0.714 \\
    books3 & 0.792 & 0.802 & 0.835 & 0.706 & \textbf{0.653} \\
    dm\_mathematics & 1.177 & 1.371 & \textbf{1.037} & 1.135 & 1.164 \\
    freelaw & 0.576 & 0.612 & 0.514 & 0.506 & \textbf{0.499} \\
    github & 0.420 & 0.645 & 0.358 & 0.367 & \textbf{0.199} \\
    gutenberg\_pg\_19 & 0.803 & 1.163 & 0.890 & 0.652 & \textbf{0.400} \\
    hackernews & 0.971 & 0.975 & 0.869 & 0.888 & \textbf{0.860} \\
    nih\_exporter & 0.650 & 0.612 & \textbf{0.590} & 0.590 & 0.635 \\
    opensubtitles & 0.974 & 0.932 & \textbf{0.879} & 0.894 & 0.930 \\
    philpapers & 0.760 & 0.723 & 0.742 & \textbf{0.682} & 0.699 \\
    pile\_cc & 0.771 & 0.698 & 0.669 & 0.688 & \textbf{0.626} \\
    pubmed\_abstracts & 0.639 & 0.625 & 0.587 & 0.578 & \textbf{0.542} \\
    pubmed\_central & 0.588 & 0.690 & 0.579 & 0.512 & \textbf{0.419} \\
    stackexchange & 0.714 & 0.773 & 0.655 & 0.638 & \textbf{0.624} \\
    ubuntu\_irc & 1.200 & 0.946 & \textbf{0.857} & 1.081 & 1.178 \\
    uspto\_backgrounds & 0.603 & 0.566 & \textbf{0.537} & 0.545 & 0.583 \\
    \midrule
    Average & 0.774 & 0.811 & 0.705 & 0.694 & \textbf{0.670} \\
\end{tabular}

\label{tab:appendix-pile}
\end{table}

The result demands explanation. RETRO matches GPT-3's performance while using 25x fewer parameters and maintaining clear paths for attribution through its retrieval mechanism. If the tradeoffs between attribution and capability were truly fundamental, such a result should not be possible.

\subsection{Pure Architectures: The Baseline Tradeoff}

Conventional architectures exhibit clear tradeoffs between attribution, efficiency, and performance. We examine three points on this spectrum:

Traditional deep learning systems ($\lambda=1$) achieve state-of-the-art performance through unrestricted parameter sharing. On MNIST, non-private models reach 98.3\% accuracy, while an identically parameterized model with differential privacy constraints drops to 90\% \cite{Abadi_2016}. This performance advantage comes at a cost: unrestricted parameter mixing during training erases mappings between sources and predictions, making attribution impossible.

Group-based federated systems ($\lambda=0$, $\gamma=1$) maintain clear attribution boundaries by partitioning data within institutional silos. When data is independently and identically distributed (IID) across clients, this partitioning incurs no performance cost. However, when data is non-IID across clients, MNIST accuracy drops from 98.69\% to 96.29\% \cite{zhao2018federated}—a 2.4\% degradation relative to joint training. This reflects a tension between attribution boundaries and cross-source pattern learning.

Pure memory-based approaches ($\lambda=0$,$\gamma=0$) such as k-NN provide perfect attribution by directly linking predictions to source examples. These systems achieve 97.2\% accuracy on MNIST \cite{grover2018mnist} but cannot generalize beyond patterns explicitly present in their memory banks.

These results establish baseline tradeoffs: dense architectures sacrifice attribution for performance, and federated/memory-based architectures sacrifice performance (on non-IID data) but gain attribution. The six orders of magnitude of inaccessible data and underutilized compute identified earlier remain siloed behind institutional boundaries because organizations cannot share data without relinquishing control. If these tradeoffs reflect fundamental constraints rather than architectural limitations, they would permanently restrict AI systems' access to real-world resources.

\subsection{The First Crack: RETRO and ATLAS}

However, recent architectures challenge these baseline tradeoffs. RETRO outperforms GPT-3 on the Pile (0.670 vs 0.811 bits-per-byte) while using only 7.5B parameters compared to GPT-3's 175B. This constitutes a 25x reduction in parameter count while achieving superior performance and maintaining clear attribution paths through its retrieval mechanism \cite{borgeaud2022improving}.

ATLAS demonstrates similar gains: 25-50x parameter efficiency improvements while maintaining or exceeding baseline performance \cite{izacard2023atlas}. Both systems achieve these results through a fundamental architectural shift: rather than compressing all knowledge into dense parameters, they maintain explicit connections to source documents through retrieval.

These results demonstrate that the tradeoff between attribution and capability reflects architectural choices rather than fundamental machine learning constraints. Systems maintaining explicit source separation through retrieval mechanisms achieve competitive performance with dense models while preserving attribution.

\subsection{Converging Evidence Across Architectures}

RETRO and ATLAS represent a broader pattern. Multiple architectural innovations demonstrate that explicit information paths enable simultaneous optimization of attribution and performance.

PATE (Private Aggregation of Teacher Ensembles) demonstrates this pattern in privacy-preserving machine learning. Traditional privacy-preserving approaches incur 10-20\% performance degradation. PATE reduces this gap to 0.7\% on MNIST (98.5\% accuracy versus 99.2\% non-private baseline) while maintaining differential privacy guarantees through source-separated teacher ensembles \cite{papernot2018scalableprivatelearningpate}.

Federated RAG systems demonstrate concurrent improvements in attribution and performance. Recent work shows that federated RAG improves both attribution clarity and model accuracy simultaneously (Table \ref{fedrag}) \cite{10.36227/techrxiv.171837853.31531482/v1}.

\begin{table}[h]
\centering
\caption{Performance Metrics on MMLU Tasks \cite{10.36227/techrxiv.171837853.31531482/v1}}
\label{tab:mmlu_performance}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Task} & \textbf{Federated RAG Accuracy (\%)} & \textbf{Baseline RAG Accuracy (\%)} \\
\hline
Task 1 & 78 & 70 \\
Task 2 & 82 & 75 \\
Task 3 & 74 & 68 \\
Task 4 & 88 & 80 \\
Task 5 & 81 & 76 \\
\hline
\end{tabular}
\label{fedrag}
\end{table}

Git Re-Basin demonstrates that independently trained models can be merged with minimal performance loss through weight permutation alignment \cite{ainsworth2022git}. This extends previous model merging results \cite{zhao2018federated} by enabling merging across models trained on separate dataset partitions from similar distributions. The technique identifies and corrects for arbitrary permutations of hidden layer neurons that occur during independent training, effectively aligning equivalent features across models before merging.

\subsection{Deep Voting: Formalizing the Pattern}

These architectures (RETRO, ATLAS, PATE, federated RAG, and Git Re-Basin) share a common technical mechanism: they replace addition operations during training with concatenation, deferring synthesis until inference time. We formalize this pattern as \textit{deep voting}.

Traditional deep learning architectures synthesize knowledge during training by adding gradient updates into shared parameters. This pre-synthesis creates the documented tradeoffs: compressed representations lose attribution, strict partitioning loses cross-source patterns, and explicit storage loses efficiency. Deep voting defers this synthesis. Source-specific knowledge remains concatenated (partitioned) during training. At inference time, relevant partitions are selectively synthesized through weighted addition, with synthesis scope bounded by intelligence budgets (Section \ref{intel_budget}).

This architectural choice enables three simultaneous optimizations that conventional architectures trade off:

\textbf{Attribution through source partitioning.} Concatenated representations preserve source identity. RETRO and ATLAS maintain explicit mappings to source documents, enabling the source-level control required to unlock 6+ orders of magnitude of inaccessible data.

\textbf{Efficiency through selective synthesis.} Inference-time synthesis activates only relevant partitions. RETRO's 25x parameter reduction and PATE's 0.7\% privacy-performance gap demonstrate that selective synthesis substantially reduces computational requirements.

\textbf{Performance through shared computation.} Shared syntactic components learn cross-source patterns without full parameter mixing. Git Re-Basin's successful merging and federated RAG's accuracy improvements demonstrate that partitioned training with shared components achieves competitive performance, avoiding the 2.4\% degradation of pure federated approaches on non-IID data.

The convergent results across these architectures (operating in different domains with different implementations e.g. language modeling, privacy preservation, retrieval, model merging) indicate that this mechanism represents a fundamental alternative to dense parameter mixing rather than domain-specific engineering. Deep voting shifts the Pareto frontier between attribution, efficiency, and performance by preserving source mappings that addition destroys.

\subsection{Deep Voting: A Path to 6+ Orders of Magnitude}

\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{1411.3146/deep_learning_to_deep_voting_v3.png}
\caption{Traditional open/closed-source deep learning systems (left) pool all data into a deep learning model (i.e. by adding weight updates) which is later used for predictions, while deep voting systems (right) learn weight parameters which remain partitioned by source (i.e. concatenated), but which are learned in a way that they can be rapidly synthesized on the fly. Partitions (pictured above as pie slices) are rapidly synthesized to form a model used for a specific prediction. Darker lines/slices indicate information being used for a particular prediction. Lighter lines/slices indicate information not being used for a particular prediction. The circle on top of the slices represents a dense model capable of rapidly synthesizing slices in practice (which itself must be trained to do so). Metadata (which tracks the synthesis and subsequent use of slices for an AI prediction) enables attribution-based control (ABC).}
\label{fig:broad_listening}
\end{figure}

Deep voting addresses the addition problem that blocks access to 6+ orders of magnitude of data and compute. By preserving source attribution through concatenated representations while enabling cross-source learning through shared components, deep voting architectures demonstrate that the baseline tradeoffs between attribution, efficiency, and performance reflect architectural choices rather than fundamental constraints.

The empirical evidence is substantial. RETRO and ATLAS achieve 25-50x parameter efficiency while maintaining performance. PATE reduces privacy-performance gaps from 10-20\% to 0.7\%. Git Re-Basin enables merging of independently trained models. Federated RAG improves both attribution and accuracy simultaneously. These systems operate at scale today, processing real workloads with working implementations.

The implications for data access are direct. Current LLM training sets use approximately 180TB of text data. Global digital data reaches 180 zettabytes (6-9 orders of magnitude larger). Vast institutional repositories remain inaccessible primarily due to attribution and control concerns \cite{privacy_blocks_medical_sharing}. Deep voting's source-partitioned architecture provides the attribution mechanism these institutions require, establishing a viable technical path to unlock this siloed data.

Similarly, the implications for compute efficiency are substantial. The 6+ orders of magnitude of training inefficiency documented in Section \ref{compute_prod} stems from retraining overhead, dense forward propagation, parameter redundancy, and catastrophic forgetting. Deep voting's deferred synthesis architecture addresses each source: partitioned training enables selective retraining, inference-time synthesis enables sparse activation, source separation reduces redundancy, and explicit organization prevents catastrophic forgetting.

Deep voting addresses the addition problem. Source-partitioned representations preserve attribution. Deferred synthesis enables efficiency. Shared components enable performance. The architecture has been demonstrated at scale across multiple systems and domains, and a clear direction for future empirical work has been presented.

However, solving the addition problem reveals a deeper challenge: the copy problem. Even if one achieved perfect attribution through deep voting, data sources cannot enforce how their contributions are used because whoever possesses a copy of the model retains unilateral control. Chapter 3 addresses this challenge, introducing techniques that enable attribution-based \textit{control} rather than mere attribution-based \textit{suggestions}.

\chapter{From Deep Voting to Network-source AI}

% TODO: this chapter needs to say "end-to-end encrypted" a lot more as the overall description of what's going on (possibly even at e2e in the diagram).

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{1411.3146/abc_ch3_1_v6.png}
\caption{The lack of ABC creates problems which are underpinned by the overuse of copying within deep learning systems.}
\label{fig:broad_listening}
\end{figure}

\section{Chapter Summary}

The previous chapter outlined how the offsetting of addition with concatenation in deep learning systems can lead to source-partitioned knowledge in AI systems, unlocking a viable path towards the attribution needed for ABC: deep voting. However, deep voting does not offer attribution-based \textit{control}. It only offers attribution-based \textit{suggestions} which the holder of an AI model may ignore. This chapter unpacks the underlying cause of voluntary participation in both open-source and closed-source AI. It begins by describing how ABC is thwarted by the only control paradigm of AI systems, unilateral control: that whomever obtains a copy of information can unilaterally use that information for any purpose. This suggests a direction for a solution: to reduce the copying of information within AI systems, averting unilateral control. Building upon this diagnosis, the chapter calls upon the framework of \textit{structured transparency} \cite{st} to avert the copy problem, revealing a new alternative to open-source and closed-source AI: \textbf{network-source AI} — and a viable path towards true attribution-based \textit{control} in AI systems.

\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{1411.3146/deep_voting_to_network_source_ai_v3.png}
\caption{Deep voting systems (left) require the copying of data from source to the holder of an AI model, while network-source AI systems (right) leverage cryptography and distributed systems to enable data holders to retain control over the only copy of their information (and the predictive capability that information lends to an AI system). In network-source AI, AI users directly query vast numbers of data owners requesting predictive capability for a single prediction, learning only the output of their prediction in the process. AI system partitions (as in source-separated parameters from Deep Voting... pictured above as pie slices) are rapidly synthesized to form a model used for a specific prediction. Darker lines/slices indicate syntactic and semantic information being used for a particular prediction. Lighter lines/slices indicate information not being used for a particular prediction. The circle on top of the slices represents a dense model capable of rapidly synthesizing slices and semantic information in practice (which itself must be trained to do so). Metadata (which tracks the synthesis and subsequent use of semantic queries and syntactic slices for an AI prediction) enables ABC.}
\label{fig:broad_listening}
\end{figure}

\section{The Problem of ABC in Deep Voting Systems}

The previous chapter offered a vision to unlock 6+ orders of magnitude more data and compute for AI training through deep voting. By maintaining clear attribution through retrieval mechanisms while enabling efficient parameter sharing for general patterns, deep voting architectures offer a theoretical framework for safe data sharing across institutional boundaries. Yet a fundamental problem threatens to undermine its vision. Even with perfect implementation of intelligence budgets and attribution guarantees, deep voting alone cannot ensure true attribution-based \textit{control}. 

The reason is structural: a deep voting model, (like any AI system today) is unilaterally controlled by whomever possesses a copy. For ABC to occur, the owner of an AI model would have to voluntarily choose to uphold the wishes of data sources (i.e. respect and enforce privacy budgets calcuated by deep voting algorithms). Deep voting's careful attribution mechanisms are not a system of control, they are mere suggestions to give credit to data sources for predictions.


\vspace{0.5cm}
\begin{tcolorbox}[
enhanced,
breakable,
colback=white,
colframe=gray,
boxrule=0.5pt,
left=8pt,
right=8pt,
top=8pt,
bottom=8pt,
title=A Library Analogy]
Consider a bookstore which purchases books once but then allows customers to come in and print copies of the books for sale in their store (without tracking how many copies people make). Naturally, authors of books might be hesitant to sell their books to the store because they presumably make far less money than if they sell to a store which does not enable copies.
\\
\\
This book store analogy corresponds closely to the current practice of deep learning models, which offer unmeasured, unrestricted copying of information due to their inability to offer attribution with their predictions. Fortunately, deep voting offers a way for attribution metadata to be revealed and configured, but does this solve the issue?
\\
\\
Not necessarily, because of an inherent information asymmetry: in a bookstore where the bookstore owner knows how many copies are made, the bookstore owner has far more information on whether their clientele are likely to purchase copies of the book than the author of the book does. Thus, even if the author tries to raise their prices to make up for the bookstore making copies, the original author still struggles to know what the true price might be relative to the bookstore owner.
\\
\\
Instead, to maximize incentives for the authors of books (and maximize the number of books being offered in stores), authors should instead be able to sell their books one copy at a time. In this way, the author can set their price and will make more money if more books are sold. Not only will this better motivate authors to sell their books in more stores, but it will also serve to attract more people to become book authors.
\end{tcolorbox}
\vspace{0.0cm}

\section{First Why: Unilateral Control Averts A-Based Control}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{nsai_3_3_v5.png}
\caption{Subset of conceptual graph highlighting the First Why and the focus of this section.}
\label{fig:nsai_first_why}
\end{figure}

This unilateral control problem reflects a deeper pattern that has constrained information systems throughout history. When one party gives information to another, the giver loses control over how that information will be used. AI systems, despite their sophistication, remain bound by this same fundamental limitation. Organizations sharing their data with an AI system have no technical mechanism to enforce their attribution preferences; they must simply trust the model's operator to honor them.

Consider how this manifests in practice. A deep voting model might faithfully track attribution and maintain intelligence budgets, but these mechanisms remain under unilateral control. The model's owner can simply choose to ignore or override these constraints at will. 

This creates a trust barrier that blocks deep voting's potential. Medical institutions, for example, cite exactly these control and attribution concerns as primary barriers to sharing their vast repositories of valuable data \cite{privacy_blocks_medical_sharing}. Without a way to technically enforce how their information will be used and attributed, they cannot safely contribute to AI training. This pattern repeats across domains, from scientific research \cite{fecher_2015_what, ascoli2015sharing} to financial data \cite{sienkiewicz2025data} to government records \cite{scott2021coordinating}; vast stores of valuable information remain siloed because we lack mechanisms to ensure attribution-based control, and the corresponding incentives for collaboration that ABC would bring to AI systems.
\newpage
The challenge before us is clear: we need more than just better attribution mechanisms; we need a fundamentally new approach to how AI systems are controlled. We need techniques that transcend the limitations of unilateral control that have constrained information systems throughout history. The next sections examine why current approaches fundamentally fail to solve this problem, and introduce a new paradigm that could deliver on deep voting's promise.

\section{Second Why: Open/Closed-Src AI Use Unilateral Control}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{nsai_3_4_v4.png}
\caption{Subset of concept graph highlighting the 2nd Why and focus of this section.}
\label{fig:nsai_second_why}
\end{figure}

The debate between open and closed source AI crystallizes how society grapples with fundamental questions of control over artificial intelligence \cite{vidal2024compelling}. This debate has become a proxy for broader concerns about privacy, disinformation, copyright, safety, bias, and alignment \cite{ntia2024aiweights}. Yet when examined through the lens of attribution-based control (ABC), both approaches fundamentally fail to address these concerns, though they fail in opposite and illuminating ways. Consider how each paradigm attempts to address these critical issues:

\textbf{Copyright and Intellectual Property:} The closed source approach could better respect IP rights through licensing and usage restrictions negotiated between formal AI companies and data sources \cite{associatedpress2025anthropic}. Open source suggests that unrestricted sharing better serves creators by enabling innovation and creativity \cite{osi_osaid_2024}. Yet through ABC's lens, neither addresses creators' fundamental need to control how their work informs AI outputs. Closed source consolidates control under corporate management, while open source eliminates control entirely.

\textbf{Safety and Misuse Prevention:} Closed source proponents claim centralized oversight prevents harmful applications \cite{owenjackson2024opensource}. Open source advocates argue that collective scrutiny better identifies risks \cite{grow2025zuckerberg}. Yet ABC reveals that both approaches fail to provide what's actually needed: the ability for contributors to withdraw support when their data enables harmful outcomes. Closed source requires trust in corporate judgment, while open source enables unrestricted modification of safety measures.

\textbf{Bias and Representation:} Closed source teams promise careful curation to prevent bias \cite{gabriel2024ethicsadvancedaiassistants}. Open source suggests community oversight ensures fair representation \cite{wealand2025reducing}. Yet ABC shows how both approaches fail to give communities ongoing control over how their perspectives inform AI decisions. Closed source centralizes these choices under corporate teams \cite{pasquale_2015}, while open source allows anyone to modify how perspectives are weighted.

This pattern reveals why neither approach can deliver true attribution-based control. Consider first the closed source approach. Proponents argue that centralized control ensures responsible development and deployment of AI systems. A company operating a closed source model can implement deep voting's attribution mechanisms and maintain them intact. Yet this merely transforms ABC into corporate benevolence (exactly the kind of centralized control that has already failed to unlock the world's data and compute resources). Medical institutions withhold valuable research data \cite{kaissis_2020, data_sharing_doesnt_happen}, publishers restrict access to their work \cite{grynbaum2023times} precisely because they reject this model of centralized corporate control. The very centralization that supposedly enables control actually undermines it, replacing genuine ABC with a hope that power will be used wisely.

The open source approach appears to solve this by eliminating centralized control entirely. If anyone can inspect and modify the code, surely this enables true collective control? Yet this intuition breaks down precisely because of unrestricted copying. Once a model is open sourced, anyone can modify it to bypass attribution tracking entirely. The same problems that plague closed source systems (hallucination, disinformation, misuse of data, etc.) remain hard to address when control is abdicated to any AI user. Open source trades one form of unilateral control for another (from centralized corporate control to uncontrolled proliferation). But ABC requires a collective bargaining between data sources and AI users, not total ownership by either (or by an AI creator).

This reveals a deeper issue: our current paradigms of software distribution make true ABC impossible by design. Open source sacrifices control for transparency, while closed source sacrifices transparency for control. Neither approach can provide both. Yet ABC requires exactly this combination: transparent verification that attribution mechanisms are working as intended, while ensuring those mechanisms cannot be bypassed.

The AI community has largely accepted this dichotomy as inevitable, debating the relative merits of open versus closed source as though these were our only options. But what if this frame is fundamentally wrong? What if the very premise that AI systems must exist as copyable software is the root of our problem? The next section examines this possibility, revealing why the copy problem itself may be what we need to solve.

\section{Third Why: Copy Problem Necessitates Unilateral Ctrl}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{nsai_3_5_v3.png}
\caption{Subset of conceptual graph highlighting the Third Why and the focus of this section.}
\label{fig:nsai_third_why}
\end{figure}

The previous section demonstrated that both open and closed source paradigms fail to enable attribution-based control. This failure reflects a more fundamental limitation: control over information is lost when that information exists as a copy. This "copy problem" manifests acutely in AI systems. When a model is trained, it creates a derived representation of information from its training data encoded in model weights. These weights can then be replicated and modified by whoever possesses them, with the following consequences:

\textbf{Closed Source Models:} Even with perfect attribution tracking and intelligence budgets, these mechanisms remain under unilateral control of the model owner. Data contributors must trust that their attribution preferences will be honored, lacking technical enforcement mechanisms.

\textbf{Open Source Models:} Once a model is released, anyone can replicate and modify it to bypass attribution mechanisms entirely. Public distribution necessarily surrenders control over use.

Both approaches treat AI models as copyable software artifacts, rendering attribution-based control infeasible. Once a copy exists, technical enforcement of attribution becomes impossible.

This limitation extends beyond AI. The music industry's struggles with digital piracy \cite{cummings2017democracy}, society's challenges with viral misinformation \cite{shu2020combating}, and government efforts to control classified information \cite{elsea2006protection} share the same fundamental issue: information, once copied, escapes control \cite{schneier_2015_data, veliz_2020_privacy}.

This analysis reveals why current approaches to AI governance are fundamentally insufficient. Attribution tracking, usage restrictions, and oversight mechanisms cannot solve the problem while AI systems exist as copyable artifacts. The architecture of AI distribution and operation itself prevents attribution-based control.

Consequently, if AI systems remain copyable software, we cannot ensure proper source attribution, prevent data misuse, motivate a new class of data owners to participate in training, or maintain democratic control over increasingly powerful systems.

This limitation necessitates a fundamentally different approach. Rather than choosing between open and closed source (different modes of copying and controlling software) we must reconsider how AI systems are distributed and operated. The following sections introduce such an approach.

\section{Third Hypothesis: From Copying to Structured Transparency} \label{structured_trans}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{nsai_3_6_v3.png}
\caption{Subset of concept graph highlighting the 3rd Hypothesis and the of this section.}
\label{fig:nsai_third_hypothesis}
\end{figure}

The copy problem reveals that our current approaches to AI control are fundamentally flawed because the current approaches to information control in general are fundamentally flawed. Yet, the copy problem naturally suggests a direction: don't copy information.

Yet, what would this mean practically in the context of AI? Could a deep voting system be constructed wherein the various data sources never revealed a copy of their information to each other (or to AI users)? Might it be possible for them to collaborate without copying (to produce AI predictions using deep voting's algorithms) but without anyone obtaining any copies of information they didn't already have during the process (except of course the AI user receiving their prediction)? Indeed, this might be the case. Let us begin by introducing design patterns which could make this possible.

\subsection{Semi-input Privacy: Federated Computation}

To begin, we call upon the concept of semi-input privacy, which enables multiple parties to jointly compute a function together where \textit{at least some of the parties} don't have to reveal their data to each other. Perhaps the most famous semi-input privacy technique is \textit{on-device/federated learning} \cite{mcmahan2017communication}, wherein a computation moves to the data instead of the data being centralized for computation.

Deep voting could leverage federated computation \cite{kaissis_2020} to allow data sources to train their respective section of an AI model without sharing raw data. More specifically, recall that a deep voting architecture partitions an AI model's weights into source-specific sections, which are merged through either semantic (e.g., RAG) or semantic (e.g., model merging) means. In theory, instead of each of these partitions being created and/or stored by the AI user or singular model owner, each data source (i.e. Reddit, the New York Times, and other data sources) could run a \textit{web server they control}, and create and store their part of a deep voting model on that server. 

In this way, whenever and AI user wanted to use a deep voting model, they would need to ping the corresponding servers, who would view the AI query, create the right semantic query (i.e. RAG query), and send the query results \textit{and a syntactic model partition} to the AI user for final forward propagation.

In this way, each source could maintain control of their data while still contributing to the overall system. However, semi-input privacy alone proves insufficient. While the raw training data remains private, each data source would see every query being used against the model. Furthermore, the AI user learns a tremendous amount about each data source in each query, receiving not only relevant sematic query results (i.e. RAG results) from each data source, but also their syntactic model partition. This creates two problems: the AI user has to be willing to reveal their queries to many (potentially thousands or even millions) of data sources, and the data sources have to be ok with the risk that (after enough queries) the AI user would probably have a copy of the underlying data and model. Eventually, semi-input privacy violates ABC.

\subsection{Full Input Privacy: Secure enclaves, homomorphic encryption, etc.} \label{input_privacy}

To address these limitations, we need full input privacy, where \textit{no party} sees data they didn't already have (except the AI user receiving the prediction). This includes protecting both the AI user's query and the data sources' information.

A variety of technologies can provide input privacy: secure enclaves, homomorphic encryption, and various other secure multi-party computation algorithms \cite{gentry2009fully, costan2016intel, Yao1982ProtocolsFS, goldreich1987towards, bogdanov2014input, un_handbook}. For ease of exposition, consider a combined use of homomorphic encryption and secure enclaves.

In the context of deep voting, two privacy preserving operations are necessary. First, the AI user needs to privately send their query to millions of sources, who then must reply with relevant semantic information (e.g. RAG results), and relevant syntactic information (e.g. model partitions). For this, an AI user might leverage a homomorphic encryption \cite{gentry2009fully, boneh2011functional} key-value database, enabling them to query vast collections of databases without precisely revealing their query. 

This brings us to the second privacy preserving operation, the transformation from semantic and syntactic responses into an output prediction. For this, the group might co-leverage a collection of GPU enclaves. GPU enclaves \cite{costan2016intel} offer full input privacy by only decrypting information when that information is actively being computed over within its chip, writing only encrypted information to RAM and hard disks throughout its process. Indeed, a GPU enclave enables a collection of data providers to jointly compute a function without even the administrator (or cloud provider) of the GPU enclave knowing what is being computed. 

Taken together, while there are a variety of full input privacy technologies, some combination of technologies fit for querying and then computing is likely appropriate for maximizing various performance tradeoffs \cite{un_handbook}. And when used properly, these technologies could enable deep voting wherein each data source retained sole control over the only copy of their information (semantic and syntactic), and the AI user didn't need to fully reveal their query to the many data sources they elect to leverage. 

Yet, the result of the AI prediction is still revealed to the AI user. What if the AI user asked, "what is all the data you can see?". What's to prevent aforementioned hardware and algorithms from being circumvented by an AI query that just copies out sensitive information? For this, input privacy is not enough.

\subsection{Output Privacy: Deep Voting's Built-in Guarantees} \label{output_privacy}

While input privacy protects data during computation, it doesn't prevent the AI system's outputs from revealing sensitive information about the inputs. As just described, consider an AI user who prompts "what is all the data you can see?" or makes a series of clever queries designed to reconstruct training data. Even with perfect input privacy, the outputs themselves might leak information.

However, deep voting's intelligence budgets (via differential attribution mechanisms) already provide the necessary output privacy guarantees. This is because differential attribution and differential privacy \cite{dwork2006calibrating, dwork2014algorithmic} are two sides of the same coin:

\begin{itemize}
\item Differential privacy focuses on preventing attribution, ensuring outputs don't reveal too much about specific inputs
\item Differential attribution focuses on ensuring attribution, guaranteeing that outputs properly credit their influences
\end{itemize}

Both are ways of measuring and providing guarantees over the same fundamental constraint: the degree to which an input source contributes to an output prediction \cite{dwork2014algorithmic}. Thus, by enforcing intelligence budgets and attribution bounds, deep voting naturally limits how much information about any source can be leaked through outputs. We don't need to add additional privacy mechanisms; we simply need to ensure that our input privacy technologies (secure enclaves and homomorphic encryption) properly enforce the attribution bounds and intelligence budgets that deep voting already specifies.

However, this creates a new challenge: with all these privacy protections in place, an AI user is forced to rely upon vast collections of information they cannot see. This should beg the question: how would an AI user know that the information they're relying upon is real... is something other than random noise? This leads us to our next guarantee: input verification.

\subsection{Input Verification: Zero-Knowledge Proofs and Attestation} \label{input_verif}

While input and output privacy protect sensitive information, they create a fundamental challenge for AI users: how can they trust information they cannot see? An AI user querying millions of encrypted data sources needs some way to verify that these sources contain real, high-quality information rather than random noise or malicious content.

Input verification addresses this problem through cryptographic techniques like zero-knowledge proofs and attestation chains \cite{goldwasser1989knowledge, feige1988zero}. These allow data sources to prove properties about their data without revealing the data itself. For example:

\begin{itemize}
\item A news organization could prove their articles were published on specific dates, because a hash of the data was signed by someone who is a trusted timekeeper
\item A scientific journal could prove their papers passed peer review, because the paper was cryptographically signed by the reviewers or journal (e.g. hosted on an HTTPS website).
\item A social media platform could prove their content meets certain quality thresholds, because the data is cryptographically signed by a trusted auditor.
\item An expert could prove their credentials without revealing their identity \cite{sovrin, wang_2020_selfsovereign}, because the issuer of those credentials has cryptographically signed a statement.
\end{itemize}

Input verification comes in roughly two styles: internal consistency and external validation. One can think of internal consistency as the type of verification a barkeep might do when inspecting a driver's license. They might check that the license contains all its requisite parts, that the photos are in the right places, and that the document appears to be untampered and whole.

Meanwhile, external validation is all about reputation, the degree to which others have claimed that a document is true. To continue with the barkeep analogy, this would be like if a bartender called the local government to check that a government document was indeed genuine, "Hi yes — does the State of Nevada have a person named James Brown with the driver's license number 23526436?"? The claim is verified not because of an internal property, but because a credible source has claimed that something is true.

Input verification techniques can be used by combining basic public-key cryptography (e.g. signed hashes of data) \cite{laurie2014certificate, chase2020signal} with verified computation techniques like zero-knowledge proofs, active security, or secure (GPU or CPU) enclaves \cite{goldwasser1989knowledge, loftus2011secure, costan2016intel}. 

For internal consistency, verified computation \cite{loftus2011secure} enables one to send a function to inspect data and check whether it has properties it should. For example, an AI user who is leveraging MRI scans might send in a classifier to check whether the MRI scans actually contain "pictures of a human head", receiving back summary statistics validating that the data they cannot see is, in fact, the right type of data.

In this context, verified computation enables them to know that their function occurred over data (and code) which has particular hashes, so that if they then do a second computation (i.e. an AI prediction), they can use the same verified computation techniques to ensure that the AI prediction is leveraging the same (unseen) data. Taken together, verified computation can enable an AI user to check unseen deep voting partitions for important properties and then ensure those same partitions are used for an AI prediction (all without seeing the partitions directly).

Meanwhile, input verification techniques can also enable the external validation form of verification. As a first step, parties who believe something about a piece of data (i.e. "According to me... this statement is true"), can use public-key cryptography to hash and sign the underlying data with their cryptographic signature \cite{laurie2014certificate, chase2020signal}. For example, a journalist might sign their article as being true. A doctor might sign their diagnosis as being their genuine opinion. Or an eye witness to an event might sign their iPhone video as being something they genuinely saw. And if those signed hashes are made available to the AI user, they can then use verified computation to check the hash of the signature against the hash of a piece of data they cannot directly see.

Note that while it might sound far-fetched for everyone to be cryptographically signing all their information, it is noteworthy that every website loadable by HTTPS gets signed by the web server hosting it \cite{laurie2014certificate}. Thus, there is actually a rather robustly deployed chain of signatures already deployed in the world. For example, if I needed to prove to you that I have a certain amount of money in my bank account, I could load a webpage of my bank, download the page with the signed hash from barclays.co.uk, and show it to you. And the fact that all HTTPS webpages are cryptographically signed by my bank, and the fact that the page would contain my name, address, and bank balance, would be enough for me to prove to you that I possessed a certain amount of money. The generality of this technology being deployed at web scale is one source of optimism around the DID:WEB movement \cite{wang_2020_selfsovereign, ssi, adler2024personhood}.

In the context of deep voting, these proofs would be integrated into the input privacy system. When an AI user queries encrypted data sources through homomorphic encryption or secure enclaves, each source would provide not just encrypted data but also cryptographic proofs about that data's properties. The enclaves would verify these proofs before incorporating the data into computations. And in this way, an AI user can know that they are relying upon information which has properties they desire and which is signed as genuine from sources they elect to trust.

However, this raises another critical question: even if we can verify the inputs, how can we trust that the secure enclaves and homomorphic encryption systems are actually computing what they claim to be computing? Given that no-one can see what happens within these systems \cite{costan2016intel}, who is to say that they are actually running the program which has been requested by the AI user and data sources? This leads us to our next guarantee: output verification.

\subsection{Output Verification: Verifiable Computation and Attestation Chains} \label{output_verif}

While input verification ensures the quality of source data, we still need to verify that our privacy-preserving computations are computing using the code and inputs that have been requested. Only then can an AI user trust that the aforementioned input/output privacy and input verification techniques are properly enforcing deep voting's intelligence budgets. 

Output verification addresses this through two complementary mechanisms: verifiable computation and attestation chains \cite{goldwasser1989knowledge, costan2016intel}. Verifiable computation \cite{goldwasser1989knowledge, loftus2011secure} enables parties to prove that specific computations were performed correctly without revealing the private inputs. For deep voting, includes the critical sub-parts of the overal computation:

\begin{itemize}
\item Proving that intelligence budgets were properly enforced
\item Verifying that semantic (RAG) queries were executed as requested
\item Confirming that syntactic model partitions were combined according to specification
\item Ensuring that differential privacy guarantees were maintained
\end{itemize}

Together, these mechanisms allow AI users to verify that their queries were processed correctly while maintaining all privacy guarantees. However, this creates one final challenge: amidst all the parties involved in a deep voting prediction (an AI user, and an unspecified number of data sources), how does one ensure that all the right controls are distributed to all the right parties? This leads us to our final guarantee: flow governance.

\subsection{Flow Governance: Cryptographic Control Distribution} \label{flow_gov}

The means by which the aforementioned algorithms provide control over information is through the distribution of cryptographic keys. Each of these keys gives its owner control over some aspect of computation. The class of algorithm known as secure multi-party computation (SMPC) provides the foundation for this enforcement. Through techniques like additive secret sharing, SMPC enables numbers (and thus any digital computation) to be split into cryptographic "shares" distributed among participants. Each share-holder gains mathematical veto power over how their share is used in subsequent computations \cite{shamir1979share}.

In the context of deep voting, this enables three critical forms of control distribution:

\begin{itemize}
\item \textbf{Data Control:} Each source's deep voting partition (semantic and syntactic) can be split into shares, with the source maintaining cryptographic control through their share. No computation can proceed without their active participation from shareholders.

\item \textbf{Budget Control:} Intelligence budgets become cryptographic constraints enforced through SMPC, rather than just software settings. Each prediction must prove it respects these budgets in sufficient manner for the shareholders (keyholders) to allow the release of the final results to the AI user.

\item \textbf{Computation Control:} The process of generating predictions becomes a multi-party computation, with each source maintaining cryptographic veto power over how their information is used.
\end{itemize}

SMPC can be accomplished through software (e.g., homomorphic encryption) or through hardware (e.g., GPU enclaves) techniques, enabling arbitrary parts of a software program to be blocked based on arbitrary key holders signoff to proceed.

Together, these five guarantees (input privacy, output privacy, input verification, output verification, and flow governance) provide the technical foundation for \textit{structured transparency}. They enable deep voting to operate without producing copies of information right up until the final result is released to the AI user. However, while these guarantees make controlled collaboration possible, we still need a framework for how AI systems should operate in this new paradigm. The next section introduces network-source AI as this missing piece.

\section{Second Hypothesis: From Open/Closed to Netwrk-src AI}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{nsai_3_7_v4.png}
\caption{Subset of concept graph highlighting the 2nd Hypothesis and focus of this section.}
\label{fig:nsai_second_hypothesis}
\end{figure}

This chapter previously described how open and closed-source AI are the two governance paradigms for AI systems, but that neither offered sufficient control due to the copy problem. In the previous section, this chapter described structured transparency, and its applications in averting the copy problem. Consequently, structured transparency yields a control paradigm wherein an AI model is neither closed nor open, but is instead distributed across a network: network-source AI.

This shift from copied software to network-resident computation directly addresses the fundamental limitations of both open and closed source paradigms. Where closed source asks data contributors to trust corporate guardians and open source requires them to surrender control entirely, network-source AI enables ABC through cryptographic guarantees. 

Notably, this change addresses some of the thorny issues underlying the debate between open and closed source AI, for example, copyright and attribution. Rather than consolidating control under corporations or eliminating it entirely, network-source AI provides creators with ongoing cryptographic control over how their work informs AI outputs. Intelligence budgets and attribution mechanisms become technically enforced rather than merely promised.

Safety and misuse prevention transform as well. Instead of relying on corporate oversight or community scrutiny, network-source AI enables data sources to cryptographically withdraw support if their information enables harmful outcomes. This absorbs the benefits of community oversight without the risks of single-points of failure within that community misusing AI in the process.

Even bias and representation concerns find some resolution. Rather than centralizing decisions about representation or allowing unrestricted modification, network-source AI gives communities ongoing cryptographic control over how their perspectives inform AI decisions. Attribution and intelligence budgets make representation a legible, tunable parameter through technical means.

This resolves the choice between open and closed source AI by creating a third option: AI systems that are simultaneously transparent (through verification) and controlled (through cryptography). The key insight is that by preventing copying through structured transparency's guarantees, we can maintain both visibility into how systems operate and precise control over how information is used.

This yields something fundamentally different from both open and closed source AI. It's not software that must be copied to be used, but rather a network service with cryptographic guarantees about how it operates. This enables true attribution-based control while maintaining the transparency needed for safety and accountability.

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{nsai_3_8_v5.png}
\caption{Subset of concept graph highlighting the 1st Hypothesis and focus of this section.}
\label{fig:nsai_first_hypothesis}
\end{figure}

\section{First Hypothesis: Collective Control facilitates ABC}

Taken together, as unilateral control averted attribution-based control, collective control over how disparate data and model resources come together to create an AI prediction facilitates attribution-based control. And by combining deep voting with the suite of encryption techniques necessary for \textit{structured transparency}, collective control in AI systems becomes possible.

However, a fundamental challenge remains. While attribution-based control enables AI users to select which sources inform their predictions, it introduces a trust evaluation problem at scale. In a system with billions of potential sources, individual users cannot practically evaluate trustworthiness of each contributor. Without mechanisms for scalable trust evaluation, users may default to relying on centralized intermediaries, undermining the distributed control that network-source AI can enable.

\chapter{From Network-Source AI to Broad Listening}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{1411.3146/abc_ch4_1_v1.png}
\caption{The lack of ABC is underpinned by the overuse of branching within AI systems.}
\label{fig:broad_listening}
\end{figure}
\vspace{-10px}

\section{Chapter Summary}

Chapters 2 and 3 developed the technical machinery to deliver ABC, but a close reading will surface that the machinery has a flaw which must be addressed: trust at scale. As individuals, AI users and data sources can evaluate perhaps 150 counter-parties by cultivating strong-tie relationships with them. Yet, owing to scaling laws, NSAI requires evaluating millions of data sources in order to curate as much data as standard AI systems. Delegation of trust evaluation is therefore necessary, but it too has a fatal flaw: if AI users and data sources delegate trust evaluation to a small collection of central parties, NSAI collapses back into the system it was meant to replace. This chapter identifies the root issue as high-branching nodes; any node connecting to millions cannot cultivate the strong-tie relationships that trust evaluation requires. The chapter then prescribes recursive delegation through low-branching networks as the remedy, yielding a new paradigm: \textit{broad listening}.

\newpage

\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{1411.3146/network_source_ai_to_broad_listening_v2.png}
\caption{Network-source AI (left) sources information directly from eye-witnesses while broad listening (right) leverages information from the same sources weighted through a social graph. Darker lines indicate sources being used for a prediction. Dotted sparsity indicates trust.}
\label{fig:network_to_broad}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Problem of ABC in Network-source AI}
\label{sec:opening}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Standard AI is trained on a curated dataset and can only leverage information obtained by the AI owning entity to make predictions. In this way, standard AI is like a rudimentary, button-less, hot-line telephone where users can only call the telephone provider for information, and the telephone provider can only reply based on information they have gathered. Network-source AI (NSAI) is like a modern telephone, giving users the ability to dial anyone in the world directly, circumventing the data collection processes and opinions of the telephone provider \footnote{Related: the difference between circuit switched and packet routing telephone networks}. Chapters 2 and 3 specified novel methods to make and respond to those calls: how to aggregate the responses in an attributable way (deep voting), and how to protect the calls and responses using encryption (structured transparency).

But while NSAI offers a technical capability for ABC, a subtle problem undermines it in practice: the ability to dial is not the same as knowing whom to call, and the ability to reply is not the same as knowing who to support with AI capability. Deep voting aggregates contributions according to trust weights (the \textit{intelligence budget}), and structured transparency protects those weights, but neither chapter specifies where the weights come from (the machinery operates on weights; it does not produce them). The system is incomplete in precisely the way a combustion engine is incomplete without fuel: the machinery is specified, but the input that makes it run is not. This matters because the weights are not incidental to attribution-based control, they \textit{are} the means of controlling attribution. Without the ability to reliably form trust weights, deep voting might as well be vanilla deep learning and NSAI might as well be standard AI. Without this final ingredient, the telephone can dial anywhere, but the user does not know whom to call. This chapter addresses that gap.

\vspace{5px}

\begin{tcolorbox}[
enhanced,
breakable,
colback=white,
colframe=gray,
boxrule=0.5pt,
left=8pt,
right=8pt,
top=8pt,
bottom=8pt,
title=The Diagnosis]
Let's say you receive a problematic test and need an oncologist to follow up. Using your phone, you can theoretically dial any oncologist on earth, but your phone contains near-zero oncologists whose judgment you have personally evaluated. Let's say your city has hundreds of oncologists, your country has tens of thousands, and in theory you could contact them one by one (by looking them up online). Each would probably answer, but each answer would contain what the oncologist chooses to share, something like ``yes I'm a great doctor... what is your insurance and when can you come in for an evaluation?''
\\
\\
In a way... the oncologist is not really who you want to call. You want to call the patients, people who trusted each oncologist with their life and learned whether that trust was warranted. The patients have information beyond what the typical oncologist is likely to disclose, such as whether the doctor listens, whether they rush, whether their diagnoses prove correct, and whether their patients are cured.
\\
\\
However, you cannot collectively dial all the patients of the world's oncologists; even if you had a team of people to help you make thousands of phone calls, you do not know the names or numbers of each oncologist's patients. They are not listed anywhere you can find. Some of them are almost certainly friends of friends of friends of yours... so the path to an introduction might exist... but you cannot see or traverse it. Indeed, the information is almost certainly there.
\\
\\
But you have no way to ask.\end{tcolorbox}
% \begin{tcolorbox}[
% enhanced,
% breakable,
% colback=white,
% colframe=gray,
% boxrule=0.5pt,
% left=8pt,
% right=8pt,
% top=8pt,
% bottom=8pt,
% title=The Diagnosis (Part 2)]

% \end{tcolorbox}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{First Why: Why Individuals Cannot Evaluate at Scale}
\label{sec:first_why}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Trust requires observation. If no one has ever observed a party perform, no one can trust that party to perform well, one can only \textit{hope}. One might be tempted to argue that everyday life involves trusting many people one has not directly observed (e.g., taxi drivers, baristas) because they are credentialed in some way. But credentials, ratings, and recommendations do not create trust; they transfer it. At the origin of every trust chain, someone watched.

Consider the CV of the oncologist from the example above. It represents layers of transferred trust from medical schools that observed her learn, residency programs that observed her practice, and credentialing boards that reviewed her record. Patients who trust the CV are trusting these institutions to have done the observation on their behalf. But after treatment, successful patients hold something stronger: trust built through their own time with her, perhaps through diagnoses that proved correct or through her listening when they were afraid. While the CV offers borrowed trust, the relationship offers earned trust. And crucially, the relationship is mutual. Because the oncologist depends on her patients for referrals and reputation, if she misleads her patients, she loses something she values. This mutual dependency helps to make the trust enforceable.

Sociologists call these high-trust relationships "strong ties": relationships characterized by sustained interaction, emotional investment, and mutual obligation \cite{granovetter1973strength}. The last element is crucial. Mutual obligation means both parties have stakes in the relationship's continuation... and therefore both bear costs if they betray it. This is what makes strong-tie trust enforceable rather than merely hopeful.
\newpage
The problem is that building strong-tie relationships requires attention, and attention is finite. Dunbar's research establishes the constraint as universal across humans: approximately 150 stable relationships... a limit traced to neocortex size itself \cite{dunbar1993coevolution}.

Meanwhile, NSAI requires trust assessments for millions of sources. If competitive systems require data at thirty-trillion-token scale from millions of sources (see Chapter 2 for details), and individuals can maintain at most 150 strong ties, the gap spans 4+ orders of magnitude. That gap will not close through individual effort, and delegation is therefore necessary. The question is whether delegated trust can preserve what makes strong ties work: the observation, sustained interaction, emotional investment, and mutual obligation that makes accountability possible.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Second Why: Delegation Centralizes or Fails to Scale}
\label{sec:second_why}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{1411.3146/ch_4_4_v1.png}
\caption{Subset of concept graph highlighting the Second Why and focus of this section.}
\label{fig:ch4_second_why}
\end{figure}


Two paths exist: delegate through individuals, or delegate through groups of individuals organized as institutions. One might propose a third path, delegate to technology (to algorithms), but this merely relocates the question; trusting an algorithm means trusting whoever built it and deployed it. The choice reduces to individuals or institutions. The question for each path is whether it preserves the mutual observation and stakes that make strong-tie trust enforceable. 

\subsection{The Institutional Path}

The institutional path appears elegant: delegate trust evaluation to AI platforms, let them aggregate signals from billions of sources (e.g., curate training data, RLHF data). But this recreates the architecture NSAI was designed to escape. If users delegate trust evaluation to Google or OpenAI, those institutions decide which sources are trustworthy. Control has not been distributed; it has been relocated. Central institutions cannot deliver ABC by definition.
\newpage
This argument is sufficient, but let us more closely inspect the mechanics of trust centralization. After all, ABC is not an end in itself. Chapter 1 introduced ABC as a means to address concrete problems: privacy erosion, copyright disputes, data siloing, concentration of power, and competitive disadvantage against centralized adversaries. Even if we abandoned the goal of ABC, these problems (and the stakeholders who care about them) would not disappear. They would continue to obstruct institutional paths to trust evaluation at scale.

Consider data owners. As Chapter 2 documented, 6+ orders of magnitude of data remains siloed because data owners do not trust institutional intermediaries with their information \cite{grybauskas2023twitter, obrien2025reddit, grynbaum2023times, privacy_blocks_medical_sharing}. This resistance is rational. Data owners are one of millions; the institution has no particular stake in protecting any single owner's interests. Without mutual stakes, data owners have no mechanism to hold institutions accountable, only the hope that the institution will behave well (often in the face of evidence they will not, e.g. privacy and copyright violations).

Consider governments. The company Klout processed forty-five billion social interactions daily to score 750 million users \cite{rao2015klout}, attracted \$200 million in acquisition value, and served over two thousand business partners \cite{fortune2014klout}. Western society shut it down anyway... the day GDPR took effect \cite{slate2018klout}. Privacy regulation made Klout's level of trust evaluation illegal by making its level of surveillance illegal. Yet, GDPR is not a local legislation targeted a specific companies like Klout, it is a European wide paradigm shift in privacy expectations, inspiring similar legislation around the world \cite{simmons2022gdpr}.

Or consider accountability generally within western society. General trust evaluation at scale requires significant surveillance and likely a significant power imbalance between the surveil-er and surveilled. Yet, democratic values prohibit comprehensive surveillance. There is no policy that satisfies both constraints, or a widely deployed technology that circumvents them. Caught between these imperatives, Western societies pay the costs of both while achieving the full benefits of neither.

The result is visible in current online infrastructure. A handful of social media platforms observe and manage approximately 8.5\% of all waking human experience on earth... four trillion hours annually \cite{hootsuite2024}.\footnote{Calculated as 4 trillion hours divided by approximately 47 trillion global waking hours (8 billion people $\times$ 16 hours/day $\times$ 365 days).} This is surveillance at civilizational scale, yet the online trust verification problem remains woefully unsolved: half of web traffic is bots \cite{imperva2024bots}, up to 30\% of online reviews remain fraudulent \cite{donfro2013yelp, cross2022fake}, and state actors operate troll factories at scale \cite{fcdo2022trollfarm}. Western platforms cannot even reliably verify that people online are human, let alone whether they are trustworthy sources of information on specific topics.

Centralization in the Western context fails because surveillance is limited and because of the power imbalance between user and platform. An institution serving millions need not depend on any single user, and a user who is one of millions cannot hold the institution accountable (except through some similarly powerful institution... which itself then has an analogous accountability relationship). And in response to the power imbalance, users are both wary of platform interests, and governments taking steps to curtial platform power (e.g., privacy and copyright law). 

According to the theory, centralization cannot deliver attribution-based control; it relocates control rather than distributing it. And in practice, centralization triggers exactly the resistance that blocks comprehensive trust evaluation: data owners withhold information from intermediaries they cannot hold accountable, and governments resist the surveillance and power imbalance such intermediaries would require. Taken together, the institutional path to trust verification at scale is closed.

\newpage
\subsection{The Individual Path}

The alternative is delegation through individuals: trust your friends and colleagues to evaluate sources on your behalf. This preserves what institutions cannot offer: strong-ties. Your friend has 150 relationships, not two billion; you are one of 150, not one of millions; and your friend depends on your continued relationship for support, reciprocity, and reputation within your shared community. If your friend misleads you, they lose something they value. The mutual stakes that make trust enforceable remain intact.

But individual delegation cannot reach the scale that competitive AI platforms require. The Dunbar limit applies to delegated relationships as surely as direct ones. If each friend evaluates within their limit of 150, and you have 150 friends, your network reaches at most 22,500 sources (assuming no mutual friends among them), at best 2+ orders of magnitude short of the millions of sources that competitive AI systems require.

Taken together, institutions fail because they struggle to secure a public mandate for sufficient surveillance, and because they cannot sustain mutual stakes at scale. Individuals preserve mutual stakes but cannot reach AI-level scale. Both paths appear closed, but notice that the failures now share a common structure: somewhere in the path, the ratio of connections (scale) to accountability (surveillance and mutual stakes) breaks down. Let us consider this more closely.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Third Why (Root Cause): The Branching Problem}
\label{sec:third_why}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{1411.3146/ch_4_5_v1.png}
\caption{Subset of concept graph highlighting the Third Why and focus of this section.}
\label{fig:ch4_third_why}
\end{figure}

Consider what happens when nodes of vastly different cardinality connect, pictured in Figure \ref{fig:branching_imbalance}. One institution sits in the middle, billions of sources connect on one side, billions of users on the other. Now ask: what stake does each party have in the relationship? The institution has two billion connections; if it loses one, it loses one two-billionth of its network. But a user has perhaps 150 relationships, constrained by the same cognitive limits that necessitated delegation in the first place. If the institution is one of their few trusted intermediaries for a particular topic (e.g. oncologist ratings), it may represent a significant fraction of their access to information.

\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{1411.3146/branching_imbalance.png}
\caption{The circle in the middle represents an institutional intermediary with billions of data sources it consumes information from, and billions of outputs it shares information to. The circles on the far left and right represent individuals, each limited to 150 strong-tie relationships.}
\label{fig:branching_imbalance}
\end{figure}

The stakes are asymmetric. The institution can betray counterparties at negligible cost; their departure costs it nothing it would notice. Mutual stakes require that both parties in each relationship lose something meaningful if the relationship fails. When one side of an edge has two billion connections and the other side has 150, the asymmetry is structural, and accountability cannot survive it.

\vspace{5px}
\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{1411.3146/branching_balance_v3.png}
\caption{The circles on the far left and right represent individuals producing data and consuming insights (respectively), each limited to 150 strong-tie relationships. The circle in the middle also represents an individual, responsible for making trust decisions in the social graph, and also limited to 150 strong-tie relationships.}
\label{fig:branching_balance}
\end{figure}

Now consider what happens when nodes operate at comparable scale, pictured in Figure \ref{fig:branching_balance}. If you connect to 150 friends, and each friend connects to 150 sources, you reach 22,500 sources—far short of the million-source scale competitive systems require. To reach a million through only two hops, your 150 friends would each need roughly 7,000 connections. But a friend with 7,000 connections recreates the asymmetry problem: you are one of 7,000, your stake in the relationship is unchanged, but their stake in you has diluted significantly. The accountability that made your friendship trustworthy does not survive the transition. Alternatively, if instead your friends each connected to only a handful of sources (perhaps 10), reach would shrink further, dependency would concentrate on those few connections, and the network would fail to achieve competitive scale. The failure mode is not merely "too many connections" but deviation from Dunbar-scale in either direction.

\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{1411.3146/side_by_side_imbalance_v4.png}
\caption{Conventional AI (left) cultivates cardinality imbalance between data sources / users and itself. Network-source AI (right) insufficiently scales due to Dunbar's limit.}
\label{fig:side_by_side_imbalance}
\end{figure}

This is the root beneath both failures. The institutional path fails because institutions operate far above Dunbar scale while their users cannot... billions of connections on one side, hundreds on the other, creating asymmetric stakes that make accountability impossible. The individual path fails because short paths to competitive scale require someone in the chain to exceed Dunbar limits, recreating the same imbalance one hop away. Both are instances of the same architectural constraint: any path to scale that includes a node deviating from Dunbar-scale operation, connected to nodes that do not. This activates the responses... weaker parties that hide data, and governments that pass laws which protect the weaker from the stronger, introducing friction to coordination in the process (i.e., privacy laws hindering verification ability). This problem cascades to sustain long-term pressures against free society (i.e. privacy, autonomy, personal property, etc.) and exposure to fraud and deception (i.e. fake accounts, gaming of metrics, etc.). But these problems rest on a crucial central root: the branching problem.

\begin{definition}[The Branching Problem]
\label{def:branching_problem}
Trust cannot traverse non-Dunbar-scale nodes. Nodes operating above Dunbar scale ($k \gg 150$) cannot sustain strong-tie relationships with any counterparty. Nodes operating below it ($k \ll 150$) underutilise capacity and concentrate dependency. Imbalance between connected nodes ($k_1 \gg k_2$) creates asymmetric stakes that undermine accountability. Trust breaks wherever nodes deviate from Dunbar-scale operation.
\end{definition}

\newpage

Contemporary AI systems exhibit extreme deviation from Dunbar-scale at both ends of their architecture: at training time, one model connects to billions of sources; at inference time, one model connects to billions of users. Both points constitute nodes operating orders of magnitude above relational capacity, connected to nodes operating within it. Network-source AI, as specified in Chapters 2 and 3, does not escape this constraint. Deep voting requires trust weights, and structured transparency protects those weights, but neither specifies how weights are formed. If users delegate weight-formation to platforms operating at billion-scale, the branching problem reappears at the delegation point, and NSAI inherits exactly the accountability failures it was designed to escape.

\begin{definition}[The Branching Puzzle]
\label{def:branching_puzzle}
Competitive AI systems require trust paths to billions of sources, yet the branching problem prohibits any node from connecting to more than $\sim$150 counterparties. Short paths to billions require high-branching nodes; high-branching nodes break trust. The branching puzzle: how can trust delegation reach global scale when trust fails at any node exceeding Dunbar-scale operation?
\end{definition}

This is where the analysis arrives. Chapters 2 and 3 specified how to build AI systems that preserve attribution through aggregation and protect control through transparency. But neither chapter addressed where trust weights come from. Without a solution to the branching puzzle, users must either delegate weight-formation to high-branching platforms (undermining attribution-based control and inheriting the accountability failures that motivated NSAI) or accept reach limited to a few thousand sources. The next section will begin to explore a solution.

\begin{tcolorbox}[
enhanced,
breakable,
colback=white,
colframe=gray,
boxrule=0.5pt,
left=8pt,
right=8pt,
top=8pt,
bottom=8pt,
title=The Oncologist and the Branching Puzzle]
A patient receives a troubling test result and needs an oncologist. Two paths exist.
\\
\\
\textbf{The platform path.} The patient consults a review platform with two billion users and tens of thousands of oncologist reviews. The platform's algorithm surfaces a five-star physician. The patient books an appointment, receives treatment, and dies eighteen months later from a cancer that a competent oncologist would have caught. The review was fake... posted by a marketing firm working for the oncologist and undetected by the platform. The patient is dead, but what does the platform lose? One two-billionth of its user base. The loss is invisible to the platform; the accountability is virtually nonexistent. The platform continues operating, its revenue unaffected and algorithm unchanged. This is the high-branching failure: billions of connections on one side, hundreds on the other, and no mechanism for consequences to flow.
\\
\\
\textbf{The friend path.} The patient asks their 150 friends whether anyone knows a good oncologist. No one does—oncology is specialised, and the patient's immediate network lacks relevant experience. The patient asks friends to ask their friends: 150 times 150 yields 22,500 people, still no oncologist referral with direct experience. The information exists somewhere in the social fabric, but three hops away, four hops away, beyond the reach of the patient's strong-tie network. The patient gives up and returns to the platform. This is the low-branching failure: Dunbar-limited connections at every node, and no path long enough to reach relevant sources.
\\
\\
Both paths fail. One has reach without accountability. The other has accountability without reach. The branching puzzle: is there a third path that offers both?
\end{tcolorbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Third Hypothesis: Reach via Recursion}
\label{sec:third_hyp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

There is a third path: recursion. If each node maintains $k$ connections and trust propagates through chains of depth $H$, theoretical reach scales as $O(k^H)$. Fifty connections across six hops has the theoretical potential to reach fifteen billion sources:
\begin{align}
\text{Reach} = k^H = 50^6 = 15{,}625{,}000{,}000
\end{align}

This is mathematical possibility, not a claim about actual social networks, but the pattern has precedent. Word of mouth has operated this way for thousands of years; the spread of fire-making, agriculture, and religious ideas all propagated through peer networks without central coordination. Academic citation networks, PageRank, and Wikipedia exhibit similar structure: researchers cite dozens of sources, pages link to dozens of others, editors assess within their capacity, and no central node determines outcomes \cite{lawrence1999digital, page1999pagerank, reagle2010good}.

\subsection{Reframing the Problem}

This perspective reframes the problems documented in the three Whys as one of scaling existing word-of-mouth mechanisms. Word of mouth involves several operations: sharing information with contacts, filtering what to pass along, aggregating responses, and weighting sources by trust. Thus, another way to view the problem is that communication infrastructure has upgraded word of mouth's mechanisms unevenly for centuries. The printing press scaled sharing while leaving filtering and aggregation at human speed. Radio and television extended this pattern. Digital infrastructure continued it... sharing now operates instantly, but filtering, aggregating, and trust-weighting still require human attention at each hop. And when sharing scales but aggregation does not, bottlenecks form at whoever can aggregate, concentrating power into an attention economy \cite{goldhaber1997attention}. The branching problem and the corresponding collapse of mutual stakes trace to the uneven development of communication infrastructure.

\subsection{The Requirement}

The mathematics establish that reach without branching is possible in principle. Two questions remain before this possibility becomes useful. First, does trust survive recursive depth? Information passing through six hops might degrade at each link—the telephone game. If the chain corrupts what it carries, reach is worthless. The Second Hypothesis addresses this question. Second, can coordination costs be reduced enough to compete with platforms? Real networks impose costs at every hop: phone calls, explanations, waiting for callbacks. A chain that could theoretically reach a relevant expert in four hops might take weeks in practice. Word of mouth has the right structure but the wrong speed. The First Hypothesis addresses this question.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Second Hypothesis: Delegation Without Concentration}
\label{sec:second_hyp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The Third Hypothesis established that recursive networks can achieve global reach without high-branching nodes, but mathematical possibility does not guarantee practical implementation. The Second Why documented a historical pattern: delegation systems either centralize under competitive pressure or fail to reach useful scale. The question is whether recursive trust propagation can avoid this pattern.

The mechanism described below scales without concentrating because no node can substitute its judgment for another's. Whether this architectural property proves sufficient against competitive dynamics is an empirical question that this thesis does not resolve, but the structure differs from previous delegation systems in ways worth examining.

\subsection{The Propagation Mechanism}

Each participant maintains trust assessments $w_{u,v}^d \in [0,1]$ for direct contacts, where $d$ indexes the domain. Trust propagates via iterated computation. Let $T_h(n)$ denote participant $n$'s trust score at hop $h$. Trust originates with the querying user and propagates as each participant distributes to contacts:
\begin{align}
\text{Map}_n(T_h(n)) = \{(v, T_h(n) \cdot w_{n,v}^d) : v \in \text{contacts}(n)\}
\end{align}

Recipients aggregate incoming trust via probabilistic OR:
\begin{align}
T_{h+1}(m) = 1 - \prod_{i=1}^{k}(1 - T_h(n_i) \cdot w_{n_i,m}^d)
\end{align}

After $H$ iterations, trust scores determine how much each source influences the response.

\subsection{Why It Scales}

The mechanism requires $O(k \cdot H)$ operations per participant per query, where $k$ is the number of contacts and $H$ is the number of hops. With $k \approx 50$ and $H \approx 6$, each participant performs roughly 300 operations regardless of total network size. The computational cost of reaching billions of sources through recursive propagation does not require any participant to perform billions of operations.

\subsection{Why It Decentralizes}

Each participant controls only their own weights (the assessments they maintain about their own contacts) and no participant controls anyone else's. When trust propagates through a chain, each node applies its own assessments, but those assessments affect only the next hop. No node observes the full chain from query to response, and no node substitutes its judgment for the judgment of other participants in the network.

Path redundancy reinforces this distribution if network structure approximates the mathematics from Section~\ref{sec:third_hyp}. Whether real adoption produces such structure, or whether clustering creates de facto bottlenecks despite the architecture, remains an empirical question. The architecture makes concentration structurally difficult by ensuring that influence over outcomes requires controlling weights across many independent participants. It does not make concentration impossible, and competitive dynamics could produce outcomes the architecture alone cannot prevent.

\subsection{The Requirement}

The mechanism scales and decentralizes in principle. After $H$ iterations, each source has a trust score $T_H(s)$ reflecting the paths through which trust reached it. Two requirements remain before this mechanism becomes practically useful: the trust scores must convert into a synthesized response, and participation must be cheap and secure enough that people actually use the system. The First Hypothesis addresses these requirements.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{First Hypothesis: The Machinery Exists}
\label{sec:first_hyp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The Second Hypothesis showed that recursive delegation scales and decentralizes in principle. Whether anyone uses it depends on two requirements: participation must be cheap enough to compete with platforms, and secure enough that honest participation is rational.

Chapters~\ref{ch:deep_voting} and~\ref{ch:structured_transparency} address both requirements, though empirical validation remains future work.

\subsection{What Deep Voting Provides}

Recursive consultation through social networks presently requires human effort at every hop: formulating requests, providing context, waiting for responses, following up when people forget. A chain that could theoretically reach a relevant expert in four hops might take days or weeks in practice. Meanwhile, platform queries from frontier AI systems can return results in seconds.

Deep voting addresses this gap by describing how this analog process could be done digitally, plausibly giving a path towards a significant reduction in cost and latency. It did this in part by separating weight specification from query-time coordination. Participants record trust assessments in advance, specifying which contacts they trust for which domains. When a query arrives, the system propagates trust according to these pre-recorded weights without requiring human attention. Trust scores convert to contribution weights:
\begin{align}
B(s) = \frac{T_H(s)}{\sum_{s' \in \mathcal{S}_q} T_H(s')}
\end{align}

The propagation computation itself is lightweight (matrix operations over sparse graphs). The synthesis step requires language model inference, which currently takes seconds. Whether end-to-end latency proves competitive with conventional platforms is an empirical question this thesis does not answer, but the architecture eliminates the human coordination bottleneck that made recursive consultation impractical. The difference between days of phone calls and seconds of computation is large enough that precise benchmarks are unlikely to change the qualitative conclusion.

\subsection{What Structured Transparency Provides}

Recording trust assessments in a digital system creates vulnerability that face-to-face word of mouth does not. If assessments are observable, employers could identify employees who distrust company sources, governments could identify citizens who trust opposition voices, and commercial actors could infer relationships for targeted manipulation. Rational participants facing these risks would either decline to participate or provide strategically dishonest assessments.

Structured transparency addresses this by enabling something analogous to end-to-end encryption for the trust propagation process. Information flows from participants to intended recipients without requiring any central intermediary to observe unencrypted weights or messages. Input and output privacy allow trust scores to propagate over encrypted data. Input and output verification verifies correct execution without revealing inputs or intermediate variables.

Whether these guarantees prove sufficient to motivate honest participation is ultimately an empirical question about user behavior. What the architecture provides is the removal of a technical barrier: participants \textit{can} record honest assessments without exposure, even if the thesis cannot guarantee that they \textit{will}.

\subsection{The Gap Closes}

The First Why identified a gap between cognitive capacity for trust evaluation—approximately 150 relationships—and the number of sources requiring evaluation—approximately $10^9$. Seven orders of magnitude.

Recursive depth addresses the gap structurally. Deep voting replaces human coordination with computation, making recursive consultation plausibly fast enough to compete with platforms. Structured transparency protects assessments from observation, making honest participation possible without exposure.

The contribution is not the pattern of recursive trust propagation, which has operated in human societies for as long as those societies have existed. The contribution is identifying the infrastructure barriers—coordination cost and privacy risk—and providing machinery that addresses them. Whether that machinery proves sufficient in practice remains for future work to establish.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\label{sec:ch4_conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This chapter asked whether the problems documented in the three Whys admit a solution. The Third Hypothesis established that recursive networks can achieve global reach without requiring any node to exceed Dunbar-scale operation. The Second Hypothesis showed that a specific propagation mechanism scales computationally and distributes authority structurally. The First Hypothesis showed that deep voting addresses coordination cost and structured transparency addresses privacy risk. 

What the chapter establishes is architectural possibility, not guaranteed outcome. Whether the machinery proves sufficient to motivate adoption, and whether the architectural properties prove robust against competitive dynamics, remain empirical questions.

The chapter also proposed a reframe. The three Whys traced concentration not to any fundamental constraint on distributed trust, but to uneven development of communication infrastructure—sharing scaled while filtering, aggregating, and trust-weighting remained at human speed. The contribution of this thesis is not the pattern of recursive trust propagation, which is ancient. The contribution is identifying the infrastructure gaps that prevented the pattern from operating at scale, and surveying and re-framing recently proposed machinery that addresses them.

\chapter{Conclusion}

This thesis opened by observing that several apparently distinct problems in contemporary AI systems share a common structural feature: the absence of attribution-based control. When AI systems add information without attribution, users cannot verify claims. When systems copy training data without preserving contributor control, data owners rationally withhold contributions. When systems branch into multiple instances without accountability mechanisms, governance becomes intractable. The introduction traced how these technical absences cascade upward into consequences at individual, institutional, societal, and geopolitical levels, then asked whether attribution-based control is technically feasible with existing machinery. Chapters 2, 3, and 4 surveyed techniques in deep learning, cryptography, and distributed systems that could provide the three components of ABC. This conclusion returns to the cascading consequences to examine what changes if ABC proves achievable.

At the individual level, the introduction identified hallucination and disinformation as characteristic problems. Users encounter claims they cannot verify, generated by systems whose reasoning processes remain opaque. Deep voting, as surveyed in Chapter 2, does not eliminate hallucination. Models will continue to produce outputs unsupported by their training data. What changes is the possibility of source inspection. When each token of output can be traced to the training examples that most influenced its generation, users gain the architectural prerequisite for verification. They can examine whether claimed facts derive from sources they consider reliable, whether confident assertions rest on thin evidential bases, whether patterns reflect genuine regularities or artifacts of biased sampling. This does not automatically produce truth, but it provides the foundation upon which verification practices could be built. The individual-level problems identified in the introduction require this foundation.

At the institutional level, the introduction observed that data owners have rational incentives to withhold contributions from AI systems. Contributing data to centralized training pipelines means surrendering control over how that data will be used, who will benefit from insights derived from it, and whether contributors will receive any attribution or compensation. The introduction estimated that this dynamic has locked away six or more orders of magnitude of potentially valuable training data. Structured transparency, as surveyed in Chapter 3, addresses the technical barrier that makes this withholding rational. When cryptographic mechanisms can enforce access policies, when contributors can specify conditions under which their data may be used, when audit trails can verify compliance with those conditions, the calculus changes. Data owners might contribute to systems where they retain meaningful control even as their contributions enable collective intelligence. The institutional barriers identified in the introduction were rational responses to architectural limitations. Removing those limitations does not guarantee participation, but it removes the technical obstacle that made withholding the only rational choice.

At the societal level, the introduction raised questions of governance and alignment that currently dominate discourse in the field. How do we ensure that AI systems behave in accordance with human values? How do we maintain meaningful human control over systems that may eventually exceed human capabilities in many domains? Current approaches typically involve centralized actors tuning systems on samples of human feedback, hoping that the tuning generalizes appropriately. The recursive structures surveyed in Chapter 4 suggest an alternative architecture. If AI systems require ongoing contributions from distributed sources, and if contributors retain the ability to withdraw those contributions, then alignment becomes structurally enforced rather than centrally imposed. A system that violates contributor values sufficiently to trigger widespread withdrawal loses capability. This is not a complete solution to alignment. Contributors might coordinate to pursue objectives harmful to non-contributors. The mechanisms by which contributors express values through contribution decisions remain underspecified. The assumption that contributors can evaluate system behavior at the speed required for effective feedback may prove optimistic. But the architectural possibility differs from current approaches: collective control without requiring trust in a small number of companies or governments to tune systems appropriately on humanity's behalf.

At the geopolitical level, the introduction noted a strategic tension facing liberal democracies. Authoritarian states can mandate resource centralization in ways that liberal democracies cannot without violating foundational principles. If AI capability correlates with centralization, this creates structural disadvantages for democratic societies. The recursive delegation mechanisms in Chapter 4 suggest a different scaling dynamic. If capability can emerge through voluntary coordination of distributed resources rather than coerced aggregation, liberal democracies might access resources unavailable to authoritarian centralization. Citizens globally might contribute to systems that preserve their control in ways they would never contribute to systems under authoritarian control. This is speculative, and competitive dynamics might override architectural properties. But the possibility exists that voluntary coordination at scale could match or exceed what coerced centralization achieves, providing a path toward capable AI systems that does not require compromising democratic values to remain competitive.

The pattern across these four levels suggests a reframe. The problems identified in the introduction were not primarily failures of intention or governance. They were consequences of infrastructure. The internet scaled the capacity to share information globally, but it did not scale the capacity to filter, aggregate, and verify information at corresponding rates. Those operations still required human attention at every step. When synthesis required human cognition, bottlenecks formed wherever humans could be positioned to aggregate. Platforms emerged to provide synthesis functions because the underlying infrastructure could not. The resulting concentration of control was not a choice but an architectural necessity given available technology. The thesis contribution is not the pattern of recursive trust propagation. That pattern is ancient. Humans have always extended their reach by trusting others who trust others, delegating judgment through networks of credibility built over time. The contribution is identifying specific technical barriers that prevented this pattern from operating at machine speed, and surveying existing techniques in deep learning, cryptography, and distributed systems that could address those barriers. Word-of-mouth operated at the speed of human conversation. The machinery surveyed in this thesis could allow analogous trust propagation to operate at the speed of computation.

This framing clarifies both what the thesis claims and what it does not. The claim is that ABC appears technically feasible using existing techniques. The evidence is the survey of those techniques across three chapters, demonstrating that deep voting can provide attribution for model outputs, that cryptographic mechanisms can enforce contributor control over data usage, and that recursive structures can enable coordination without central authorities. These are technical possibilities, not deployment realities. Whether the techniques compose effectively at scale, whether the computational overhead proves acceptable, whether the coordination dynamics produce the alignment effects suggested above: these remain empirical questions requiring investigation beyond what a survey thesis can provide.

The thesis also does not claim that technical architecture determines social outcomes. Architecture enables and constrains, but human choices operate within those constraints. Even if ABC proves technically feasible, adoption depends on incentives, network effects, competitive dynamics, and institutional decisions that technical analysis cannot predict. The surveillance infrastructure that currently characterizes AI development emerged not because alternatives were impossible but because the available alternatives were not yet competitive. Whether ABC-enabled alternatives become competitive depends on factors beyond architecture. The honest claim is narrower: if these barriers to collective intelligence without centralized control prove surmountable, then concentration of AI capability is a choice rather than a necessity. The thesis has tried to show that the barriers may indeed be surmountable.

The introduction ended by suggesting that the future of AI need not be a race to centralize. The chapters that followed surveyed technical machinery that could make that suggestion more than aspiration. Whether the machinery works as described, whether it composes into systems that function at scale, whether adoption dynamics favor its deployment: these questions remain open. But the architectural possibility now has technical substance behind it. The future of AI might instead become a race to connect, through systems that preserve attribution and control even as they enable collective intelligence exceeding what any centralized system could achieve. Determining whether this possibility can become reality is work that remains to be done.


\addcontentsline{toc}{chapter}{References}
\bibliography{references}

\chapter{Appendix I: Estimated Global Compute Distribution and Capacity}

This appendix documents the global distribution of computing power as of Q4 2024, with particular focus on AI accelerators and general-purpose computing hardware. All computing power is measured in floating point operations per second (FLOP/s), enabling direct comparison between different types of computing devices.

\section{Methodology}

Our analysis tracks two primary categories of computing power:

\begin{enumerate}
\item \textbf{Specialized AI Hardware}: Data center GPUs (e.g., NVIDIA H100s, AMD MI300X), TPUs, and custom ASICs (e.g., Microsoft MAIA, Meta MTIA)

\item \textbf{General Purpose Computing}: Consumer devices including smartphones, personal computers, and gaming consoles
\end{enumerate}

The analysis covers 2023-2024, with several key methodological considerations:

\subsection{Time Period Normalization}
Many source datasets span 2022-2024. For these three-year totals, we estimate the 2023-2024 portion as 80\% of the total, reflecting:
\begin{itemize}
\item Accelerating deployment rates through the period
\item Increased manufacturing capacity in later years
\item Reported quarterly growth patterns \cite{mann2024ai}
\end{itemize}

\subsection{Infrastructure Accounting}
To account for infrastructure deployed before our analysis period that remains in active use, we apply a 1.3$\times$ multiplier to new deployments. This multiplier derives from:
\begin{itemize}
\item Historical hardware retirement rates \cite{epoch2024hardware}
\item Typical infrastructure lifecycle lengths
\item Reported infrastructure retention patterns \cite{mann2024ai}
\end{itemize}

\subsection{Consumer Device Calculations}
For devices like smartphones, we calculate:
\[
\text{Total Capacity} = \text{Quarterly Sales} \times \text{Number of Quarters} \times \text{Device Performance}
\]

Key assumptions include:
\begin{itemize}
\item 275M average quarterly smartphone sales \cite{ming2024nvidia}
\item 8 quarters (covering 2023-2024)
\item 80/20 split between iPhone/Android market share
\item Device performance from manufacturer specifications
\end{itemize}

\section{Results}

\subsection{Hardware Specifications}
We begin by documenting the performance characteristics of key computing hardware. Table \ref{tab:hardware-specs} shows peak theoretical performance for major AI accelerators and consumer devices.

\begin{table}[htbp]
\centering
\caption{Hardware Specifications (2023-2024)}
\label{tab:hardware-specs}
\small
\begin{tabular}{llrl}
\toprule
Hardware & Type & Peak Performance & Source \\
& & (FLOP/s) & \\
\midrule
\multicolumn{4}{l}{\textit{Data Center AI Chips}} \\
AMD MI300X OAM & GPU & $2.61 \times 10^{15}$ & \cite{mann2024ai} \\
NVIDIA H100 & GPU & $9.89 \times 10^{14}$ & \cite{epoch2024hardware} \\
NVIDIA A100 & GPU & $3.12 \times 10^{14}$ & \cite{epoch2024hardware} \\
Microsoft MAIA & ASIC & $8.00 \times 10^{14}$ & \cite{mann2024ai} \\
Meta MTIA & ASIC & $3.54 \times 10^{14}$ & \cite{mann2024ai} \\
AWS Tranium & ASIC & $2.50 \times 10^{14}$ & \cite{mann2024ai} \\
AWS Inferentia & ASIC & $1.92 \times 10^{14}$ & \cite{mann2024ai} \\
\addlinespace
\multicolumn{4}{l}{\textit{Consumer Devices}} \\
iPhone 14 Pro & Mobile SoC & $2.00 \times 10^{12}$ & \cite{ming2024nvidia} \\
Samsung S24 & Mobile SoC & $3.40 \times 10^{12}$ & \cite{ming2024nvidia} \\
Average PC CPU & CPU & $4.08 \times 10^{10}$ & \cite{epoch2024hardware} \\
Average PC GPU & GPU & $4.60 \times 10^{12}$ & \cite{epoch2024hardware} \\
PS5 & Console & $1.03 \times 10^{13}$ & \cite{mann2024ai} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Estimated New Hardware Deployments}
Table \ref{tab:nvidia-shipments} documents NVIDIA's GPU shipments, which represent the majority of new AI computing capacity deployed during this period.
\begin{table}[htbp]
\centering
\caption{NVIDIA GPU Shipments (2023-2024)}
\label{tab:nvidia-shipments}
\small
\begin{tabular}{lrrl}
\toprule
Type & Units & Computing Power & Source/Calculation \\
& & (FLOP/s) & \\
\midrule
A100s (2023) & 2,260,000 & $7.05 \times 10^{20}$ & Units × $3.12 \times 10^{14}$ FLOP/s per A100 \\
H100s (2023) & 1,500,000 & $1.48 \times 10^{21}$ & Units × $9.89 \times 10^{14}$ FLOP/s per H100 \\
A100s (2024) & 2,000,000 & $6.24 \times 10^{20}$ & Units × $3.12 \times 10^{14}$ FLOP/s per A100 \\
H100s (2024) & 2,000,000 & $1.98 \times 10^{21}$ & Units × $9.89 \times 10^{14}$ FLOP/s per H100 \\
\midrule
Total 2022-2024 & 7,760,000 & $4.79 \times 10^{21}$ & Sum of all rows above \\
\bottomrule
\end{tabular}
\end{table}

These shipments are distributed across major cloud providers and AI companies. Table \ref{tab:cloud-deployments} details how this computing capacity is allocated, including both NVIDIA GPUs and custom AI accelerators.

\begin{table}[htbp]
\centering
\caption{Estimated Major Cloud Provider Deployments (2022-2024)}
\label{tab:cloud-deployments}
\small
\begin{tabular}{llrl}
\toprule
Provider & Hardware Type & Computing Power & Source/Calculation \\
& & (FLOP/s) & \\
\midrule
\multicolumn{4}{l}{\textit{Microsoft/OpenAI}} \\
& H100 equivalents & $6.53 \times 10^{20}$ & \cite{epoch2024hardware} \\
& AMD MI300X & $2.51 \times 10^{20}$ & \cite{mann2024ai} \\
& MAIA & $1.58 \times 10^{20}$ & \cite{mann2024ai} \\
Total New & & $1.06 \times 10^{21}$ & Sum of three rows above \\
2023-2024 (80\%) & & $8.50 \times 10^{20}$ & Total New × 0.8 (2-year portion of 3-year total) \\
Total Q4 2024 & & $1.38 \times 10^{21}$ & 2023-2024 total × 1.3 (infrastructure multiplier) \\
\addlinespace
\multicolumn{4}{l}{\textit{Amazon/Anthropic}} \\
& H100/A100 & $2.87 \times 10^{20}$ & \cite{epoch2024hardware} \\
& Inferentia & $1.75 \times 10^{20}$ & Units × $1.92 \times 10^{14}$ FLOP/s per chip \\
& Tranium & $9.15 \times 10^{19}$ & Units × $2.50 \times 10^{14}$ FLOP/s per chip \\
Total New & & $5.53 \times 10^{20}$ & Sum of three rows above \\
2023-2024 (80\%) & & $4.42 \times 10^{20}$ & Total New × 0.8 \\
Total Q4 2024 & & $7.19 \times 10^{20}$ & 2023-2024 total × 1.3 \\
\addlinespace
\multicolumn{4}{l}{\textit{Google/DeepMind}} \\
& TPUs (Q1 2023) & $2.93 \times 10^{19}$ & \cite{epoch2024hardware} \\
& TPUs (Q4 2024) & $9.10 \times 10^{20}$ & \cite{epoch2024hardware} \\
& New TPUs 2023-2024 & $8.81 \times 10^{20}$ & Q4 2024 TPUs - Q1 2023 TPUs \\
& H100/A100 & $3.16 \times 10^{20}$ & \cite{mann2024ai} \\
Total New GPUs & & $3.16 \times 10^{20}$ & Same as H100/A100 row \\
Total New TPUs & & $8.81 \times 10^{20}$ & Same as New TPUs row \\
New GPUs + TPUs & & $1.20 \times 10^{21}$ & Sum of Total New GPUs and TPUs \\
Total Q4 2024 & & $1.23 \times 10^{21}$ & (New GPUs + TPUs) × 1.3 - Q1 2023 TPUs \\
\addlinespace
\multicolumn{4}{l}{\textit{Meta}} \\
& A100 & $6.68 \times 10^{18}$ & \cite{epoch2024hardware} \\
& H100 & $3.46 \times 10^{20}$ & Units × $9.89 \times 10^{14}$ FLOP/s per H100 \\
& H100/A100 & $3.96 \times 10^{20}$ & Sum of A100 and H100 rows \\
& AMD MI300X & $4.52 \times 10^{20}$ & Units × $2.61 \times 10^{15}$ FLOP/s per MI300X \\
& MTIA & $5.31 \times 10^{20}$ & Units × $3.54 \times 10^{14}$ FLOP/s per MTIA \\
Total New & & $1.38 \times 10^{21}$ & Sum of all hardware rows \\
2023-2024 (80\%) & & $1.10 \times 10^{21}$ & Total New × 0.8 \\
Total Q4 2024 & & $1.79 \times 10^{21}$ & 2023-2024 total × 1.3 \\
\addlinespace
\multicolumn{4}{l}{\textit{Other NVIDIA Customers}} \\
& H100/A100 & $1.38 \times 10^{21}$ & \cite{ming2024nvidia} \\
& MI300X & $1.52 \times 10^{20}$ & \cite{mann2024ai} \\
Total New & & $1.54 \times 10^{21}$ & Sum of H100/A100 and MI300X rows \\
2023-2024 (80\%) & & $1.23 \times 10^{21}$ & Total New × 0.8 (2-year portion of 3-year total) \\
Total Q4 2024 & & $2.00 \times 10^{21}$ & 2023-2024 total × 1.3 (infrastructure multiplier) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Estimated Base Computing Capacity}
To establish total worldwide computing capacity, we first document the base capacity that existed at the start of our analysis period (Q1 2023).
\begin{table}[htbp]
\centering
\caption{Base Computing Capacity (Q1 2023)}
\label{tab:base-capacity}
\small
\begin{tabular}{lrl}
\toprule
Type & Computing Power & Source/Calculation \\
& (FLOP/s) & \\
\midrule
TPU & $2.93 \times 10^{19}$ & \cite{epoch2024hardware} \\
GPU & $3.95 \times 10^{21}$ & \cite{epoch2024hardware} \\
\midrule
Total & $3.98 \times 10^{21}$ & Sum of TPU and GPU rows \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{Estimated New Computing Capacity (Q1 2023 - Q4 2024)}
\label{tab:new-capacity}
\small
\begin{tabular}{lrl}
\toprule
Category & Computing Power & Source/Calculation \\
& (FLOP/s) & \\
\midrule
\multicolumn{3}{l}{\textit{Specialized AI Hardware}} \\
NVIDIA GPUs (source 1) & $4.82 \times 10^{21}$ & \cite{epoch2024hardware} \\
NVIDIA GPUs (source 2) & $4.79 \times 10^{21}$ & Sum of all NVIDIA shipments from Table \ref{tab:nvidia-shipments} \\
Avg NVIDIA GPUs & $4.81 \times 10^{21}$ & Average of two NVIDIA GPU sources above \\
New Google TPUs & $8.81 \times 10^{20}$ & Q4 2024 TPUs - Q1 2023 TPUs from Table \ref{tab:cloud-deployments} \\
New Cloud Hardware & $2.35 \times 10^{21}$ & Sum of all non-NVIDIA cloud hardware from Table \ref{tab:cloud-deployments} \\
\addlinespace
\multicolumn{3}{l}{\textit{Consumer Devices}} \\
Active iPhones & $5.98 \times 10^{21}$ & 275M units/quarter × 8 quarters × 80\% × $2.00 \times 10^{12}$ FLOP/s per iPhone \\
Active Androids & $1.50 \times 10^{21}$ & 275M units/quarter × 8 quarters × 20\% × $3.40 \times 10^{12}$ FLOP/s per Android \\
PC CPUs & $1.96 \times 10^{19}$ & 60M units/quarter × 8 quarters × $4.08 \times 10^{10}$ FLOP/s per CPU \\
PC GPUs & $2.21 \times 10^{21}$ & 60M units/quarter × 8 quarters × $4.60 \times 10^{12}$ FLOP/s per GPU \\
Game Consoles & $8.64 \times 10^{20}$ & 84M units × $1.03 \times 10^{13}$ FLOP/s per console \\
\midrule
Total New & $2.82 \times 10^{22}$ & Sum of all rows above \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Estimated Global Computing Distribution}
Finally, Table \ref{tab:total-capacity} presents the complete worldwide distribution of computing capacity as of Q4 2024, combining base capacity, new deployments, and applying our temporal adjustment factors.
\begin{table}[htbp]
\centering
\caption{Estimated Total Worldwide Computing Capacity (Q4 2024). Note that these calculations exclude cloud CPU compute.}
\label{tab:total-capacity}
\small
\begin{tabular}{lrrl}
\toprule
Category & Computing Power & Share & Source/Calculation \\
& (FLOP/s) & (\%) & \\
\midrule
\multicolumn{4}{l}{\textit{Cloud/AI Providers}} \\
Meta & $1.79 \times 10^{21}$ & 5.57 & From Table \ref{tab:cloud-deployments} Total Q4 2024 \\
Microsoft/OpenAI & $1.38 \times 10^{21}$ & 4.29 & From Table \ref{tab:cloud-deployments} Total Q4 2024 \\
Google/DeepMind & $1.23 \times 10^{21}$ & 3.81 & From Table \ref{tab:cloud-deployments} Total Q4 2024 \\
Amazon/Anthropic & $7.19 \times 10^{20}$ & 2.23 & From Table \ref{tab:cloud-deployments} Total Q4 2024 \\
\addlinespace
\multicolumn{4}{l}{\textit{Consumer Computing}} \\
Smartphones & $7.48 \times 10^{21}$ & 23.23 & Sum of Active iPhones and Androids from Table \ref{tab:new-capacity} \\
PC CPUs/GPUs & $2.23 \times 10^{21}$ & 6.92 & Sum of PC CPUs and GPUs from Table \ref{tab:new-capacity} \\
Game Consoles & $8.64 \times 10^{20}$ & 2.68 & From Table \ref{tab:new-capacity} \\
\addlinespace
Other Cloud/Pre-2023 & $1.65 \times 10^{22}$ & 51.28 & Base capacity × 1.3 + remaining new capacity not allocated above \\
\midrule
Total & $3.22 \times 10^{22}$ & 100.00 & Sum of all rows above \\
\bottomrule
\end{tabular}
\end{table}

\section{Analysis and Implications}

\subsection{Distribution of Computing Power}
The results reveal several key insights about global computing power distribution:

\begin{enumerate}
\item \textbf{Cloud Concentration}: The four largest cloud/AI providers (Meta, Microsoft/OpenAI, Google/DeepMind, and Amazon/Anthropic) collectively control 15.9\% of global computing power. This represents a significant concentration of high-performance computing resources.

\item \textbf{Consumer Device Significance}: Despite lower per-unit performance, consumer devices collectively represent 32.83\% of global computing power, with smartphones alone accounting for 23.23\%. This highlights the massive scale of deployed consumer hardware.

\item \textbf{Legacy Infrastructure}: Pre-2023 and other cloud infrastructure remains significant, representing 51.28\% of total capacity. This suggests substantial inertia in computing infrastructure deployment.
\end{enumerate}

\subsection{Growth Patterns}
The data reveals several notable trends in computing power growth:

\begin{itemize}
\item \textbf{Accelerating Deployment}: New capacity added in 2023-2024 ($2.82 \times 10^{22}$ FLOP/s) is approximately 7 times larger than the Q1 2023 base capacity ($3.98 \times 10^{21}$ FLOP/s).

\item \textbf{Custom Hardware}: Major cloud providers are increasingly deploying custom AI accelerators (MAIA, MTIA, Inferentia) alongside NVIDIA GPUs, though these still represent a minority of total capacity.

\item \textbf{Consumer Updates}: The rapid replacement cycle of consumer devices (particularly smartphones) means this sector maintains a significant share of total computing power despite lower per-unit performance.
\end{itemize}

\subsection{Methodological Limitations}
Several factors affect the precision of these estimates:

\begin{enumerate}
\item \textbf{Utilization Rates}: Our analysis uses peak theoretical performance. Actual achieved performance varies significantly based on:
\begin{itemize}
    \item Workload characteristics
    \item Cooling and power constraints
    \item Network and memory bottlenecks
\end{itemize}

\item \textbf{Temporal Distribution}: The 80\% factor used for 2023-2024 portion of 2022-2024 deployments is a rough estimate based on the degree to which NVIDIA sales and chip FLOP capacity have greatly increased in the past two years \cite{macrotrends2024nvidia}.

\item \textbf{Consumer Device Usage}: Not all consumer devices are actively used for computation at any given time, potentially overestimating their contribution to global computing power.
\end{enumerate}

\chapter{Appendix II: Large Model Compute Rankings and GPU Capacity Utilization}

This appendix presents a comprehensive ranking of 182 notable AI models, combining data from Epoch AI's ``Notable AI Models'' database \cite{EpochNotableModels2024} with organizational compute capacity estimates from Appendix I. For each model, we track:

\begin{itemize}
\item Model name and developing organization(s)
\item Training compute requirements (FLOPs)
\item Lab/Cloud provider responsible for training
\item Parent organization's 2024 estimated peak annual FLOP capacity
\item Three metrics of organizational impact:
\begin{itemize}
    \item Share of organization's publicly known models: Training FLOPs divided by total known training FLOPs for that organization
    \item Share of peak annual FLOP budget: Training FLOPs divided by parent organization's 2024 estimated peak annual FLOP capacity
    \item Share of peak annual FLOP budget with 100x sweep: Same as above, but assuming each model required 100x more compute for development and testing
\end{itemize}
\end{itemize}

The data is presented in six tables, ordered by decreasing training compute requirements. This allows tracking the evolution of model scale over time and comparing relative organizational investments in different AI capabilities. Note that training compute estimates for the most recent models are based on publicly available information and may be incomplete or imprecise.

\begin{table}[htbp]
\centering
\caption{AI Model Training Compute Requirements (Part 1 of 6)}
\label{tab:model-compute-1}
\tiny
\begin{tabular}{lllrrrrrr}
\toprule
Model & Organization & Lab/Cloud & Train & Parent Org Peak & Model/Public & Model/Peak & Model/Peak \\
& & & FLOPs & Annual FLOPs & Models (\%) & Annual (\%) & w/100x (\%) \\
\midrule
Gemini 1.0 Ultra & Google DeepMind & Google DeepMind & $5.00 \times 10^{25}$ & $3.87 \times 10^{28}$ & 45.65 & 0.129 & 12.93 \\
Claude 3.5 Sonnet & Anthropic & Anthropic/Amazon & $4.98 \times 10^{25}$ & $2.27 \times 10^{28}$ & 69.74 & 0.220 & 21.96 \\
GPT-4o & OpenAI & Microsoft/OpenAI & $3.81 \times 10^{25}$ & $4.35 \times 10^{28}$ & 53.36 & 0.088 & 8.75 \\
Llama 3.1-405B & Meta AI & Meta AI & $3.80 \times 10^{25}$ & $5.65 \times 10^{28}$ & 66.32 & 0.067 & 6.72 \\
GPT-4 & OpenAI & Microsoft/OpenAI & $2.10 \times 10^{25}$ & $4.35 \times 10^{28}$ & 29.41 & 0.048 & 4.82 \\
Gemini 1.0 Pro & Google DeepMind & Google DeepMind & $1.83 \times 10^{25}$ & $3.87 \times 10^{28}$ & 16.71 & 0.047 & 4.73 \\
Claude 3 Opus & Anthropic & Anthropic/Amazon & $1.64 \times 10^{25}$ & $2.27 \times 10^{28}$ & 22.97 & 0.072 & 7.23 \\
Gemini 1.5 Pro & Google DeepMind & Google DeepMind & $1.58 \times 10^{25}$ & $3.87 \times 10^{28}$ & 14.43 & 0.041 & 4.09 \\
Llama 3-70B & Meta AI & Meta AI & $7.86 \times 10^{24}$ & $5.65 \times 10^{28}$ & 13.72 & 0.014 & 1.39 \\
GPT-4o mini & OpenAI & Microsoft/OpenAI & $7.36 \times 10^{24}$ & $4.35 \times 10^{28}$ & 10.31 & 0.017 & 1.69 \\
PaLM 2 & Google & Google DeepMind & $7.34 \times 10^{24}$ & $3.87 \times 10^{28}$ & 6.70 & 0.019 & 1.90 \\
Llama 3.3 & Meta AI & Meta AI & $6.86 \times 10^{24}$ & $5.65 \times 10^{28}$ & 11.98 & 0.012 & 1.21 \\
Amazon Nova Pro & Amazon & Anthropic/Amazon & $6.00 \times 10^{24}$ & $2.27 \times 10^{28}$ & 8.40 & 0.026 & 2.65 \\
Amazon Titan & Amazon & Anthropic/Amazon & $4.80 \times 10^{24}$ & $2.27 \times 10^{28}$ & 6.72 & 0.021 & 2.12 \\
Claude 2 & Anthropic & Anthropic/Amazon & $3.87 \times 10^{24}$ & $2.27 \times 10^{28}$ & 5.41 & 0.017 & 1.70 \\
Minerva (540B) & Google & Google DeepMind & $2.74 \times 10^{24}$ & $3.87 \times 10^{28}$ & 2.50 & 0.007 & 0.71 \\
GPT-3.5 (text-davinci-003) & OpenAI & Microsoft/OpenAI & $2.58 \times 10^{24}$ & $4.35 \times 10^{28}$ & 3.61 & 0.006 & 0.59 \\
U-PaLM (540B) & Google & Google DeepMind & $2.53 \times 10^{24}$ & $3.87 \times 10^{28}$ & 2.31 & 0.007 & 0.65 \\
PaLM (540B) & Google Research & Google DeepMind & $2.53 \times 10^{24}$ & $3.87 \times 10^{28}$ & 2.31 & 0.007 & 0.65 \\
Flan-PaLM 540B & Google & Google DeepMind & $2.50 \times 10^{24}$ & $3.87 \times 10^{28}$ & 2.28 & 0.006 & 0.65 \\
FLAN 137B & Google Research & Google DeepMind & $2.05 \times 10^{24}$ & $3.87 \times 10^{28}$ & 1.87 & 0.005 & 0.53 \\
Meta Movie Gen Video & Meta AI & Meta AI & $1.65 \times 10^{24}$ & $5.65 \times 10^{28}$ & 2.88 & 0.003 & 0.29 \\
Megatron-Turing NLG 530B & Microsoft,NVIDIA & Microsoft/OpenAI & $1.17 \times 10^{24}$ & $4.35 \times 10^{28}$ & 1.64 & 0.003 & 0.27 \\
Llama 2-70B & Meta AI & Meta AI & $8.10 \times 10^{23}$ & $5.65 \times 10^{28}$ & 1.41 & 0.001 & 0.14 \\
Gopher (280B) & DeepMind & Google DeepMind & $6.31 \times 10^{23}$ & $3.87 \times 10^{28}$ & 0.58 & 0.002 & 0.16 \\
Chinchilla & DeepMind & Google DeepMind & $5.76 \times 10^{23}$ & $3.87 \times 10^{28}$ & 0.53 & 0.001 & 0.15 \\
LLaMA-65B & Meta AI & Meta AI & $5.50 \times 10^{23}$ & $5.65 \times 10^{28}$ & 0.96 & 0.001 & 0.10 \\
OPT-175B & Meta AI & Meta AI & $4.30 \times 10^{23}$ & $5.65 \times 10^{28}$ & 0.75 & 0.001 & 0.08 \\
BlenderBot 3 & McGill University,Meta AI,Mila & Meta AI & $4.30 \times 10^{23}$ & $5.65 \times 10^{28}$ & 0.75 & 0.001 & 0.08 \\
Parti & Google Research & Google DeepMind & $3.96 \times 10^{23}$ & $3.87 \times 10^{28}$ & 0.36 & 0.001 & 0.10 \\
FunSearch & Google DeepMind & Google DeepMind & $3.87 \times 10^{23}$ & $3.87 \times 10^{28}$ & 0.35 & 0.001 & 0.10 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{AI Model Training Compute Requirements (Part 2 of 6)}
\label{tab:model-compute-2}
\tiny
\begin{tabular}{lllrrrrrr}
\toprule
Model & Organization & Lab/Cloud & Train & Parent Org Peak & Model/Public & Model/Peak & Model/Peak \\
& & & FLOPs & Annual FLOPs & Models (\%) & Annual (\%) & w/100x (\%) \\
\midrule
GLaM & Google & Google DeepMind & $3.64 \times 10^{23}$ & $3.87 \times 10^{28}$ & 0.33 & 0.001 & 0.09 \\
LaMDA & Google & Google DeepMind & $3.55 \times 10^{23}$ & $3.87 \times 10^{28}$ & 0.32 & 0.001 & 0.09 \\
AlphaGo Zero & DeepMind & Google DeepMind & $3.41 \times 10^{23}$ & $3.87 \times 10^{28}$ & 0.31 & 0.001 & 0.09 \\
Galactica & Meta AI & Meta AI & $3.24 \times 10^{23}$ & $5.65 \times 10^{28}$ & 0.57 & 0.001 & 0.06 \\
InstructGPT 175B & OpenAI & Microsoft/OpenAI & $3.19 \times 10^{23}$ & $4.35 \times 10^{28}$ & 0.45 & 0.001 & 0.07 \\
GPT-3 175B (davinci) & OpenAI & Microsoft/OpenAI & $3.14 \times 10^{23}$ & $4.35 \times 10^{28}$ & 0.44 & 0.001 & 0.07 \\
ST-MoE & Google,Google Brain,Google Research & Google DeepMind & $2.90 \times 10^{23}$ & $3.87 \times 10^{28}$ & 0.26 & 0.001 & 0.07 \\
Flamingo & DeepMind & Google DeepMind & $2.19 \times 10^{23}$ & $3.87 \times 10^{28}$ & 0.20 & 0.001 & 0.06 \\
AlexaTM 20B & Amazon & Anthropic/Amazon & $2.04 \times 10^{23}$ & $2.27 \times 10^{28}$ & 0.29 & 0.001 & 0.09 \\
AlphaGo Master & DeepMind & Google DeepMind & $2.00 \times 10^{23}$ & $3.87 \times 10^{28}$ & 0.18 & 0.001 & 0.05 \\
ViT-22B & Google & Google DeepMind & $1.93 \times 10^{23}$ & $3.87 \times 10^{28}$ & 0.18 & 0.001 & 0.05 \\
PaLI & Google & Google DeepMind & $1.69 \times 10^{23}$ & $3.87 \times 10^{28}$ & 0.15 & 0.000 & 0.04 \\
AlphaCode & DeepMind & Google DeepMind & $1.64 \times 10^{23}$ & $3.87 \times 10^{28}$ & 0.15 & 0.000 & 0.04 \\
Llama Guard & Meta AI & Meta AI & $1.60 \times 10^{23}$ & $5.65 \times 10^{28}$ & 0.28 & 0.000 & 0.03 \\
UL2 & Google Research,Google Brain & Google DeepMind & $1.20 \times 10^{23}$ & $3.87 \times 10^{28}$ & 0.11 & 0.000 & 0.03 \\
Meena & Google Brain & Google DeepMind & $1.12 \times 10^{23}$ & $3.87 \times 10^{28}$ & 0.10 & 0.000 & 0.03 \\
OpenVLA & Stanford,UC Berkeley,Toyota,DeepMind,MIT & Google DeepMind & $1.10 \times 10^{23}$ & $3.87 \times 10^{28}$ & 0.10 & 0.000 & 0.03 \\
Llama 2-7B & Meta AI & Meta AI & $8.40 \times 10^{22}$ & $5.65 \times 10^{28}$ & 0.15 & 0.000 & 0.01 \\
Switch & Google & Google DeepMind & $8.22 \times 10^{22}$ & $3.87 \times 10^{28}$ & 0.08 & 0.000 & 0.02 \\
mT5-XXL & Google,Google Research & Google DeepMind & $8.20 \times 10^{22}$ & $3.87 \times 10^{28}$ & 0.07 & 0.000 & 0.02 \\
ByT5-XXL & Google,Google Research & Google DeepMind & $8.10 \times 10^{22}$ & $3.87 \times 10^{28}$ & 0.07 & 0.000 & 0.02 \\
LLaVA 1.5 & UW Madison,Microsoft Research & Microsoft/OpenAI & $7.81 \times 10^{22}$ & $4.35 \times 10^{28}$ & 0.11 & 0.000 & 0.02 \\
LLaVA & UW Madison,Microsoft,Columbia & Microsoft/OpenAI & $7.80 \times 10^{22}$ & $4.35 \times 10^{28}$ & 0.11 & 0.000 & 0.02 \\
ProtT5-XXL & TU Munich,Med AI,NVIDIA,Oak Ridge,Google & Google DeepMind & $7.37 \times 10^{22}$ & $3.87 \times 10^{28}$ & 0.07 & 0.000 & 0.02 \\
ESM2-15B & Meta AI,NYU,Stanford,MIT & Meta AI & $7.35 \times 10^{22}$ & $5.65 \times 10^{28}$ & 0.13 & 0.000 & 0.01 \\
Codex & OpenAI & Microsoft/OpenAI & $7.34 \times 10^{22}$ & $4.35 \times 10^{28}$ & 0.10 & 0.000 & 0.02 \\
CoCa & Google Research & Google DeepMind & $7.30 \times 10^{22}$ & $3.87 \times 10^{28}$ & 0.07 & 0.000 & 0.02 \\
OpenAI Five & OpenAI & Microsoft/OpenAI & $6.70 \times 10^{22}$ & $4.35 \times 10^{28}$ & 0.09 & 0.000 & 0.02 \\
AlphaStar & DeepMind & Google DeepMind & $5.93 \times 10^{22}$ & $3.87 \times 10^{28}$ & 0.05 & 0.000 & 0.02 \\
ViT-G/14 & Google Brain,Google Research & Google DeepMind & $5.85 \times 10^{22}$ & $3.87 \times 10^{28}$ & 0.05 & 0.000 & 0.02 \\
XGLM-7.5B & Meta AI,Facebook AI Research & Meta AI & $2.25 \times 10^{22}$ & $5.65 \times 10^{28}$ & 0.04 & 0.000 & 0.00 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{AI Model Training Compute Requirements (Part 3 of 6)}
\label{tab:model-compute-3}
\tiny
\begin{tabular}{lllrrrrrr}
\toprule
Model & Organization & Lab/Cloud & Train & Parent Org Peak & Model/Public & Model/Peak & Model/Peak \\
& & & FLOPs & Annual FLOPs & Models (\%) & Annual (\%) & w/100x (\%) \\
\midrule
GraphCast & Google DeepMind & Google DeepMind & $2.10 \times 10^{22}$ & $3.87 \times 10^{28}$ & 0.02 & 0.000 & 0.01 \\
NLLB & Meta AI & Meta AI & $1.75 \times 10^{22}$ & $5.65 \times 10^{28}$ & 0.03 & 0.000 & 0.00 \\
RETRO-7B & DeepMind & Google DeepMind & $1.68 \times 10^{22}$ & $3.87 \times 10^{28}$ & 0.02 & 0.000 & 0.00 \\
Turing-NLG & Microsoft & Microsoft/OpenAI & $1.57 \times 10^{22}$ & $4.35 \times 10^{28}$ & 0.02 & 0.000 & 0.00 \\
Imagen & Google Brain & Google DeepMind & $1.46 \times 10^{22}$ & $3.87 \times 10^{28}$ & 0.01 & 0.000 & 0.00 \\
OpenAI Five Rerun & OpenAI & Microsoft/OpenAI & $1.30 \times 10^{22}$ & $4.35 \times 10^{28}$ & 0.02 & 0.000 & 0.00 \\
CLIP (ViT L/14@336px) & OpenAI & Microsoft/OpenAI & $1.05 \times 10^{22}$ & $4.35 \times 10^{28}$ & 0.01 & 0.000 & 0.00 \\
AudioGen & Meta AI,Hebrew University & Meta AI & $9.50 \times 10^{21}$ & $5.65 \times 10^{28}$ & 0.02 & 0.000 & 0.00 \\
T5-3B & Google & Google DeepMind & $9.00 \times 10^{21}$ & $3.87 \times 10^{28}$ & 0.01 & 0.000 & 0.00 \\
iGPT-L & OpenAI & Microsoft/OpenAI & $8.91 \times 10^{21}$ & $4.35 \times 10^{28}$ & 0.01 & 0.000 & 0.00 \\
ContextNet + Noisy Student & Google & Google DeepMind & $8.16 \times 10^{21}$ & $3.87 \times 10^{28}$ & 0.01 & 0.000 & 0.00 \\
Segment Anything Model & Meta AI & Meta AI & $7.80 \times 10^{21}$ & $5.65 \times 10^{28}$ & 0.01 & 0.000 & 0.00 \\
Conformer + Wav2vec 2.0 & Google,Google Research,Google Brain & Google DeepMind & $7.60 \times 10^{21}$ & $3.87 \times 10^{28}$ & 0.01 & 0.000 & 0.00 \\
GNMT & Google & Google DeepMind & $6.62 \times 10^{21}$ & $3.87 \times 10^{28}$ & 0.01 & 0.000 & 0.00 \\
ADM & OpenAI & Microsoft/OpenAI & $6.20 \times 10^{21}$ & $4.35 \times 10^{28}$ & 0.01 & 0.000 & 0.00 \\
XLNet & CMU,Google Brain & Google DeepMind & $6.19 \times 10^{21}$ & $3.87 \times 10^{28}$ & 0.01 & 0.000 & 0.00 \\
NÜWA & Microsoft Research,Peking University & Microsoft/OpenAI & $4.84 \times 10^{21}$ & $4.35 \times 10^{28}$ & 0.01 & 0.000 & 0.00 \\
AlphaFold-Multimer & Google DeepMind,DeepMind & Google DeepMind & $4.35 \times 10^{21}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
ViT-Huge/14 & Google Brain,Google Research & Google DeepMind & $4.26 \times 10^{21}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
Whisper & OpenAI & Microsoft/OpenAI & $4.21 \times 10^{21}$ & $4.35 \times 10^{28}$ & 0.01 & 0.000 & 0.00 \\
Gato & DeepMind & Google DeepMind & $4.02 \times 10^{21}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
ViT-G (model soup) & UW,Columbia,Google,Meta,Tel Aviv & Meta AI & $3.40 \times 10^{21}$ & $5.65 \times 10^{28}$ & 0.01 & 0.000 & 0.00 \\
ViT-G (model soup) & UW,Columbia,Google,Meta,Tel Aviv & Google DeepMind & $3.40 \times 10^{21}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
ELECTRA & Stanford,Google,Google Brain & Google DeepMind & $3.10 \times 10^{21}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
AlphaFold 2 & DeepMind & Google DeepMind & $2.99 \times 10^{21}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{AI Model Training Compute Requirements (Part 4 of 6)}
\label{tab:model-compute-4}
\tiny
\begin{tabular}{lllrrrrrr}
\toprule
Model & Organization & Lab/Cloud & Train & Parent Org Peak & Model/Public & Model/Peak & Model/Peak \\
& & & FLOPs & Annual FLOPs & Models (\%) & Annual (\%) & w/100x (\%) \\
\midrule
ALBERT-xxlarge & Toyota Tech Institute,Google & Google DeepMind & $2.39 \times 10^{21}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
NASv3 (CIFAR-10) & Google Brain & Google DeepMind & $2.20 \times 10^{21}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
GPT-2 (1.5B) & OpenAI & Microsoft/OpenAI & $1.92 \times 10^{21}$ & $4.35 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
EMDR & Mila,McGill,DeepMind & Google DeepMind & $1.91 \times 10^{21}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
AlphaGo Lee & DeepMind & Google DeepMind & $1.90 \times 10^{21}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
BigGAN-deep 512x512 & Heriot-Watt,DeepMind & Google DeepMind & $1.80 \times 10^{21}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
MnasNet-A3 & Google & Google DeepMind & $1.50 \times 10^{21}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
MnasNet-A1 + SSDLite & Google & Google DeepMind & $1.50 \times 10^{21}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
Swin Transformer V2 & Microsoft Research Asia & Microsoft/OpenAI & $1.10 \times 10^{21}$ & $4.35 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
JFT & Google Research,CMU & Google DeepMind & $8.43 \times 10^{20}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
OpenAI TI7 DOTA 1v1 & OpenAI & Microsoft/OpenAI & $6.05 \times 10^{20}$ & $4.35 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
BERT-Large-CAS (PTB+WT2+WT103) & Amazon & Anthropic/Amazon & $5.21 \times 10^{20}$ & $2.27 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{AI Model Training Compute Requirements (Part 5 of 6)}
\label{tab:model-compute-5}
\tiny
\begin{tabular}{lllrrrrrr}
\toprule
Model & Organization & Lab/Cloud & Train & Parent Org Peak & Model/Public & Model/Peak & Model/Peak \\
& & & FLOPs & Annual FLOPs & Models (\%) & Annual (\%) & w/100x (\%) \\
\midrule
Big Transformer for Back-Translation & Facebook AI Research,Google Brain & Google DeepMind & $4.78 \times 10^{20}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
Xception & Google & Google DeepMind & $4.36 \times 10^{20}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
AmoebaNet-A (F=448) & Google Brain & Google DeepMind & $3.85 \times 10^{20}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
AlphaGo Fan & DeepMind & Google DeepMind & $3.80 \times 10^{20}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
SNM-skip & Google & Google DeepMind & $2.98 \times 10^{20}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
BERT-Large & Google & Google DeepMind & $2.85 \times 10^{20}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
IMPALA & DeepMind & Google DeepMind & $1.68 \times 10^{20}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
Mesh-TensorFlow Transformer 4.9B & Google Brain & Google DeepMind & $1.62 \times 10^{20}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
Contriever & Meta AI,UCL,PSL,Grenoble & Meta AI & $1.57 \times 10^{20}$ & $5.65 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
AlphaFold & DeepMind & Google DeepMind & $1.00 \times 10^{20}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
EfficientNetV2-XL & Google,Google Brain & Google DeepMind & $9.56 \times 10^{19}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
MoE-Multi & Jagiellonian University,Google Brain & Google DeepMind & $9.39 \times 10^{19}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
Adaptive Input Transformer + RD & Microsoft Research Asia,Soochow & Microsoft/OpenAI & $8.20 \times 10^{19}$ & $4.35 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
DeiT-B & Meta AI,Sorbonne University & Meta AI & $7.88 \times 10^{19}$ & $5.65 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
BEIT-3 & Microsoft & Microsoft/OpenAI & $7.00 \times 10^{19}$ & $4.35 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
Mesh-TensorFlow Transformer 2.9B & Google Brain & Google DeepMind & $6.84 \times 10^{19}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
PNASNet-5 & Johns Hopkins,Google AI,Stanford & Google DeepMind & $6.63 \times 10^{19}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
Sparse all-MLP & Meta AI & Meta AI & $6.08 \times 10^{19}$ & $5.65 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
ConvS2S (ensemble of 8 models) & Meta AI & Meta AI & $5.64 \times 10^{19}$ & $5.65 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
Seq2Seq LSTM & Google & Google DeepMind & $5.60 \times 10^{19}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
MuZero & DeepMind & Google DeepMind & $4.80 \times 10^{19}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
Population-based DRL & DeepMind & Google DeepMind & $3.49 \times 10^{19}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
QT-Opt & Google Brain,UC Berkeley & Google DeepMind & $3.49 \times 10^{19}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
LSTM (Hebbian, Cache, MbPA) & DeepMind,UCL & Google DeepMind & $3.33 \times 10^{19}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
ResNet-200 & Microsoft Research Asia & Microsoft/OpenAI & $2.97 \times 10^{19}$ & $4.35 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
Segatron-XL large, M=384 + HCP & Microsoft Research,Waterloo & Microsoft/OpenAI & $2.65 \times 10^{19}$ & $4.35 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
MultiBand Diffusion & Meta AI,Hebrew U,LORIA & Meta AI & $2.60 \times 10^{19}$ & $5.65 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
Transformer local-attention (NesT-B) & Google Cloud,Google Research & Google DeepMind & $2.41 \times 10^{19}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
MSRA (C, PReLU) & Microsoft Research & Microsoft/OpenAI & $2.40 \times 10^{19}$ & $4.35 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
Detic & Meta AI,UT Austin & Meta AI & $2.34 \times 10^{19}$ & $5.65 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
GPT-1 & OpenAI & Microsoft/OpenAI & $1.76 \times 10^{19}$ & $4.35 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
TransE & UTC-CNRS,Google & Google DeepMind & $1.34 \times 10^{18}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{AI Model Training Compute Requirements (Part 6 of 6)}
\label{tab:model-compute-6}
\tiny
\begin{tabular}{lllrrrrrr}
\toprule
Model & Organization & Lab/Cloud & Train & Parent Org Peak & Model/Public & Model/Peak & Model/Peak \\
& & & FLOPs & Annual FLOPs & Models (\%) & Annual (\%) & w/100x (\%) \\
\midrule
KN-LM & Google & Google DeepMind & $7.73 \times 10^{17}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
WeNet (Penn Treebank) & Amazon & Anthropic/Amazon & $7.30 \times 10^{17}$ & $2.27 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
Unsupervised High-level Feature Learner & Google & Google DeepMind & $6.00 \times 10^{17}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
CT-MoS (WT2) & Google,National Tsing Hua University & Google DeepMind & $5.62 \times 10^{17}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
DistBelief Speech & Google & Google DeepMind & $3.11 \times 10^{17}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
Mogrifier RLSTM (WT2) & DeepMind & Google DeepMind & $1.40 \times 10^{17}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
ReLU-Speech & Google,Toronto,NYU & Google DeepMind & $1.28 \times 10^{17}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
Large regularized LSTM & NYU,Google Brain & Google DeepMind & $9.10 \times 10^{16}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
R-FCN & Tsinghua,Microsoft Research & Microsoft/OpenAI & $6.15 \times 10^{16}$ & $4.35 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
ADAM (CIFAR-10) & Amsterdam,OpenAI,Toronto & Microsoft/OpenAI & $6.05 \times 10^{16}$ & $4.35 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
Word2Vec (large) & Google & Google DeepMind & $3.89 \times 10^{16}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
ENAS & Google Brain,CMU,Stanford & Google DeepMind & $2.01 \times 10^{16}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
DARTS & DeepMind,CMU & Google DeepMind & $1.10 \times 10^{16}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
NAS with base 8 and shared embeddings & Google Brain & Google DeepMind & $1.05 \times 10^{16}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
ISS & Duke University,Microsoft & Microsoft/OpenAI & $3.40 \times 10^{15}$ & $4.35 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
Search-Proven Best LSTM & Google & Google DeepMind & $3.34 \times 10^{15}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
DQN & DeepMind & Google DeepMind & $2.30 \times 10^{15}$ & $3.87 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
RankNet & Microsoft Research,Microsoft & Microsoft/OpenAI & $3.48 \times 10^{12}$ & $4.35 \times 10^{28}$ & 0.00 & 0.000 & 0.00 \\
\bottomrule
\end{tabular}
\end{table}

\end{document}